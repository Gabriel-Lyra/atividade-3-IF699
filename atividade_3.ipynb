{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "atividade-3-IF699.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (system-wide)",
      "language": "python",
      "metadata": {
        "cocalc": {
          "description": "Python 3 programming language",
          "priority": 100,
          "url": "https://www.python.org/"
        }
      },
      "name": "python3",
      "resource_dir": "/ext/jupyter/kernels/python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "IzO5Zko7ivDZ"
      },
      "source": [
        "# Instruções Gerais\n",
        "\n",
        "*   A atividade é **individual**. Cópias e plágios não serão tolerados\n",
        "*   Data de entrega: **16/11/2021, até as 23:59h**\n",
        "\n",
        "  * Apenas pelo **Classroom**\n",
        "  * Façam uma cópia do notebook, e trabalhem nela para a entrega até o prazo definido\n",
        "  * **Atentem para todos os pontos de `# TODO:`**, pois estes são os pontos de implementação\n",
        "* Sobre política de atrasos: \n",
        "  * cada aluno terá a **tolerância de 2 dias de atraso, considerado todos os exercícios** propostos. \n",
        "  * A partir do terceiro dia, 60% da nota obtida será computada. \n",
        "  * Do quarto dia em diante, não será atribuída nota alguma.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "YY0zw2HDRtA-"
      },
      "source": [
        "# Atividade 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "w7nx5rTPTClR"
      },
      "source": [
        "**Testar CUDA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_M9ho_SjKNat",
        "outputId": "d5209116-02ac-4e1a-a4fd-a0362f57eb8d"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "usar_gpu = torch.cuda.is_available()\n",
        "if usar_gpu:\n",
        "  print(\"CUDA disponível. Processamento em GPU!\")\n",
        "else:\n",
        "  print(\"CUDA NÃO disponível. Processamento em CPU!\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA NÃO disponível. Processamento em CPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Bx64mbsTpc4d"
      },
      "source": [
        "**Carregando e Divisão de Dados**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxCyVPOqj6kK",
        "outputId": "c2266a98-5cd7-452f-ed62-450f678bb468"
      },
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "# Converte os dados para um FloatTensor \n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Também pode ser importante fazer uma normalização nos dados.\n",
        "# Nesses casos, segue abaixo um exemplo de composição de transformações\n",
        "# utilizando uma normalização tradicioal do CIFAR10.\n",
        "\n",
        "# transform = transforms.Compose([transforms.ToTensor(),\n",
        "#                                 transforms.Normalize(mean=[0.491, 0.482, 0.447],\n",
        "#                                                      std=[0.247, 0.243, 0.262])])\n",
        "\n",
        "# Escolhe os dados de treino e teste\n",
        "train_data = datasets.CIFAR10(root='data', train=True,\n",
        "                                   download=True, transform=transform)\n",
        "test_data = datasets.CIFAR10(root='data', train=False,\n",
        "                                  download=True, transform=transform)\n",
        "\n",
        "# Define o carregamento dos dados utilizado os dados escolhidos\n",
        "batch_size = 20\n",
        "num_workers = 0\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ChuSPXSFldTm"
      },
      "source": [
        "**Definição do Modelo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C867WoGeJGsx"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Sequential(nn.Conv2d(3, 16, 3, padding=1),\n",
        "                               nn.ReLU(True),\n",
        "                               nn.MaxPool2d(2, 2))\n",
        "    \n",
        "    self.conv2 = nn.Sequential(nn.Conv2d(16, 32, 3, padding=1),\n",
        "                               nn.ReLU(True),\n",
        "                               nn.MaxPool2d(2, 2))\n",
        "    \n",
        "    self.conv3 = nn.Sequential(nn.Conv2d(32, 64, 3, padding=1),\n",
        "                               nn.ReLU(True),\n",
        "                               nn.MaxPool2d(2, 2))\n",
        "    \n",
        "    self.fc1 = nn.Sequential(nn.Linear(64*4*4, 500),\n",
        "                             nn.ReLU(True))\n",
        "    \n",
        "    self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "\n",
        "    # transforma a imagem num vetor bidimensional (flatten)\n",
        "    x = x.view(-1, 64*4*4)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "R2gzYdw8OSwv"
      },
      "source": [
        "**Alocação do modelo e device adequado (dependente da disponibilidade GPU)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxQ4CzrdLO_d",
        "outputId": "c5b1cc4a-3e61-4ed8-a293-6337d1f2f94d"
      },
      "source": [
        "# cria o modelo\n",
        "modelo = CNN()\n",
        "print(modelo)\n",
        "\n",
        "# joga o modelo para GPU, caso aplicável\n",
        "if usar_gpu:\n",
        "  torch.cuda.init()\n",
        "  model.cuda()\n",
        "  cudnn.benchmark = True"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=500, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.25, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "SVRhp-8bpp16"
      },
      "source": [
        "**Função de Custo e Otimizador**. \n",
        "\n",
        "Vide [`torch.optim`](https://pytorch.org/docs/stable/optim.html) e [`torch.nn` Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions). Atentem para todos os parametros necessários."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmAMXsCZpqNf"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# especifique a funcao de custo (loss) adequada\n",
        "#DONE\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# especifique o otimizador\n",
        "#DONE\n",
        "optim = optim.Adam(modelo.parameters(), lr=0.001)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "BsAbmEhLm2Ua"
      },
      "source": [
        "**Treinamento**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS4k0qBcNTMS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6678618d-3c18-4ed7-999d-12d37d48cda4"
      },
      "source": [
        "# definição do numero de épocas\n",
        "#DONE\n",
        "n_epocas = 10\n",
        "\n",
        "modelo.train()\n",
        "for epoca in range(n_epocas):\n",
        "  loss_treino = 0\n",
        "\n",
        "  for batch_idx, (x, y) in enumerate(train_loader):\n",
        "    # mover tensores para GPU, se aplicável\n",
        "    if usar_gpu:\n",
        "      x, y = x.cuda(), y.cuda()\n",
        "\n",
        "    # TODO: 1. Zere o gradiente de todas as variávels do otimizador \n",
        "    #          (vide zero_grad do nn.optim)\n",
        "    #DONE\n",
        "    optim.zero_grad()\n",
        "\n",
        "    # TODO: 2. Faça um forward no modelo passando x como entrada \n",
        "    #          (não deixe de capturar a saída)\n",
        "    #DONE\n",
        "    output = modelo(x)\n",
        "\n",
        "\n",
        "    # TODO: 3. Calcule a loss do batch (utilize o criterion para comparar a \n",
        "    #          saída do modelo com os targets y)\n",
        "    #DONE\n",
        "    loss = criterion(output, y)\n",
        "\n",
        "    # TODO: 4. Compute o gradiente (backward na loss)\n",
        "    #DONE\n",
        "    loss.backward()\n",
        "\n",
        "    # TODO: 5. Realize um passo de otimização (step no optim)\n",
        "    #DONE\n",
        "    optim.step()\n",
        "\n",
        "    print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
        "\n",
        "    # atualização da loss de treinamento\n",
        "    loss_treino += loss\n",
        "\n",
        "  loss_treino = loss_treino/len(train_loader.dataset)\n",
        "  print(f\"Epoca {epoca+1}{n_epocas} \\t Loss de Treinamento: {loss_treino}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\tTraining batch 3 Loss: 0.685860\n",
            "\tTraining batch 4 Loss: 0.577702\n",
            "\tTraining batch 5 Loss: 0.315807\n",
            "\tTraining batch 6 Loss: 0.324251\n",
            "\tTraining batch 7 Loss: 0.345488\n",
            "\tTraining batch 8 Loss: 0.756276\n",
            "\tTraining batch 9 Loss: 0.586017\n",
            "\tTraining batch 10 Loss: 0.925084\n",
            "\tTraining batch 11 Loss: 0.820534\n",
            "\tTraining batch 12 Loss: 0.313197\n",
            "\tTraining batch 13 Loss: 1.311554\n",
            "\tTraining batch 14 Loss: 0.659598\n",
            "\tTraining batch 15 Loss: 0.439470\n",
            "\tTraining batch 16 Loss: 0.424330\n",
            "\tTraining batch 17 Loss: 0.680691\n",
            "\tTraining batch 18 Loss: 0.474150\n",
            "\tTraining batch 19 Loss: 0.549716\n",
            "\tTraining batch 20 Loss: 0.668045\n",
            "\tTraining batch 21 Loss: 0.772066\n",
            "\tTraining batch 22 Loss: 0.481583\n",
            "\tTraining batch 23 Loss: 0.423886\n",
            "\tTraining batch 24 Loss: 0.447608\n",
            "\tTraining batch 25 Loss: 0.548169\n",
            "\tTraining batch 26 Loss: 0.419115\n",
            "\tTraining batch 27 Loss: 0.442629\n",
            "\tTraining batch 28 Loss: 0.588976\n",
            "\tTraining batch 29 Loss: 0.637054\n",
            "\tTraining batch 30 Loss: 0.523995\n",
            "\tTraining batch 31 Loss: 0.582841\n",
            "\tTraining batch 32 Loss: 0.758681\n",
            "\tTraining batch 33 Loss: 0.489226\n",
            "\tTraining batch 34 Loss: 0.646646\n",
            "\tTraining batch 35 Loss: 0.586657\n",
            "\tTraining batch 36 Loss: 0.778048\n",
            "\tTraining batch 37 Loss: 0.796796\n",
            "\tTraining batch 38 Loss: 0.770378\n",
            "\tTraining batch 39 Loss: 0.447789\n",
            "\tTraining batch 40 Loss: 0.379384\n",
            "\tTraining batch 41 Loss: 0.713637\n",
            "\tTraining batch 42 Loss: 0.391641\n",
            "\tTraining batch 43 Loss: 0.961367\n",
            "\tTraining batch 44 Loss: 0.404566\n",
            "\tTraining batch 45 Loss: 0.559306\n",
            "\tTraining batch 46 Loss: 0.307703\n",
            "\tTraining batch 47 Loss: 0.933890\n",
            "\tTraining batch 48 Loss: 0.472036\n",
            "\tTraining batch 49 Loss: 0.174656\n",
            "\tTraining batch 50 Loss: 0.465909\n",
            "\tTraining batch 51 Loss: 0.534845\n",
            "\tTraining batch 52 Loss: 0.592300\n",
            "\tTraining batch 53 Loss: 0.716818\n",
            "\tTraining batch 54 Loss: 0.682142\n",
            "\tTraining batch 55 Loss: 0.478608\n",
            "\tTraining batch 56 Loss: 0.837117\n",
            "\tTraining batch 57 Loss: 0.526033\n",
            "\tTraining batch 58 Loss: 0.486668\n",
            "\tTraining batch 59 Loss: 0.765260\n",
            "\tTraining batch 60 Loss: 0.398795\n",
            "\tTraining batch 61 Loss: 0.394850\n",
            "\tTraining batch 62 Loss: 0.696855\n",
            "\tTraining batch 63 Loss: 0.539666\n",
            "\tTraining batch 64 Loss: 0.486475\n",
            "\tTraining batch 65 Loss: 0.698092\n",
            "\tTraining batch 66 Loss: 0.410031\n",
            "\tTraining batch 67 Loss: 0.498824\n",
            "\tTraining batch 68 Loss: 0.682097\n",
            "\tTraining batch 69 Loss: 0.366501\n",
            "\tTraining batch 70 Loss: 0.642338\n",
            "\tTraining batch 71 Loss: 0.497891\n",
            "\tTraining batch 72 Loss: 0.341053\n",
            "\tTraining batch 73 Loss: 0.435169\n",
            "\tTraining batch 74 Loss: 0.644115\n",
            "\tTraining batch 75 Loss: 0.870578\n",
            "\tTraining batch 76 Loss: 0.596510\n",
            "\tTraining batch 77 Loss: 1.117184\n",
            "\tTraining batch 78 Loss: 0.594142\n",
            "\tTraining batch 79 Loss: 0.834664\n",
            "\tTraining batch 80 Loss: 0.381318\n",
            "\tTraining batch 81 Loss: 0.909662\n",
            "\tTraining batch 82 Loss: 0.405742\n",
            "\tTraining batch 83 Loss: 0.562503\n",
            "\tTraining batch 84 Loss: 0.722927\n",
            "\tTraining batch 85 Loss: 0.300981\n",
            "\tTraining batch 86 Loss: 0.410013\n",
            "\tTraining batch 87 Loss: 0.540834\n",
            "\tTraining batch 88 Loss: 0.712616\n",
            "\tTraining batch 89 Loss: 0.596990\n",
            "\tTraining batch 90 Loss: 1.253698\n",
            "\tTraining batch 91 Loss: 0.296417\n",
            "\tTraining batch 92 Loss: 0.455988\n",
            "\tTraining batch 93 Loss: 0.609142\n",
            "\tTraining batch 94 Loss: 0.296600\n",
            "\tTraining batch 95 Loss: 0.822157\n",
            "\tTraining batch 96 Loss: 0.534110\n",
            "\tTraining batch 97 Loss: 0.830379\n",
            "\tTraining batch 98 Loss: 0.865275\n",
            "\tTraining batch 99 Loss: 0.398388\n",
            "\tTraining batch 100 Loss: 0.593367\n",
            "\tTraining batch 101 Loss: 0.606505\n",
            "\tTraining batch 102 Loss: 0.533283\n",
            "\tTraining batch 103 Loss: 0.586603\n",
            "\tTraining batch 104 Loss: 0.326971\n",
            "\tTraining batch 105 Loss: 0.658209\n",
            "\tTraining batch 106 Loss: 0.487850\n",
            "\tTraining batch 107 Loss: 0.322996\n",
            "\tTraining batch 108 Loss: 0.444525\n",
            "\tTraining batch 109 Loss: 0.261314\n",
            "\tTraining batch 110 Loss: 0.534998\n",
            "\tTraining batch 111 Loss: 0.524927\n",
            "\tTraining batch 112 Loss: 0.755558\n",
            "\tTraining batch 113 Loss: 0.261354\n",
            "\tTraining batch 114 Loss: 0.439257\n",
            "\tTraining batch 115 Loss: 0.369405\n",
            "\tTraining batch 116 Loss: 0.905349\n",
            "\tTraining batch 117 Loss: 0.300090\n",
            "\tTraining batch 118 Loss: 0.318969\n",
            "\tTraining batch 119 Loss: 0.332683\n",
            "\tTraining batch 120 Loss: 0.501429\n",
            "\tTraining batch 121 Loss: 0.818177\n",
            "\tTraining batch 122 Loss: 0.338168\n",
            "\tTraining batch 123 Loss: 0.442437\n",
            "\tTraining batch 124 Loss: 0.207993\n",
            "\tTraining batch 125 Loss: 0.537956\n",
            "\tTraining batch 126 Loss: 0.592203\n",
            "\tTraining batch 127 Loss: 0.825458\n",
            "\tTraining batch 128 Loss: 0.320589\n",
            "\tTraining batch 129 Loss: 0.400835\n",
            "\tTraining batch 130 Loss: 0.258372\n",
            "\tTraining batch 131 Loss: 0.488633\n",
            "\tTraining batch 132 Loss: 0.221521\n",
            "\tTraining batch 133 Loss: 0.147136\n",
            "\tTraining batch 134 Loss: 0.261676\n",
            "\tTraining batch 135 Loss: 0.611401\n",
            "\tTraining batch 136 Loss: 0.713553\n",
            "\tTraining batch 137 Loss: 0.912456\n",
            "\tTraining batch 138 Loss: 0.664470\n",
            "\tTraining batch 139 Loss: 0.440258\n",
            "\tTraining batch 140 Loss: 0.472214\n",
            "\tTraining batch 141 Loss: 0.508713\n",
            "\tTraining batch 142 Loss: 0.278122\n",
            "\tTraining batch 143 Loss: 0.432041\n",
            "\tTraining batch 144 Loss: 0.367858\n",
            "\tTraining batch 145 Loss: 0.329124\n",
            "\tTraining batch 146 Loss: 0.744673\n",
            "\tTraining batch 147 Loss: 0.412689\n",
            "\tTraining batch 148 Loss: 0.758797\n",
            "\tTraining batch 149 Loss: 0.631779\n",
            "\tTraining batch 150 Loss: 0.482564\n",
            "\tTraining batch 151 Loss: 0.542376\n",
            "\tTraining batch 152 Loss: 0.897389\n",
            "\tTraining batch 153 Loss: 0.592955\n",
            "\tTraining batch 154 Loss: 0.571532\n",
            "\tTraining batch 155 Loss: 0.618030\n",
            "\tTraining batch 156 Loss: 0.331255\n",
            "\tTraining batch 157 Loss: 0.453448\n",
            "\tTraining batch 158 Loss: 0.491521\n",
            "\tTraining batch 159 Loss: 0.715303\n",
            "\tTraining batch 160 Loss: 0.695946\n",
            "\tTraining batch 161 Loss: 0.222959\n",
            "\tTraining batch 162 Loss: 0.669639\n",
            "\tTraining batch 163 Loss: 0.478721\n",
            "\tTraining batch 164 Loss: 0.588640\n",
            "\tTraining batch 165 Loss: 0.491099\n",
            "\tTraining batch 166 Loss: 0.434674\n",
            "\tTraining batch 167 Loss: 0.727096\n",
            "\tTraining batch 168 Loss: 0.834576\n",
            "\tTraining batch 169 Loss: 0.870342\n",
            "\tTraining batch 170 Loss: 0.933092\n",
            "\tTraining batch 171 Loss: 0.382840\n",
            "\tTraining batch 172 Loss: 0.733408\n",
            "\tTraining batch 173 Loss: 0.888742\n",
            "\tTraining batch 174 Loss: 0.321978\n",
            "\tTraining batch 175 Loss: 0.996632\n",
            "\tTraining batch 176 Loss: 0.274783\n",
            "\tTraining batch 177 Loss: 0.383604\n",
            "\tTraining batch 178 Loss: 0.414294\n",
            "\tTraining batch 179 Loss: 0.493195\n",
            "\tTraining batch 180 Loss: 0.610588\n",
            "\tTraining batch 181 Loss: 0.535212\n",
            "\tTraining batch 182 Loss: 0.242257\n",
            "\tTraining batch 183 Loss: 0.369094\n",
            "\tTraining batch 184 Loss: 0.389262\n",
            "\tTraining batch 185 Loss: 0.214597\n",
            "\tTraining batch 186 Loss: 0.210347\n",
            "\tTraining batch 187 Loss: 0.391920\n",
            "\tTraining batch 188 Loss: 0.762092\n",
            "\tTraining batch 189 Loss: 0.691105\n",
            "\tTraining batch 190 Loss: 1.143660\n",
            "\tTraining batch 191 Loss: 0.811494\n",
            "\tTraining batch 192 Loss: 0.369875\n",
            "\tTraining batch 193 Loss: 0.370775\n",
            "\tTraining batch 194 Loss: 0.638074\n",
            "\tTraining batch 195 Loss: 0.699768\n",
            "\tTraining batch 196 Loss: 0.745575\n",
            "\tTraining batch 197 Loss: 0.672688\n",
            "\tTraining batch 198 Loss: 0.278365\n",
            "\tTraining batch 199 Loss: 0.175341\n",
            "\tTraining batch 200 Loss: 0.421413\n",
            "\tTraining batch 201 Loss: 0.847381\n",
            "\tTraining batch 202 Loss: 0.297701\n",
            "\tTraining batch 203 Loss: 0.520773\n",
            "\tTraining batch 204 Loss: 0.427726\n",
            "\tTraining batch 205 Loss: 0.610960\n",
            "\tTraining batch 206 Loss: 0.640040\n",
            "\tTraining batch 207 Loss: 0.854135\n",
            "\tTraining batch 208 Loss: 0.556898\n",
            "\tTraining batch 209 Loss: 0.753785\n",
            "\tTraining batch 210 Loss: 0.478139\n",
            "\tTraining batch 211 Loss: 0.556856\n",
            "\tTraining batch 212 Loss: 0.302577\n",
            "\tTraining batch 213 Loss: 0.544648\n",
            "\tTraining batch 214 Loss: 0.437348\n",
            "\tTraining batch 215 Loss: 0.229729\n",
            "\tTraining batch 216 Loss: 0.597235\n",
            "\tTraining batch 217 Loss: 0.373515\n",
            "\tTraining batch 218 Loss: 0.879273\n",
            "\tTraining batch 219 Loss: 0.537530\n",
            "\tTraining batch 220 Loss: 0.241125\n",
            "\tTraining batch 221 Loss: 0.246253\n",
            "\tTraining batch 222 Loss: 0.655966\n",
            "\tTraining batch 223 Loss: 0.776292\n",
            "\tTraining batch 224 Loss: 0.309527\n",
            "\tTraining batch 225 Loss: 1.194243\n",
            "\tTraining batch 226 Loss: 0.557566\n",
            "\tTraining batch 227 Loss: 0.669443\n",
            "\tTraining batch 228 Loss: 0.517526\n",
            "\tTraining batch 229 Loss: 0.351074\n",
            "\tTraining batch 230 Loss: 0.469939\n",
            "\tTraining batch 231 Loss: 0.182014\n",
            "\tTraining batch 232 Loss: 0.795125\n",
            "\tTraining batch 233 Loss: 0.573173\n",
            "\tTraining batch 234 Loss: 0.784021\n",
            "\tTraining batch 235 Loss: 0.581835\n",
            "\tTraining batch 236 Loss: 0.303684\n",
            "\tTraining batch 237 Loss: 0.504735\n",
            "\tTraining batch 238 Loss: 0.370478\n",
            "\tTraining batch 239 Loss: 0.952577\n",
            "\tTraining batch 240 Loss: 0.563307\n",
            "\tTraining batch 241 Loss: 0.522606\n",
            "\tTraining batch 242 Loss: 0.640225\n",
            "\tTraining batch 243 Loss: 0.909366\n",
            "\tTraining batch 244 Loss: 0.323385\n",
            "\tTraining batch 245 Loss: 0.729543\n",
            "\tTraining batch 246 Loss: 1.065800\n",
            "\tTraining batch 247 Loss: 0.696370\n",
            "\tTraining batch 248 Loss: 0.335924\n",
            "\tTraining batch 249 Loss: 1.273619\n",
            "\tTraining batch 250 Loss: 0.705211\n",
            "\tTraining batch 251 Loss: 0.542722\n",
            "\tTraining batch 252 Loss: 0.794153\n",
            "\tTraining batch 253 Loss: 0.460727\n",
            "\tTraining batch 254 Loss: 1.103280\n",
            "\tTraining batch 255 Loss: 0.913018\n",
            "\tTraining batch 256 Loss: 0.422782\n",
            "\tTraining batch 257 Loss: 0.788047\n",
            "\tTraining batch 258 Loss: 0.345366\n",
            "\tTraining batch 259 Loss: 0.576413\n",
            "\tTraining batch 260 Loss: 0.550165\n",
            "\tTraining batch 261 Loss: 0.980890\n",
            "\tTraining batch 262 Loss: 0.326833\n",
            "\tTraining batch 263 Loss: 0.434397\n",
            "\tTraining batch 264 Loss: 0.426888\n",
            "\tTraining batch 265 Loss: 0.697538\n",
            "\tTraining batch 266 Loss: 0.597065\n",
            "\tTraining batch 267 Loss: 0.408096\n",
            "\tTraining batch 268 Loss: 0.672920\n",
            "\tTraining batch 269 Loss: 0.537661\n",
            "\tTraining batch 270 Loss: 0.545266\n",
            "\tTraining batch 271 Loss: 1.026962\n",
            "\tTraining batch 272 Loss: 0.783971\n",
            "\tTraining batch 273 Loss: 0.674923\n",
            "\tTraining batch 274 Loss: 0.570967\n",
            "\tTraining batch 275 Loss: 0.476691\n",
            "\tTraining batch 276 Loss: 0.692042\n",
            "\tTraining batch 277 Loss: 0.732143\n",
            "\tTraining batch 278 Loss: 0.765275\n",
            "\tTraining batch 279 Loss: 0.419690\n",
            "\tTraining batch 280 Loss: 0.545591\n",
            "\tTraining batch 281 Loss: 0.316897\n",
            "\tTraining batch 282 Loss: 0.759408\n",
            "\tTraining batch 283 Loss: 0.588836\n",
            "\tTraining batch 284 Loss: 0.278892\n",
            "\tTraining batch 285 Loss: 0.514748\n",
            "\tTraining batch 286 Loss: 0.776270\n",
            "\tTraining batch 287 Loss: 0.812657\n",
            "\tTraining batch 288 Loss: 0.769082\n",
            "\tTraining batch 289 Loss: 0.168455\n",
            "\tTraining batch 290 Loss: 0.501377\n",
            "\tTraining batch 291 Loss: 0.619840\n",
            "\tTraining batch 292 Loss: 0.852423\n",
            "\tTraining batch 293 Loss: 1.035113\n",
            "\tTraining batch 294 Loss: 0.731687\n",
            "\tTraining batch 295 Loss: 0.217712\n",
            "\tTraining batch 296 Loss: 0.510615\n",
            "\tTraining batch 297 Loss: 0.783406\n",
            "\tTraining batch 298 Loss: 0.567813\n",
            "\tTraining batch 299 Loss: 0.618018\n",
            "\tTraining batch 300 Loss: 0.422920\n",
            "\tTraining batch 301 Loss: 0.665493\n",
            "\tTraining batch 302 Loss: 0.783618\n",
            "\tTraining batch 303 Loss: 0.249675\n",
            "\tTraining batch 304 Loss: 0.797728\n",
            "\tTraining batch 305 Loss: 0.820396\n",
            "\tTraining batch 306 Loss: 0.890896\n",
            "\tTraining batch 307 Loss: 0.631318\n",
            "\tTraining batch 308 Loss: 0.576724\n",
            "\tTraining batch 309 Loss: 0.568093\n",
            "\tTraining batch 310 Loss: 0.689528\n",
            "\tTraining batch 311 Loss: 0.430769\n",
            "\tTraining batch 312 Loss: 0.798262\n",
            "\tTraining batch 313 Loss: 0.473211\n",
            "\tTraining batch 314 Loss: 0.521652\n",
            "\tTraining batch 315 Loss: 0.596010\n",
            "\tTraining batch 316 Loss: 0.476325\n",
            "\tTraining batch 317 Loss: 0.583589\n",
            "\tTraining batch 318 Loss: 0.517148\n",
            "\tTraining batch 319 Loss: 0.743621\n",
            "\tTraining batch 320 Loss: 0.170416\n",
            "\tTraining batch 321 Loss: 0.570630\n",
            "\tTraining batch 322 Loss: 0.502290\n",
            "\tTraining batch 323 Loss: 0.346952\n",
            "\tTraining batch 324 Loss: 0.816943\n",
            "\tTraining batch 325 Loss: 0.559917\n",
            "\tTraining batch 326 Loss: 0.350787\n",
            "\tTraining batch 327 Loss: 0.450121\n",
            "\tTraining batch 328 Loss: 0.679659\n",
            "\tTraining batch 329 Loss: 0.295660\n",
            "\tTraining batch 330 Loss: 0.335616\n",
            "\tTraining batch 331 Loss: 0.670023\n",
            "\tTraining batch 332 Loss: 0.335739\n",
            "\tTraining batch 333 Loss: 0.758398\n",
            "\tTraining batch 334 Loss: 0.471723\n",
            "\tTraining batch 335 Loss: 1.170284\n",
            "\tTraining batch 336 Loss: 0.840085\n",
            "\tTraining batch 337 Loss: 0.393168\n",
            "\tTraining batch 338 Loss: 0.429796\n",
            "\tTraining batch 339 Loss: 0.368453\n",
            "\tTraining batch 340 Loss: 0.279918\n",
            "\tTraining batch 341 Loss: 0.391279\n",
            "\tTraining batch 342 Loss: 0.854928\n",
            "\tTraining batch 343 Loss: 0.546032\n",
            "\tTraining batch 344 Loss: 0.764874\n",
            "\tTraining batch 345 Loss: 1.040109\n",
            "\tTraining batch 346 Loss: 0.652631\n",
            "\tTraining batch 347 Loss: 0.541979\n",
            "\tTraining batch 348 Loss: 0.751904\n",
            "\tTraining batch 349 Loss: 0.164231\n",
            "\tTraining batch 350 Loss: 0.731702\n",
            "\tTraining batch 351 Loss: 0.708327\n",
            "\tTraining batch 352 Loss: 0.570081\n",
            "\tTraining batch 353 Loss: 0.699975\n",
            "\tTraining batch 354 Loss: 0.913968\n",
            "\tTraining batch 355 Loss: 0.739298\n",
            "\tTraining batch 356 Loss: 0.951130\n",
            "\tTraining batch 357 Loss: 0.304499\n",
            "\tTraining batch 358 Loss: 0.603338\n",
            "\tTraining batch 359 Loss: 0.639252\n",
            "\tTraining batch 360 Loss: 0.469711\n",
            "\tTraining batch 361 Loss: 0.387343\n",
            "\tTraining batch 362 Loss: 0.409198\n",
            "\tTraining batch 363 Loss: 0.544854\n",
            "\tTraining batch 364 Loss: 0.618860\n",
            "\tTraining batch 365 Loss: 0.369723\n",
            "\tTraining batch 366 Loss: 0.375935\n",
            "\tTraining batch 367 Loss: 0.171987\n",
            "\tTraining batch 368 Loss: 0.362429\n",
            "\tTraining batch 369 Loss: 0.644486\n",
            "\tTraining batch 370 Loss: 0.549122\n",
            "\tTraining batch 371 Loss: 0.885029\n",
            "\tTraining batch 372 Loss: 0.466071\n",
            "\tTraining batch 373 Loss: 0.593611\n",
            "\tTraining batch 374 Loss: 0.534436\n",
            "\tTraining batch 375 Loss: 0.501355\n",
            "\tTraining batch 376 Loss: 0.604922\n",
            "\tTraining batch 377 Loss: 0.481330\n",
            "\tTraining batch 378 Loss: 0.587184\n",
            "\tTraining batch 379 Loss: 0.812717\n",
            "\tTraining batch 380 Loss: 0.716652\n",
            "\tTraining batch 381 Loss: 0.360375\n",
            "\tTraining batch 382 Loss: 0.638582\n",
            "\tTraining batch 383 Loss: 0.780123\n",
            "\tTraining batch 384 Loss: 0.708063\n",
            "\tTraining batch 385 Loss: 0.863201\n",
            "\tTraining batch 386 Loss: 0.697443\n",
            "\tTraining batch 387 Loss: 0.540689\n",
            "\tTraining batch 388 Loss: 0.436778\n",
            "\tTraining batch 389 Loss: 0.350218\n",
            "\tTraining batch 390 Loss: 0.380979\n",
            "\tTraining batch 391 Loss: 0.731387\n",
            "\tTraining batch 392 Loss: 0.579940\n",
            "\tTraining batch 393 Loss: 0.652324\n",
            "\tTraining batch 394 Loss: 0.375562\n",
            "\tTraining batch 395 Loss: 1.055021\n",
            "\tTraining batch 396 Loss: 0.385970\n",
            "\tTraining batch 397 Loss: 0.668628\n",
            "\tTraining batch 398 Loss: 0.824738\n",
            "\tTraining batch 399 Loss: 0.735747\n",
            "\tTraining batch 400 Loss: 0.516123\n",
            "\tTraining batch 401 Loss: 0.263646\n",
            "\tTraining batch 402 Loss: 0.852529\n",
            "\tTraining batch 403 Loss: 0.400564\n",
            "\tTraining batch 404 Loss: 0.469119\n",
            "\tTraining batch 405 Loss: 0.405755\n",
            "\tTraining batch 406 Loss: 0.462123\n",
            "\tTraining batch 407 Loss: 0.240976\n",
            "\tTraining batch 408 Loss: 0.516865\n",
            "\tTraining batch 409 Loss: 0.867574\n",
            "\tTraining batch 410 Loss: 0.237142\n",
            "\tTraining batch 411 Loss: 0.518116\n",
            "\tTraining batch 412 Loss: 0.957954\n",
            "\tTraining batch 413 Loss: 0.719105\n",
            "\tTraining batch 414 Loss: 0.529846\n",
            "\tTraining batch 415 Loss: 0.742704\n",
            "\tTraining batch 416 Loss: 0.545058\n",
            "\tTraining batch 417 Loss: 0.564206\n",
            "\tTraining batch 418 Loss: 0.403922\n",
            "\tTraining batch 419 Loss: 0.741290\n",
            "\tTraining batch 420 Loss: 0.505107\n",
            "\tTraining batch 421 Loss: 0.397126\n",
            "\tTraining batch 422 Loss: 0.313444\n",
            "\tTraining batch 423 Loss: 0.505282\n",
            "\tTraining batch 424 Loss: 0.635015\n",
            "\tTraining batch 425 Loss: 0.496851\n",
            "\tTraining batch 426 Loss: 0.462671\n",
            "\tTraining batch 427 Loss: 1.058915\n",
            "\tTraining batch 428 Loss: 0.671814\n",
            "\tTraining batch 429 Loss: 0.199131\n",
            "\tTraining batch 430 Loss: 1.052609\n",
            "\tTraining batch 431 Loss: 0.550610\n",
            "\tTraining batch 432 Loss: 0.296511\n",
            "\tTraining batch 433 Loss: 0.670546\n",
            "\tTraining batch 434 Loss: 0.201498\n",
            "\tTraining batch 435 Loss: 0.302174\n",
            "\tTraining batch 436 Loss: 0.266068\n",
            "\tTraining batch 437 Loss: 0.986152\n",
            "\tTraining batch 438 Loss: 0.497443\n",
            "\tTraining batch 439 Loss: 0.955916\n",
            "\tTraining batch 440 Loss: 0.688716\n",
            "\tTraining batch 441 Loss: 0.443746\n",
            "\tTraining batch 442 Loss: 0.546688\n",
            "\tTraining batch 443 Loss: 0.894518\n",
            "\tTraining batch 444 Loss: 0.280878\n",
            "\tTraining batch 445 Loss: 0.358944\n",
            "\tTraining batch 446 Loss: 0.984912\n",
            "\tTraining batch 447 Loss: 0.825145\n",
            "\tTraining batch 448 Loss: 0.712168\n",
            "\tTraining batch 449 Loss: 0.628886\n",
            "\tTraining batch 450 Loss: 0.617885\n",
            "\tTraining batch 451 Loss: 0.812128\n",
            "\tTraining batch 452 Loss: 0.647604\n",
            "\tTraining batch 453 Loss: 0.490640\n",
            "\tTraining batch 454 Loss: 0.741018\n",
            "\tTraining batch 455 Loss: 0.495924\n",
            "\tTraining batch 456 Loss: 0.568343\n",
            "\tTraining batch 457 Loss: 0.799247\n",
            "\tTraining batch 458 Loss: 0.663955\n",
            "\tTraining batch 459 Loss: 0.314290\n",
            "\tTraining batch 460 Loss: 0.783536\n",
            "\tTraining batch 461 Loss: 0.364977\n",
            "\tTraining batch 462 Loss: 1.372781\n",
            "\tTraining batch 463 Loss: 0.504173\n",
            "\tTraining batch 464 Loss: 0.234717\n",
            "\tTraining batch 465 Loss: 0.891956\n",
            "\tTraining batch 466 Loss: 0.542388\n",
            "\tTraining batch 467 Loss: 0.528594\n",
            "\tTraining batch 468 Loss: 0.903973\n",
            "\tTraining batch 469 Loss: 0.665706\n",
            "\tTraining batch 470 Loss: 0.746275\n",
            "\tTraining batch 471 Loss: 0.340146\n",
            "\tTraining batch 472 Loss: 0.128556\n",
            "\tTraining batch 473 Loss: 0.964768\n",
            "\tTraining batch 474 Loss: 0.645283\n",
            "\tTraining batch 475 Loss: 0.965198\n",
            "\tTraining batch 476 Loss: 0.302312\n",
            "\tTraining batch 477 Loss: 0.429390\n",
            "\tTraining batch 478 Loss: 0.570894\n",
            "\tTraining batch 479 Loss: 0.832512\n",
            "\tTraining batch 480 Loss: 0.584733\n",
            "\tTraining batch 481 Loss: 0.538753\n",
            "\tTraining batch 482 Loss: 0.311421\n",
            "\tTraining batch 483 Loss: 0.489845\n",
            "\tTraining batch 484 Loss: 0.286430\n",
            "\tTraining batch 485 Loss: 0.475789\n",
            "\tTraining batch 486 Loss: 0.570887\n",
            "\tTraining batch 487 Loss: 0.541911\n",
            "\tTraining batch 488 Loss: 0.845977\n",
            "\tTraining batch 489 Loss: 0.679422\n",
            "\tTraining batch 490 Loss: 0.620334\n",
            "\tTraining batch 491 Loss: 0.424334\n",
            "\tTraining batch 492 Loss: 0.594460\n",
            "\tTraining batch 493 Loss: 0.559866\n",
            "\tTraining batch 494 Loss: 0.633058\n",
            "\tTraining batch 495 Loss: 0.565200\n",
            "\tTraining batch 496 Loss: 0.521827\n",
            "\tTraining batch 497 Loss: 0.615960\n",
            "\tTraining batch 498 Loss: 0.441649\n",
            "\tTraining batch 499 Loss: 0.314731\n",
            "\tTraining batch 500 Loss: 0.943292\n",
            "\tTraining batch 501 Loss: 0.094571\n",
            "\tTraining batch 502 Loss: 0.924992\n",
            "\tTraining batch 503 Loss: 0.247998\n",
            "\tTraining batch 504 Loss: 0.378983\n",
            "\tTraining batch 505 Loss: 0.369494\n",
            "\tTraining batch 506 Loss: 0.396586\n",
            "\tTraining batch 507 Loss: 0.592522\n",
            "\tTraining batch 508 Loss: 0.378437\n",
            "\tTraining batch 509 Loss: 0.833157\n",
            "\tTraining batch 510 Loss: 0.441567\n",
            "\tTraining batch 511 Loss: 1.161415\n",
            "\tTraining batch 512 Loss: 0.578957\n",
            "\tTraining batch 513 Loss: 0.678772\n",
            "\tTraining batch 514 Loss: 0.330927\n",
            "\tTraining batch 515 Loss: 0.888906\n",
            "\tTraining batch 516 Loss: 0.417162\n",
            "\tTraining batch 517 Loss: 0.277578\n",
            "\tTraining batch 518 Loss: 0.266175\n",
            "\tTraining batch 519 Loss: 0.428643\n",
            "\tTraining batch 520 Loss: 0.767330\n",
            "\tTraining batch 521 Loss: 0.456254\n",
            "\tTraining batch 522 Loss: 0.577318\n",
            "\tTraining batch 523 Loss: 0.908366\n",
            "\tTraining batch 524 Loss: 0.619745\n",
            "\tTraining batch 525 Loss: 0.560084\n",
            "\tTraining batch 526 Loss: 0.653256\n",
            "\tTraining batch 527 Loss: 0.407046\n",
            "\tTraining batch 528 Loss: 0.514178\n",
            "\tTraining batch 529 Loss: 0.563703\n",
            "\tTraining batch 530 Loss: 0.893416\n",
            "\tTraining batch 531 Loss: 0.559876\n",
            "\tTraining batch 532 Loss: 0.461657\n",
            "\tTraining batch 533 Loss: 0.700043\n",
            "\tTraining batch 534 Loss: 0.483189\n",
            "\tTraining batch 535 Loss: 0.755586\n",
            "\tTraining batch 536 Loss: 0.630138\n",
            "\tTraining batch 537 Loss: 0.646104\n",
            "\tTraining batch 538 Loss: 0.762328\n",
            "\tTraining batch 539 Loss: 0.209800\n",
            "\tTraining batch 540 Loss: 0.325932\n",
            "\tTraining batch 541 Loss: 0.361857\n",
            "\tTraining batch 542 Loss: 0.375779\n",
            "\tTraining batch 543 Loss: 0.714780\n",
            "\tTraining batch 544 Loss: 0.413005\n",
            "\tTraining batch 545 Loss: 0.527377\n",
            "\tTraining batch 546 Loss: 0.579879\n",
            "\tTraining batch 547 Loss: 0.918878\n",
            "\tTraining batch 548 Loss: 0.797163\n",
            "\tTraining batch 549 Loss: 0.647882\n",
            "\tTraining batch 550 Loss: 0.603381\n",
            "\tTraining batch 551 Loss: 0.516061\n",
            "\tTraining batch 552 Loss: 0.592862\n",
            "\tTraining batch 553 Loss: 0.577551\n",
            "\tTraining batch 554 Loss: 0.753636\n",
            "\tTraining batch 555 Loss: 0.486737\n",
            "\tTraining batch 556 Loss: 0.406933\n",
            "\tTraining batch 557 Loss: 0.530042\n",
            "\tTraining batch 558 Loss: 0.650346\n",
            "\tTraining batch 559 Loss: 0.521202\n",
            "\tTraining batch 560 Loss: 0.856717\n",
            "\tTraining batch 561 Loss: 0.392321\n",
            "\tTraining batch 562 Loss: 1.011182\n",
            "\tTraining batch 563 Loss: 0.479471\n",
            "\tTraining batch 564 Loss: 0.471798\n",
            "\tTraining batch 565 Loss: 0.604192\n",
            "\tTraining batch 566 Loss: 0.461644\n",
            "\tTraining batch 567 Loss: 0.720628\n",
            "\tTraining batch 568 Loss: 0.574419\n",
            "\tTraining batch 569 Loss: 0.431575\n",
            "\tTraining batch 570 Loss: 0.581569\n",
            "\tTraining batch 571 Loss: 0.553968\n",
            "\tTraining batch 572 Loss: 0.390379\n",
            "\tTraining batch 573 Loss: 0.749627\n",
            "\tTraining batch 574 Loss: 0.444571\n",
            "\tTraining batch 575 Loss: 0.763173\n",
            "\tTraining batch 576 Loss: 1.322981\n",
            "\tTraining batch 577 Loss: 0.757915\n",
            "\tTraining batch 578 Loss: 0.493125\n",
            "\tTraining batch 579 Loss: 0.859782\n",
            "\tTraining batch 580 Loss: 0.790223\n",
            "\tTraining batch 581 Loss: 1.033115\n",
            "\tTraining batch 582 Loss: 0.533256\n",
            "\tTraining batch 583 Loss: 0.934945\n",
            "\tTraining batch 584 Loss: 0.506701\n",
            "\tTraining batch 585 Loss: 0.215984\n",
            "\tTraining batch 586 Loss: 0.533705\n",
            "\tTraining batch 587 Loss: 0.857911\n",
            "\tTraining batch 588 Loss: 0.592955\n",
            "\tTraining batch 589 Loss: 0.901512\n",
            "\tTraining batch 590 Loss: 1.034657\n",
            "\tTraining batch 591 Loss: 0.444796\n",
            "\tTraining batch 592 Loss: 0.668631\n",
            "\tTraining batch 593 Loss: 0.606127\n",
            "\tTraining batch 594 Loss: 0.469148\n",
            "\tTraining batch 595 Loss: 0.324775\n",
            "\tTraining batch 596 Loss: 0.416532\n",
            "\tTraining batch 597 Loss: 0.286069\n",
            "\tTraining batch 598 Loss: 0.373068\n",
            "\tTraining batch 599 Loss: 0.536337\n",
            "\tTraining batch 600 Loss: 0.419844\n",
            "\tTraining batch 601 Loss: 0.498185\n",
            "\tTraining batch 602 Loss: 0.380385\n",
            "\tTraining batch 603 Loss: 0.592390\n",
            "\tTraining batch 604 Loss: 0.835766\n",
            "\tTraining batch 605 Loss: 0.571499\n",
            "\tTraining batch 606 Loss: 0.586874\n",
            "\tTraining batch 607 Loss: 0.411030\n",
            "\tTraining batch 608 Loss: 0.650076\n",
            "\tTraining batch 609 Loss: 0.445806\n",
            "\tTraining batch 610 Loss: 0.661352\n",
            "\tTraining batch 611 Loss: 0.667429\n",
            "\tTraining batch 612 Loss: 0.341360\n",
            "\tTraining batch 613 Loss: 0.438642\n",
            "\tTraining batch 614 Loss: 0.863969\n",
            "\tTraining batch 615 Loss: 0.691043\n",
            "\tTraining batch 616 Loss: 1.009115\n",
            "\tTraining batch 617 Loss: 0.595843\n",
            "\tTraining batch 618 Loss: 0.657533\n",
            "\tTraining batch 619 Loss: 0.578113\n",
            "\tTraining batch 620 Loss: 0.629822\n",
            "\tTraining batch 621 Loss: 0.686943\n",
            "\tTraining batch 622 Loss: 0.515305\n",
            "\tTraining batch 623 Loss: 0.809413\n",
            "\tTraining batch 624 Loss: 0.638940\n",
            "\tTraining batch 625 Loss: 0.718219\n",
            "\tTraining batch 626 Loss: 0.908226\n",
            "\tTraining batch 627 Loss: 0.512395\n",
            "\tTraining batch 628 Loss: 0.403590\n",
            "\tTraining batch 629 Loss: 0.491013\n",
            "\tTraining batch 630 Loss: 0.677380\n",
            "\tTraining batch 631 Loss: 0.608952\n",
            "\tTraining batch 632 Loss: 0.668039\n",
            "\tTraining batch 633 Loss: 0.589578\n",
            "\tTraining batch 634 Loss: 0.360535\n",
            "\tTraining batch 635 Loss: 0.392369\n",
            "\tTraining batch 636 Loss: 0.707096\n",
            "\tTraining batch 637 Loss: 0.697117\n",
            "\tTraining batch 638 Loss: 0.633921\n",
            "\tTraining batch 639 Loss: 0.735984\n",
            "\tTraining batch 640 Loss: 0.306667\n",
            "\tTraining batch 641 Loss: 0.838969\n",
            "\tTraining batch 642 Loss: 0.658500\n",
            "\tTraining batch 643 Loss: 0.333122\n",
            "\tTraining batch 644 Loss: 0.812232\n",
            "\tTraining batch 645 Loss: 0.466523\n",
            "\tTraining batch 646 Loss: 0.497194\n",
            "\tTraining batch 647 Loss: 0.394305\n",
            "\tTraining batch 648 Loss: 0.464146\n",
            "\tTraining batch 649 Loss: 0.758666\n",
            "\tTraining batch 650 Loss: 0.450342\n",
            "\tTraining batch 651 Loss: 0.917147\n",
            "\tTraining batch 652 Loss: 0.567652\n",
            "\tTraining batch 653 Loss: 0.613807\n",
            "\tTraining batch 654 Loss: 0.779523\n",
            "\tTraining batch 655 Loss: 0.696090\n",
            "\tTraining batch 656 Loss: 0.338568\n",
            "\tTraining batch 657 Loss: 0.225541\n",
            "\tTraining batch 658 Loss: 0.539662\n",
            "\tTraining batch 659 Loss: 0.455678\n",
            "\tTraining batch 660 Loss: 0.548889\n",
            "\tTraining batch 661 Loss: 0.480271\n",
            "\tTraining batch 662 Loss: 0.713665\n",
            "\tTraining batch 663 Loss: 0.601101\n",
            "\tTraining batch 664 Loss: 0.519450\n",
            "\tTraining batch 665 Loss: 0.704308\n",
            "\tTraining batch 666 Loss: 1.050192\n",
            "\tTraining batch 667 Loss: 0.267740\n",
            "\tTraining batch 668 Loss: 0.381126\n",
            "\tTraining batch 669 Loss: 0.619175\n",
            "\tTraining batch 670 Loss: 0.810197\n",
            "\tTraining batch 671 Loss: 0.824709\n",
            "\tTraining batch 672 Loss: 0.575389\n",
            "\tTraining batch 673 Loss: 0.383474\n",
            "\tTraining batch 674 Loss: 0.666894\n",
            "\tTraining batch 675 Loss: 0.376591\n",
            "\tTraining batch 676 Loss: 0.446660\n",
            "\tTraining batch 677 Loss: 0.561200\n",
            "\tTraining batch 678 Loss: 0.494546\n",
            "\tTraining batch 679 Loss: 0.382787\n",
            "\tTraining batch 680 Loss: 0.446744\n",
            "\tTraining batch 681 Loss: 0.641188\n",
            "\tTraining batch 682 Loss: 0.800306\n",
            "\tTraining batch 683 Loss: 0.718840\n",
            "\tTraining batch 684 Loss: 0.932721\n",
            "\tTraining batch 685 Loss: 0.657245\n",
            "\tTraining batch 686 Loss: 0.412535\n",
            "\tTraining batch 687 Loss: 0.775818\n",
            "\tTraining batch 688 Loss: 0.637023\n",
            "\tTraining batch 689 Loss: 0.696315\n",
            "\tTraining batch 690 Loss: 0.529902\n",
            "\tTraining batch 691 Loss: 0.737431\n",
            "\tTraining batch 692 Loss: 0.477693\n",
            "\tTraining batch 693 Loss: 0.169875\n",
            "\tTraining batch 694 Loss: 0.670906\n",
            "\tTraining batch 695 Loss: 0.699689\n",
            "\tTraining batch 696 Loss: 0.823899\n",
            "\tTraining batch 697 Loss: 0.817075\n",
            "\tTraining batch 698 Loss: 0.360304\n",
            "\tTraining batch 699 Loss: 0.491144\n",
            "\tTraining batch 700 Loss: 0.672430\n",
            "\tTraining batch 701 Loss: 0.552076\n",
            "\tTraining batch 702 Loss: 0.838552\n",
            "\tTraining batch 703 Loss: 0.459444\n",
            "\tTraining batch 704 Loss: 0.711923\n",
            "\tTraining batch 705 Loss: 0.244574\n",
            "\tTraining batch 706 Loss: 0.820914\n",
            "\tTraining batch 707 Loss: 0.494112\n",
            "\tTraining batch 708 Loss: 0.447677\n",
            "\tTraining batch 709 Loss: 0.673556\n",
            "\tTraining batch 710 Loss: 1.148738\n",
            "\tTraining batch 711 Loss: 0.368380\n",
            "\tTraining batch 712 Loss: 0.525631\n",
            "\tTraining batch 713 Loss: 0.550050\n",
            "\tTraining batch 714 Loss: 0.696919\n",
            "\tTraining batch 715 Loss: 0.548584\n",
            "\tTraining batch 716 Loss: 0.515187\n",
            "\tTraining batch 717 Loss: 0.612107\n",
            "\tTraining batch 718 Loss: 0.636544\n",
            "\tTraining batch 719 Loss: 0.533237\n",
            "\tTraining batch 720 Loss: 0.539336\n",
            "\tTraining batch 721 Loss: 0.452386\n",
            "\tTraining batch 722 Loss: 0.699837\n",
            "\tTraining batch 723 Loss: 0.303078\n",
            "\tTraining batch 724 Loss: 0.462167\n",
            "\tTraining batch 725 Loss: 0.590926\n",
            "\tTraining batch 726 Loss: 0.627145\n",
            "\tTraining batch 727 Loss: 0.620715\n",
            "\tTraining batch 728 Loss: 0.577390\n",
            "\tTraining batch 729 Loss: 0.530360\n",
            "\tTraining batch 730 Loss: 0.433523\n",
            "\tTraining batch 731 Loss: 0.473425\n",
            "\tTraining batch 732 Loss: 0.381950\n",
            "\tTraining batch 733 Loss: 0.811314\n",
            "\tTraining batch 734 Loss: 0.475840\n",
            "\tTraining batch 735 Loss: 0.574408\n",
            "\tTraining batch 736 Loss: 0.886808\n",
            "\tTraining batch 737 Loss: 0.410770\n",
            "\tTraining batch 738 Loss: 0.389319\n",
            "\tTraining batch 739 Loss: 0.695917\n",
            "\tTraining batch 740 Loss: 0.523408\n",
            "\tTraining batch 741 Loss: 0.709540\n",
            "\tTraining batch 742 Loss: 0.314972\n",
            "\tTraining batch 743 Loss: 0.463675\n",
            "\tTraining batch 744 Loss: 0.598298\n",
            "\tTraining batch 745 Loss: 0.590040\n",
            "\tTraining batch 746 Loss: 0.648815\n",
            "\tTraining batch 747 Loss: 0.768029\n",
            "\tTraining batch 748 Loss: 0.600010\n",
            "\tTraining batch 749 Loss: 0.786746\n",
            "\tTraining batch 750 Loss: 0.473550\n",
            "\tTraining batch 751 Loss: 0.195332\n",
            "\tTraining batch 752 Loss: 0.393171\n",
            "\tTraining batch 753 Loss: 0.316492\n",
            "\tTraining batch 754 Loss: 0.506760\n",
            "\tTraining batch 755 Loss: 0.398109\n",
            "\tTraining batch 756 Loss: 0.613613\n",
            "\tTraining batch 757 Loss: 0.925262\n",
            "\tTraining batch 758 Loss: 0.247183\n",
            "\tTraining batch 759 Loss: 0.308276\n",
            "\tTraining batch 760 Loss: 0.683246\n",
            "\tTraining batch 761 Loss: 0.784422\n",
            "\tTraining batch 762 Loss: 0.535880\n",
            "\tTraining batch 763 Loss: 0.301546\n",
            "\tTraining batch 764 Loss: 0.434062\n",
            "\tTraining batch 765 Loss: 0.454369\n",
            "\tTraining batch 766 Loss: 0.801249\n",
            "\tTraining batch 767 Loss: 0.572620\n",
            "\tTraining batch 768 Loss: 0.651064\n",
            "\tTraining batch 769 Loss: 0.684432\n",
            "\tTraining batch 770 Loss: 0.240863\n",
            "\tTraining batch 771 Loss: 0.422028\n",
            "\tTraining batch 772 Loss: 0.511544\n",
            "\tTraining batch 773 Loss: 0.500539\n",
            "\tTraining batch 774 Loss: 0.809431\n",
            "\tTraining batch 775 Loss: 0.519993\n",
            "\tTraining batch 776 Loss: 1.054960\n",
            "\tTraining batch 777 Loss: 0.231645\n",
            "\tTraining batch 778 Loss: 0.228741\n",
            "\tTraining batch 779 Loss: 0.611020\n",
            "\tTraining batch 780 Loss: 0.638854\n",
            "\tTraining batch 781 Loss: 0.800012\n",
            "\tTraining batch 782 Loss: 0.531105\n",
            "\tTraining batch 783 Loss: 0.379317\n",
            "\tTraining batch 784 Loss: 0.685690\n",
            "\tTraining batch 785 Loss: 0.518700\n",
            "\tTraining batch 786 Loss: 0.879651\n",
            "\tTraining batch 787 Loss: 0.884876\n",
            "\tTraining batch 788 Loss: 0.857442\n",
            "\tTraining batch 789 Loss: 0.402884\n",
            "\tTraining batch 790 Loss: 0.522059\n",
            "\tTraining batch 791 Loss: 0.695750\n",
            "\tTraining batch 792 Loss: 0.858001\n",
            "\tTraining batch 793 Loss: 0.412359\n",
            "\tTraining batch 794 Loss: 0.620274\n",
            "\tTraining batch 795 Loss: 0.606560\n",
            "\tTraining batch 796 Loss: 1.011687\n",
            "\tTraining batch 797 Loss: 0.327812\n",
            "\tTraining batch 798 Loss: 0.736356\n",
            "\tTraining batch 799 Loss: 0.503569\n",
            "\tTraining batch 800 Loss: 0.714803\n",
            "\tTraining batch 801 Loss: 0.556772\n",
            "\tTraining batch 802 Loss: 0.488012\n",
            "\tTraining batch 803 Loss: 0.498167\n",
            "\tTraining batch 804 Loss: 0.446955\n",
            "\tTraining batch 805 Loss: 0.698027\n",
            "\tTraining batch 806 Loss: 0.500810\n",
            "\tTraining batch 807 Loss: 0.485213\n",
            "\tTraining batch 808 Loss: 0.396342\n",
            "\tTraining batch 809 Loss: 0.671714\n",
            "\tTraining batch 810 Loss: 0.427960\n",
            "\tTraining batch 811 Loss: 0.341142\n",
            "\tTraining batch 812 Loss: 0.724761\n",
            "\tTraining batch 813 Loss: 0.609813\n",
            "\tTraining batch 814 Loss: 0.330248\n",
            "\tTraining batch 815 Loss: 0.573288\n",
            "\tTraining batch 816 Loss: 0.537484\n",
            "\tTraining batch 817 Loss: 0.441621\n",
            "\tTraining batch 818 Loss: 0.366825\n",
            "\tTraining batch 819 Loss: 0.686206\n",
            "\tTraining batch 820 Loss: 0.797869\n",
            "\tTraining batch 821 Loss: 0.483039\n",
            "\tTraining batch 822 Loss: 0.756003\n",
            "\tTraining batch 823 Loss: 0.527793\n",
            "\tTraining batch 824 Loss: 0.977542\n",
            "\tTraining batch 825 Loss: 0.582057\n",
            "\tTraining batch 826 Loss: 0.356970\n",
            "\tTraining batch 827 Loss: 0.764084\n",
            "\tTraining batch 828 Loss: 0.598979\n",
            "\tTraining batch 829 Loss: 0.717660\n",
            "\tTraining batch 830 Loss: 0.972611\n",
            "\tTraining batch 831 Loss: 0.641028\n",
            "\tTraining batch 832 Loss: 0.581683\n",
            "\tTraining batch 833 Loss: 0.843231\n",
            "\tTraining batch 834 Loss: 0.752229\n",
            "\tTraining batch 835 Loss: 0.381159\n",
            "\tTraining batch 836 Loss: 0.751050\n",
            "\tTraining batch 837 Loss: 0.388844\n",
            "\tTraining batch 838 Loss: 0.462782\n",
            "\tTraining batch 839 Loss: 0.611246\n",
            "\tTraining batch 840 Loss: 0.565536\n",
            "\tTraining batch 841 Loss: 0.823355\n",
            "\tTraining batch 842 Loss: 0.214684\n",
            "\tTraining batch 843 Loss: 0.381208\n",
            "\tTraining batch 844 Loss: 0.652480\n",
            "\tTraining batch 845 Loss: 0.518006\n",
            "\tTraining batch 846 Loss: 0.722650\n",
            "\tTraining batch 847 Loss: 0.567625\n",
            "\tTraining batch 848 Loss: 0.475324\n",
            "\tTraining batch 849 Loss: 0.395294\n",
            "\tTraining batch 850 Loss: 0.618429\n",
            "\tTraining batch 851 Loss: 0.334279\n",
            "\tTraining batch 852 Loss: 0.647918\n",
            "\tTraining batch 853 Loss: 0.599929\n",
            "\tTraining batch 854 Loss: 0.575959\n",
            "\tTraining batch 855 Loss: 0.581500\n",
            "\tTraining batch 856 Loss: 0.591158\n",
            "\tTraining batch 857 Loss: 0.259306\n",
            "\tTraining batch 858 Loss: 0.524805\n",
            "\tTraining batch 859 Loss: 0.414963\n",
            "\tTraining batch 860 Loss: 0.456501\n",
            "\tTraining batch 861 Loss: 0.486154\n",
            "\tTraining batch 862 Loss: 0.639568\n",
            "\tTraining batch 863 Loss: 0.720402\n",
            "\tTraining batch 864 Loss: 0.413222\n",
            "\tTraining batch 865 Loss: 0.485945\n",
            "\tTraining batch 866 Loss: 0.746138\n",
            "\tTraining batch 867 Loss: 0.370473\n",
            "\tTraining batch 868 Loss: 0.616555\n",
            "\tTraining batch 869 Loss: 0.535651\n",
            "\tTraining batch 870 Loss: 0.515585\n",
            "\tTraining batch 871 Loss: 0.925784\n",
            "\tTraining batch 872 Loss: 0.590341\n",
            "\tTraining batch 873 Loss: 0.717101\n",
            "\tTraining batch 874 Loss: 0.858606\n",
            "\tTraining batch 875 Loss: 0.870716\n",
            "\tTraining batch 876 Loss: 0.570545\n",
            "\tTraining batch 877 Loss: 0.366835\n",
            "\tTraining batch 878 Loss: 0.609623\n",
            "\tTraining batch 879 Loss: 0.618414\n",
            "\tTraining batch 880 Loss: 0.866353\n",
            "\tTraining batch 881 Loss: 0.255012\n",
            "\tTraining batch 882 Loss: 0.456410\n",
            "\tTraining batch 883 Loss: 0.552958\n",
            "\tTraining batch 884 Loss: 0.679121\n",
            "\tTraining batch 885 Loss: 0.762220\n",
            "\tTraining batch 886 Loss: 0.503745\n",
            "\tTraining batch 887 Loss: 0.276878\n",
            "\tTraining batch 888 Loss: 0.437079\n",
            "\tTraining batch 889 Loss: 0.733354\n",
            "\tTraining batch 890 Loss: 0.705891\n",
            "\tTraining batch 891 Loss: 0.380727\n",
            "\tTraining batch 892 Loss: 0.710023\n",
            "\tTraining batch 893 Loss: 0.586124\n",
            "\tTraining batch 894 Loss: 0.565490\n",
            "\tTraining batch 895 Loss: 0.365825\n",
            "\tTraining batch 896 Loss: 0.380821\n",
            "\tTraining batch 897 Loss: 0.504291\n",
            "\tTraining batch 898 Loss: 0.206563\n",
            "\tTraining batch 899 Loss: 0.328430\n",
            "\tTraining batch 900 Loss: 0.281549\n",
            "\tTraining batch 901 Loss: 0.736180\n",
            "\tTraining batch 902 Loss: 0.467551\n",
            "\tTraining batch 903 Loss: 0.851144\n",
            "\tTraining batch 904 Loss: 0.411212\n",
            "\tTraining batch 905 Loss: 0.658665\n",
            "\tTraining batch 906 Loss: 0.390094\n",
            "\tTraining batch 907 Loss: 0.459714\n",
            "\tTraining batch 908 Loss: 1.000316\n",
            "\tTraining batch 909 Loss: 0.997401\n",
            "\tTraining batch 910 Loss: 0.715700\n",
            "\tTraining batch 911 Loss: 0.559261\n",
            "\tTraining batch 912 Loss: 0.412333\n",
            "\tTraining batch 913 Loss: 0.314871\n",
            "\tTraining batch 914 Loss: 0.467980\n",
            "\tTraining batch 915 Loss: 1.066445\n",
            "\tTraining batch 916 Loss: 0.593213\n",
            "\tTraining batch 917 Loss: 0.361680\n",
            "\tTraining batch 918 Loss: 0.457634\n",
            "\tTraining batch 919 Loss: 0.634717\n",
            "\tTraining batch 920 Loss: 0.607706\n",
            "\tTraining batch 921 Loss: 0.683013\n",
            "\tTraining batch 922 Loss: 0.521027\n",
            "\tTraining batch 923 Loss: 0.380845\n",
            "\tTraining batch 924 Loss: 0.980308\n",
            "\tTraining batch 925 Loss: 0.838047\n",
            "\tTraining batch 926 Loss: 0.502150\n",
            "\tTraining batch 927 Loss: 0.471279\n",
            "\tTraining batch 928 Loss: 0.411887\n",
            "\tTraining batch 929 Loss: 0.926723\n",
            "\tTraining batch 930 Loss: 0.455211\n",
            "\tTraining batch 931 Loss: 0.624781\n",
            "\tTraining batch 932 Loss: 0.575611\n",
            "\tTraining batch 933 Loss: 0.324234\n",
            "\tTraining batch 934 Loss: 1.057510\n",
            "\tTraining batch 935 Loss: 0.222678\n",
            "\tTraining batch 936 Loss: 0.559552\n",
            "\tTraining batch 937 Loss: 0.630248\n",
            "\tTraining batch 938 Loss: 0.328118\n",
            "\tTraining batch 939 Loss: 0.973724\n",
            "\tTraining batch 940 Loss: 1.344618\n",
            "\tTraining batch 941 Loss: 0.491495\n",
            "\tTraining batch 942 Loss: 0.690720\n",
            "\tTraining batch 943 Loss: 0.427038\n",
            "\tTraining batch 944 Loss: 0.602774\n",
            "\tTraining batch 945 Loss: 0.745335\n",
            "\tTraining batch 946 Loss: 0.355283\n",
            "\tTraining batch 947 Loss: 0.378036\n",
            "\tTraining batch 948 Loss: 0.587647\n",
            "\tTraining batch 949 Loss: 0.454485\n",
            "\tTraining batch 950 Loss: 0.318085\n",
            "\tTraining batch 951 Loss: 0.735936\n",
            "\tTraining batch 952 Loss: 0.392665\n",
            "\tTraining batch 953 Loss: 1.068616\n",
            "\tTraining batch 954 Loss: 0.183779\n",
            "\tTraining batch 955 Loss: 0.919076\n",
            "\tTraining batch 956 Loss: 0.663910\n",
            "\tTraining batch 957 Loss: 0.553730\n",
            "\tTraining batch 958 Loss: 0.499762\n",
            "\tTraining batch 959 Loss: 0.645564\n",
            "\tTraining batch 960 Loss: 0.431415\n",
            "\tTraining batch 961 Loss: 0.762340\n",
            "\tTraining batch 962 Loss: 0.794562\n",
            "\tTraining batch 963 Loss: 0.639785\n",
            "\tTraining batch 964 Loss: 0.569530\n",
            "\tTraining batch 965 Loss: 0.517527\n",
            "\tTraining batch 966 Loss: 0.527886\n",
            "\tTraining batch 967 Loss: 0.573081\n",
            "\tTraining batch 968 Loss: 0.436834\n",
            "\tTraining batch 969 Loss: 0.374352\n",
            "\tTraining batch 970 Loss: 0.728059\n",
            "\tTraining batch 971 Loss: 0.407046\n",
            "\tTraining batch 972 Loss: 0.685428\n",
            "\tTraining batch 973 Loss: 0.411775\n",
            "\tTraining batch 974 Loss: 0.951397\n",
            "\tTraining batch 975 Loss: 0.605512\n",
            "\tTraining batch 976 Loss: 0.273658\n",
            "\tTraining batch 977 Loss: 0.687975\n",
            "\tTraining batch 978 Loss: 0.838112\n",
            "\tTraining batch 979 Loss: 0.818240\n",
            "\tTraining batch 980 Loss: 0.506726\n",
            "\tTraining batch 981 Loss: 0.443968\n",
            "\tTraining batch 982 Loss: 0.383165\n",
            "\tTraining batch 983 Loss: 0.334764\n",
            "\tTraining batch 984 Loss: 0.910354\n",
            "\tTraining batch 985 Loss: 0.937469\n",
            "\tTraining batch 986 Loss: 1.135725\n",
            "\tTraining batch 987 Loss: 0.887614\n",
            "\tTraining batch 988 Loss: 0.450839\n",
            "\tTraining batch 989 Loss: 0.354070\n",
            "\tTraining batch 990 Loss: 0.848396\n",
            "\tTraining batch 991 Loss: 0.501700\n",
            "\tTraining batch 992 Loss: 0.636857\n",
            "\tTraining batch 993 Loss: 0.666987\n",
            "\tTraining batch 994 Loss: 0.459875\n",
            "\tTraining batch 995 Loss: 0.350454\n",
            "\tTraining batch 996 Loss: 0.821310\n",
            "\tTraining batch 997 Loss: 0.545745\n",
            "\tTraining batch 998 Loss: 0.459073\n",
            "\tTraining batch 999 Loss: 0.435989\n",
            "\tTraining batch 1000 Loss: 0.812043\n",
            "\tTraining batch 1001 Loss: 0.403482\n",
            "\tTraining batch 1002 Loss: 0.623688\n",
            "\tTraining batch 1003 Loss: 0.451745\n",
            "\tTraining batch 1004 Loss: 0.637282\n",
            "\tTraining batch 1005 Loss: 0.688088\n",
            "\tTraining batch 1006 Loss: 0.592724\n",
            "\tTraining batch 1007 Loss: 0.446737\n",
            "\tTraining batch 1008 Loss: 0.454299\n",
            "\tTraining batch 1009 Loss: 0.381961\n",
            "\tTraining batch 1010 Loss: 0.325117\n",
            "\tTraining batch 1011 Loss: 0.783785\n",
            "\tTraining batch 1012 Loss: 0.517799\n",
            "\tTraining batch 1013 Loss: 0.478744\n",
            "\tTraining batch 1014 Loss: 1.071480\n",
            "\tTraining batch 1015 Loss: 0.575313\n",
            "\tTraining batch 1016 Loss: 0.525924\n",
            "\tTraining batch 1017 Loss: 0.630322\n",
            "\tTraining batch 1018 Loss: 0.295767\n",
            "\tTraining batch 1019 Loss: 0.452702\n",
            "\tTraining batch 1020 Loss: 0.431201\n",
            "\tTraining batch 1021 Loss: 0.654736\n",
            "\tTraining batch 1022 Loss: 0.507261\n",
            "\tTraining batch 1023 Loss: 0.446279\n",
            "\tTraining batch 1024 Loss: 0.441031\n",
            "\tTraining batch 1025 Loss: 0.695404\n",
            "\tTraining batch 1026 Loss: 0.708407\n",
            "\tTraining batch 1027 Loss: 0.465206\n",
            "\tTraining batch 1028 Loss: 0.314610\n",
            "\tTraining batch 1029 Loss: 0.722962\n",
            "\tTraining batch 1030 Loss: 0.344070\n",
            "\tTraining batch 1031 Loss: 0.306600\n",
            "\tTraining batch 1032 Loss: 0.747547\n",
            "\tTraining batch 1033 Loss: 0.555067\n",
            "\tTraining batch 1034 Loss: 0.686001\n",
            "\tTraining batch 1035 Loss: 0.615267\n",
            "\tTraining batch 1036 Loss: 0.473944\n",
            "\tTraining batch 1037 Loss: 0.580495\n",
            "\tTraining batch 1038 Loss: 0.991740\n",
            "\tTraining batch 1039 Loss: 0.290769\n",
            "\tTraining batch 1040 Loss: 0.673890\n",
            "\tTraining batch 1041 Loss: 0.510033\n",
            "\tTraining batch 1042 Loss: 0.375699\n",
            "\tTraining batch 1043 Loss: 0.443339\n",
            "\tTraining batch 1044 Loss: 0.896473\n",
            "\tTraining batch 1045 Loss: 0.409281\n",
            "\tTraining batch 1046 Loss: 0.253340\n",
            "\tTraining batch 1047 Loss: 0.230740\n",
            "\tTraining batch 1048 Loss: 0.568399\n",
            "\tTraining batch 1049 Loss: 0.370961\n",
            "\tTraining batch 1050 Loss: 0.709513\n",
            "\tTraining batch 1051 Loss: 0.503564\n",
            "\tTraining batch 1052 Loss: 0.524138\n",
            "\tTraining batch 1053 Loss: 0.984846\n",
            "\tTraining batch 1054 Loss: 0.498604\n",
            "\tTraining batch 1055 Loss: 0.444375\n",
            "\tTraining batch 1056 Loss: 0.241407\n",
            "\tTraining batch 1057 Loss: 0.988691\n",
            "\tTraining batch 1058 Loss: 0.535220\n",
            "\tTraining batch 1059 Loss: 0.691746\n",
            "\tTraining batch 1060 Loss: 0.264022\n",
            "\tTraining batch 1061 Loss: 0.289656\n",
            "\tTraining batch 1062 Loss: 0.472620\n",
            "\tTraining batch 1063 Loss: 1.091003\n",
            "\tTraining batch 1064 Loss: 0.636667\n",
            "\tTraining batch 1065 Loss: 0.287383\n",
            "\tTraining batch 1066 Loss: 0.781167\n",
            "\tTraining batch 1067 Loss: 0.371277\n",
            "\tTraining batch 1068 Loss: 0.658150\n",
            "\tTraining batch 1069 Loss: 0.517684\n",
            "\tTraining batch 1070 Loss: 0.514459\n",
            "\tTraining batch 1071 Loss: 0.581830\n",
            "\tTraining batch 1072 Loss: 0.505935\n",
            "\tTraining batch 1073 Loss: 0.385107\n",
            "\tTraining batch 1074 Loss: 0.391507\n",
            "\tTraining batch 1075 Loss: 0.898283\n",
            "\tTraining batch 1076 Loss: 0.521014\n",
            "\tTraining batch 1077 Loss: 1.224357\n",
            "\tTraining batch 1078 Loss: 0.366547\n",
            "\tTraining batch 1079 Loss: 0.820165\n",
            "\tTraining batch 1080 Loss: 0.567045\n",
            "\tTraining batch 1081 Loss: 0.751514\n",
            "\tTraining batch 1082 Loss: 0.259422\n",
            "\tTraining batch 1083 Loss: 0.987672\n",
            "\tTraining batch 1084 Loss: 0.398051\n",
            "\tTraining batch 1085 Loss: 0.691920\n",
            "\tTraining batch 1086 Loss: 0.715805\n",
            "\tTraining batch 1087 Loss: 0.446159\n",
            "\tTraining batch 1088 Loss: 0.546811\n",
            "\tTraining batch 1089 Loss: 0.939326\n",
            "\tTraining batch 1090 Loss: 0.546316\n",
            "\tTraining batch 1091 Loss: 0.536211\n",
            "\tTraining batch 1092 Loss: 0.582827\n",
            "\tTraining batch 1093 Loss: 0.747799\n",
            "\tTraining batch 1094 Loss: 0.520621\n",
            "\tTraining batch 1095 Loss: 0.528346\n",
            "\tTraining batch 1096 Loss: 0.345200\n",
            "\tTraining batch 1097 Loss: 1.343014\n",
            "\tTraining batch 1098 Loss: 0.382101\n",
            "\tTraining batch 1099 Loss: 0.612574\n",
            "\tTraining batch 1100 Loss: 0.525098\n",
            "\tTraining batch 1101 Loss: 0.677316\n",
            "\tTraining batch 1102 Loss: 0.526146\n",
            "\tTraining batch 1103 Loss: 0.807644\n",
            "\tTraining batch 1104 Loss: 0.579567\n",
            "\tTraining batch 1105 Loss: 0.431540\n",
            "\tTraining batch 1106 Loss: 0.502802\n",
            "\tTraining batch 1107 Loss: 0.535672\n",
            "\tTraining batch 1108 Loss: 0.590166\n",
            "\tTraining batch 1109 Loss: 0.426300\n",
            "\tTraining batch 1110 Loss: 0.494029\n",
            "\tTraining batch 1111 Loss: 0.551987\n",
            "\tTraining batch 1112 Loss: 0.340646\n",
            "\tTraining batch 1113 Loss: 0.828801\n",
            "\tTraining batch 1114 Loss: 0.293497\n",
            "\tTraining batch 1115 Loss: 0.482049\n",
            "\tTraining batch 1116 Loss: 0.567252\n",
            "\tTraining batch 1117 Loss: 0.347707\n",
            "\tTraining batch 1118 Loss: 0.594872\n",
            "\tTraining batch 1119 Loss: 0.753000\n",
            "\tTraining batch 1120 Loss: 0.526180\n",
            "\tTraining batch 1121 Loss: 0.554850\n",
            "\tTraining batch 1122 Loss: 0.580726\n",
            "\tTraining batch 1123 Loss: 0.404570\n",
            "\tTraining batch 1124 Loss: 1.163832\n",
            "\tTraining batch 1125 Loss: 0.835252\n",
            "\tTraining batch 1126 Loss: 0.553222\n",
            "\tTraining batch 1127 Loss: 0.387344\n",
            "\tTraining batch 1128 Loss: 0.446248\n",
            "\tTraining batch 1129 Loss: 0.473589\n",
            "\tTraining batch 1130 Loss: 0.556984\n",
            "\tTraining batch 1131 Loss: 0.613898\n",
            "\tTraining batch 1132 Loss: 0.540571\n",
            "\tTraining batch 1133 Loss: 0.195498\n",
            "\tTraining batch 1134 Loss: 0.689247\n",
            "\tTraining batch 1135 Loss: 0.539376\n",
            "\tTraining batch 1136 Loss: 0.930519\n",
            "\tTraining batch 1137 Loss: 0.691564\n",
            "\tTraining batch 1138 Loss: 0.409699\n",
            "\tTraining batch 1139 Loss: 0.366827\n",
            "\tTraining batch 1140 Loss: 0.688313\n",
            "\tTraining batch 1141 Loss: 0.512085\n",
            "\tTraining batch 1142 Loss: 0.745395\n",
            "\tTraining batch 1143 Loss: 0.633890\n",
            "\tTraining batch 1144 Loss: 0.926405\n",
            "\tTraining batch 1145 Loss: 0.516715\n",
            "\tTraining batch 1146 Loss: 1.019020\n",
            "\tTraining batch 1147 Loss: 0.231907\n",
            "\tTraining batch 1148 Loss: 0.691594\n",
            "\tTraining batch 1149 Loss: 0.875735\n",
            "\tTraining batch 1150 Loss: 0.599226\n",
            "\tTraining batch 1151 Loss: 0.661053\n",
            "\tTraining batch 1152 Loss: 0.443768\n",
            "\tTraining batch 1153 Loss: 0.423813\n",
            "\tTraining batch 1154 Loss: 0.546778\n",
            "\tTraining batch 1155 Loss: 0.593021\n",
            "\tTraining batch 1156 Loss: 0.275225\n",
            "\tTraining batch 1157 Loss: 0.781022\n",
            "\tTraining batch 1158 Loss: 0.394679\n",
            "\tTraining batch 1159 Loss: 0.298690\n",
            "\tTraining batch 1160 Loss: 0.743153\n",
            "\tTraining batch 1161 Loss: 1.061222\n",
            "\tTraining batch 1162 Loss: 0.497265\n",
            "\tTraining batch 1163 Loss: 0.387063\n",
            "\tTraining batch 1164 Loss: 0.806692\n",
            "\tTraining batch 1165 Loss: 0.293004\n",
            "\tTraining batch 1166 Loss: 0.371464\n",
            "\tTraining batch 1167 Loss: 0.497686\n",
            "\tTraining batch 1168 Loss: 0.575429\n",
            "\tTraining batch 1169 Loss: 0.597924\n",
            "\tTraining batch 1170 Loss: 0.415388\n",
            "\tTraining batch 1171 Loss: 0.301654\n",
            "\tTraining batch 1172 Loss: 0.643372\n",
            "\tTraining batch 1173 Loss: 0.673687\n",
            "\tTraining batch 1174 Loss: 0.389357\n",
            "\tTraining batch 1175 Loss: 0.777183\n",
            "\tTraining batch 1176 Loss: 0.584170\n",
            "\tTraining batch 1177 Loss: 0.962573\n",
            "\tTraining batch 1178 Loss: 0.297152\n",
            "\tTraining batch 1179 Loss: 0.477413\n",
            "\tTraining batch 1180 Loss: 0.790696\n",
            "\tTraining batch 1181 Loss: 0.645668\n",
            "\tTraining batch 1182 Loss: 0.682180\n",
            "\tTraining batch 1183 Loss: 0.521958\n",
            "\tTraining batch 1184 Loss: 0.391528\n",
            "\tTraining batch 1185 Loss: 0.450965\n",
            "\tTraining batch 1186 Loss: 0.413307\n",
            "\tTraining batch 1187 Loss: 0.461669\n",
            "\tTraining batch 1188 Loss: 0.178067\n",
            "\tTraining batch 1189 Loss: 0.390927\n",
            "\tTraining batch 1190 Loss: 0.362057\n",
            "\tTraining batch 1191 Loss: 0.740555\n",
            "\tTraining batch 1192 Loss: 0.429657\n",
            "\tTraining batch 1193 Loss: 0.429845\n",
            "\tTraining batch 1194 Loss: 0.386103\n",
            "\tTraining batch 1195 Loss: 0.324662\n",
            "\tTraining batch 1196 Loss: 0.497406\n",
            "\tTraining batch 1197 Loss: 0.858365\n",
            "\tTraining batch 1198 Loss: 0.669877\n",
            "\tTraining batch 1199 Loss: 0.750746\n",
            "\tTraining batch 1200 Loss: 0.744752\n",
            "\tTraining batch 1201 Loss: 0.361795\n",
            "\tTraining batch 1202 Loss: 0.472176\n",
            "\tTraining batch 1203 Loss: 0.425711\n",
            "\tTraining batch 1204 Loss: 0.995124\n",
            "\tTraining batch 1205 Loss: 0.398926\n",
            "\tTraining batch 1206 Loss: 0.587932\n",
            "\tTraining batch 1207 Loss: 0.582228\n",
            "\tTraining batch 1208 Loss: 0.408556\n",
            "\tTraining batch 1209 Loss: 0.614836\n",
            "\tTraining batch 1210 Loss: 0.451175\n",
            "\tTraining batch 1211 Loss: 0.356766\n",
            "\tTraining batch 1212 Loss: 0.768387\n",
            "\tTraining batch 1213 Loss: 0.616144\n",
            "\tTraining batch 1214 Loss: 0.437513\n",
            "\tTraining batch 1215 Loss: 0.528747\n",
            "\tTraining batch 1216 Loss: 0.446890\n",
            "\tTraining batch 1217 Loss: 0.345766\n",
            "\tTraining batch 1218 Loss: 0.300418\n",
            "\tTraining batch 1219 Loss: 0.563575\n",
            "\tTraining batch 1220 Loss: 0.761594\n",
            "\tTraining batch 1221 Loss: 0.508346\n",
            "\tTraining batch 1222 Loss: 0.595966\n",
            "\tTraining batch 1223 Loss: 0.240145\n",
            "\tTraining batch 1224 Loss: 0.485500\n",
            "\tTraining batch 1225 Loss: 0.671680\n",
            "\tTraining batch 1226 Loss: 0.442513\n",
            "\tTraining batch 1227 Loss: 0.281874\n",
            "\tTraining batch 1228 Loss: 0.604161\n",
            "\tTraining batch 1229 Loss: 1.011654\n",
            "\tTraining batch 1230 Loss: 0.570813\n",
            "\tTraining batch 1231 Loss: 0.268121\n",
            "\tTraining batch 1232 Loss: 1.096860\n",
            "\tTraining batch 1233 Loss: 0.318727\n",
            "\tTraining batch 1234 Loss: 0.484059\n",
            "\tTraining batch 1235 Loss: 0.384121\n",
            "\tTraining batch 1236 Loss: 0.351515\n",
            "\tTraining batch 1237 Loss: 0.581772\n",
            "\tTraining batch 1238 Loss: 0.801362\n",
            "\tTraining batch 1239 Loss: 0.677498\n",
            "\tTraining batch 1240 Loss: 0.534197\n",
            "\tTraining batch 1241 Loss: 0.789920\n",
            "\tTraining batch 1242 Loss: 0.733688\n",
            "\tTraining batch 1243 Loss: 0.440040\n",
            "\tTraining batch 1244 Loss: 0.676825\n",
            "\tTraining batch 1245 Loss: 0.805028\n",
            "\tTraining batch 1246 Loss: 0.648194\n",
            "\tTraining batch 1247 Loss: 0.381452\n",
            "\tTraining batch 1248 Loss: 0.264423\n",
            "\tTraining batch 1249 Loss: 0.407953\n",
            "\tTraining batch 1250 Loss: 0.397063\n",
            "\tTraining batch 1251 Loss: 0.463888\n",
            "\tTraining batch 1252 Loss: 0.153733\n",
            "\tTraining batch 1253 Loss: 0.501065\n",
            "\tTraining batch 1254 Loss: 0.232864\n",
            "\tTraining batch 1255 Loss: 1.094853\n",
            "\tTraining batch 1256 Loss: 0.537002\n",
            "\tTraining batch 1257 Loss: 0.284002\n",
            "\tTraining batch 1258 Loss: 0.768780\n",
            "\tTraining batch 1259 Loss: 0.562669\n",
            "\tTraining batch 1260 Loss: 0.640123\n",
            "\tTraining batch 1261 Loss: 0.906931\n",
            "\tTraining batch 1262 Loss: 0.683908\n",
            "\tTraining batch 1263 Loss: 0.933124\n",
            "\tTraining batch 1264 Loss: 0.686267\n",
            "\tTraining batch 1265 Loss: 0.435166\n",
            "\tTraining batch 1266 Loss: 0.376451\n",
            "\tTraining batch 1267 Loss: 0.867465\n",
            "\tTraining batch 1268 Loss: 0.276733\n",
            "\tTraining batch 1269 Loss: 0.628209\n",
            "\tTraining batch 1270 Loss: 0.624098\n",
            "\tTraining batch 1271 Loss: 0.701157\n",
            "\tTraining batch 1272 Loss: 0.787977\n",
            "\tTraining batch 1273 Loss: 0.700328\n",
            "\tTraining batch 1274 Loss: 0.501303\n",
            "\tTraining batch 1275 Loss: 0.698937\n",
            "\tTraining batch 1276 Loss: 0.458632\n",
            "\tTraining batch 1277 Loss: 0.628944\n",
            "\tTraining batch 1278 Loss: 0.353430\n",
            "\tTraining batch 1279 Loss: 0.427090\n",
            "\tTraining batch 1280 Loss: 0.553948\n",
            "\tTraining batch 1281 Loss: 0.380526\n",
            "\tTraining batch 1282 Loss: 0.422861\n",
            "\tTraining batch 1283 Loss: 0.637800\n",
            "\tTraining batch 1284 Loss: 0.617572\n",
            "\tTraining batch 1285 Loss: 1.038373\n",
            "\tTraining batch 1286 Loss: 0.419189\n",
            "\tTraining batch 1287 Loss: 0.596671\n",
            "\tTraining batch 1288 Loss: 0.632694\n",
            "\tTraining batch 1289 Loss: 0.874107\n",
            "\tTraining batch 1290 Loss: 0.749766\n",
            "\tTraining batch 1291 Loss: 0.128651\n",
            "\tTraining batch 1292 Loss: 0.621206\n",
            "\tTraining batch 1293 Loss: 0.392367\n",
            "\tTraining batch 1294 Loss: 0.233176\n",
            "\tTraining batch 1295 Loss: 0.753439\n",
            "\tTraining batch 1296 Loss: 0.697988\n",
            "\tTraining batch 1297 Loss: 0.731250\n",
            "\tTraining batch 1298 Loss: 0.359187\n",
            "\tTraining batch 1299 Loss: 0.544952\n",
            "\tTraining batch 1300 Loss: 0.368272\n",
            "\tTraining batch 1301 Loss: 0.307995\n",
            "\tTraining batch 1302 Loss: 0.583345\n",
            "\tTraining batch 1303 Loss: 0.405726\n",
            "\tTraining batch 1304 Loss: 0.923229\n",
            "\tTraining batch 1305 Loss: 0.298997\n",
            "\tTraining batch 1306 Loss: 0.354678\n",
            "\tTraining batch 1307 Loss: 0.570990\n",
            "\tTraining batch 1308 Loss: 0.357859\n",
            "\tTraining batch 1309 Loss: 0.336842\n",
            "\tTraining batch 1310 Loss: 0.331434\n",
            "\tTraining batch 1311 Loss: 0.377629\n",
            "\tTraining batch 1312 Loss: 0.304933\n",
            "\tTraining batch 1313 Loss: 0.596343\n",
            "\tTraining batch 1314 Loss: 1.157282\n",
            "\tTraining batch 1315 Loss: 0.573377\n",
            "\tTraining batch 1316 Loss: 0.326449\n",
            "\tTraining batch 1317 Loss: 0.572650\n",
            "\tTraining batch 1318 Loss: 0.672881\n",
            "\tTraining batch 1319 Loss: 0.497338\n",
            "\tTraining batch 1320 Loss: 0.367397\n",
            "\tTraining batch 1321 Loss: 0.671324\n",
            "\tTraining batch 1322 Loss: 0.503587\n",
            "\tTraining batch 1323 Loss: 0.611765\n",
            "\tTraining batch 1324 Loss: 0.369419\n",
            "\tTraining batch 1325 Loss: 0.511108\n",
            "\tTraining batch 1326 Loss: 1.096468\n",
            "\tTraining batch 1327 Loss: 0.485039\n",
            "\tTraining batch 1328 Loss: 0.669271\n",
            "\tTraining batch 1329 Loss: 0.503496\n",
            "\tTraining batch 1330 Loss: 0.372587\n",
            "\tTraining batch 1331 Loss: 0.731657\n",
            "\tTraining batch 1332 Loss: 0.846784\n",
            "\tTraining batch 1333 Loss: 0.426034\n",
            "\tTraining batch 1334 Loss: 0.358738\n",
            "\tTraining batch 1335 Loss: 0.927365\n",
            "\tTraining batch 1336 Loss: 0.732556\n",
            "\tTraining batch 1337 Loss: 0.213942\n",
            "\tTraining batch 1338 Loss: 0.474848\n",
            "\tTraining batch 1339 Loss: 0.657395\n",
            "\tTraining batch 1340 Loss: 0.388580\n",
            "\tTraining batch 1341 Loss: 0.453444\n",
            "\tTraining batch 1342 Loss: 0.596882\n",
            "\tTraining batch 1343 Loss: 0.788897\n",
            "\tTraining batch 1344 Loss: 0.390809\n",
            "\tTraining batch 1345 Loss: 0.340512\n",
            "\tTraining batch 1346 Loss: 0.614141\n",
            "\tTraining batch 1347 Loss: 1.065922\n",
            "\tTraining batch 1348 Loss: 0.180146\n",
            "\tTraining batch 1349 Loss: 0.330303\n",
            "\tTraining batch 1350 Loss: 0.353245\n",
            "\tTraining batch 1351 Loss: 0.220381\n",
            "\tTraining batch 1352 Loss: 0.663110\n",
            "\tTraining batch 1353 Loss: 0.759621\n",
            "\tTraining batch 1354 Loss: 0.549149\n",
            "\tTraining batch 1355 Loss: 1.170220\n",
            "\tTraining batch 1356 Loss: 0.617423\n",
            "\tTraining batch 1357 Loss: 0.306487\n",
            "\tTraining batch 1358 Loss: 0.949301\n",
            "\tTraining batch 1359 Loss: 0.403174\n",
            "\tTraining batch 1360 Loss: 0.373640\n",
            "\tTraining batch 1361 Loss: 0.784256\n",
            "\tTraining batch 1362 Loss: 0.630899\n",
            "\tTraining batch 1363 Loss: 0.606044\n",
            "\tTraining batch 1364 Loss: 0.326702\n",
            "\tTraining batch 1365 Loss: 0.678604\n",
            "\tTraining batch 1366 Loss: 0.560536\n",
            "\tTraining batch 1367 Loss: 0.490138\n",
            "\tTraining batch 1368 Loss: 0.476780\n",
            "\tTraining batch 1369 Loss: 0.290968\n",
            "\tTraining batch 1370 Loss: 0.740281\n",
            "\tTraining batch 1371 Loss: 0.685653\n",
            "\tTraining batch 1372 Loss: 0.339283\n",
            "\tTraining batch 1373 Loss: 0.534503\n",
            "\tTraining batch 1374 Loss: 0.567244\n",
            "\tTraining batch 1375 Loss: 0.572637\n",
            "\tTraining batch 1376 Loss: 0.681936\n",
            "\tTraining batch 1377 Loss: 0.531819\n",
            "\tTraining batch 1378 Loss: 0.776033\n",
            "\tTraining batch 1379 Loss: 0.337295\n",
            "\tTraining batch 1380 Loss: 0.521747\n",
            "\tTraining batch 1381 Loss: 0.622644\n",
            "\tTraining batch 1382 Loss: 0.344989\n",
            "\tTraining batch 1383 Loss: 0.628060\n",
            "\tTraining batch 1384 Loss: 0.510359\n",
            "\tTraining batch 1385 Loss: 0.347370\n",
            "\tTraining batch 1386 Loss: 0.235554\n",
            "\tTraining batch 1387 Loss: 0.299371\n",
            "\tTraining batch 1388 Loss: 0.412553\n",
            "\tTraining batch 1389 Loss: 0.357704\n",
            "\tTraining batch 1390 Loss: 0.319552\n",
            "\tTraining batch 1391 Loss: 1.100000\n",
            "\tTraining batch 1392 Loss: 0.279477\n",
            "\tTraining batch 1393 Loss: 0.200923\n",
            "\tTraining batch 1394 Loss: 0.785512\n",
            "\tTraining batch 1395 Loss: 1.082367\n",
            "\tTraining batch 1396 Loss: 0.191106\n",
            "\tTraining batch 1397 Loss: 0.628146\n",
            "\tTraining batch 1398 Loss: 0.463364\n",
            "\tTraining batch 1399 Loss: 0.539648\n",
            "\tTraining batch 1400 Loss: 0.338501\n",
            "\tTraining batch 1401 Loss: 0.204501\n",
            "\tTraining batch 1402 Loss: 0.729669\n",
            "\tTraining batch 1403 Loss: 0.743109\n",
            "\tTraining batch 1404 Loss: 0.481295\n",
            "\tTraining batch 1405 Loss: 0.488180\n",
            "\tTraining batch 1406 Loss: 0.775369\n",
            "\tTraining batch 1407 Loss: 0.430010\n",
            "\tTraining batch 1408 Loss: 0.370688\n",
            "\tTraining batch 1409 Loss: 0.599937\n",
            "\tTraining batch 1410 Loss: 0.519153\n",
            "\tTraining batch 1411 Loss: 0.104982\n",
            "\tTraining batch 1412 Loss: 0.732925\n",
            "\tTraining batch 1413 Loss: 0.292645\n",
            "\tTraining batch 1414 Loss: 0.370873\n",
            "\tTraining batch 1415 Loss: 0.789108\n",
            "\tTraining batch 1416 Loss: 0.500929\n",
            "\tTraining batch 1417 Loss: 0.400208\n",
            "\tTraining batch 1418 Loss: 0.924265\n",
            "\tTraining batch 1419 Loss: 0.593463\n",
            "\tTraining batch 1420 Loss: 0.424240\n",
            "\tTraining batch 1421 Loss: 0.455117\n",
            "\tTraining batch 1422 Loss: 0.688924\n",
            "\tTraining batch 1423 Loss: 0.467270\n",
            "\tTraining batch 1424 Loss: 0.787709\n",
            "\tTraining batch 1425 Loss: 0.502832\n",
            "\tTraining batch 1426 Loss: 0.709072\n",
            "\tTraining batch 1427 Loss: 0.282944\n",
            "\tTraining batch 1428 Loss: 0.434602\n",
            "\tTraining batch 1429 Loss: 0.390883\n",
            "\tTraining batch 1430 Loss: 1.011208\n",
            "\tTraining batch 1431 Loss: 0.622298\n",
            "\tTraining batch 1432 Loss: 0.354181\n",
            "\tTraining batch 1433 Loss: 0.431545\n",
            "\tTraining batch 1434 Loss: 0.910516\n",
            "\tTraining batch 1435 Loss: 0.536191\n",
            "\tTraining batch 1436 Loss: 0.584803\n",
            "\tTraining batch 1437 Loss: 0.638578\n",
            "\tTraining batch 1438 Loss: 0.591770\n",
            "\tTraining batch 1439 Loss: 0.687652\n",
            "\tTraining batch 1440 Loss: 0.763540\n",
            "\tTraining batch 1441 Loss: 0.335936\n",
            "\tTraining batch 1442 Loss: 0.393506\n",
            "\tTraining batch 1443 Loss: 0.828053\n",
            "\tTraining batch 1444 Loss: 0.551429\n",
            "\tTraining batch 1445 Loss: 0.378449\n",
            "\tTraining batch 1446 Loss: 0.473870\n",
            "\tTraining batch 1447 Loss: 0.607951\n",
            "\tTraining batch 1448 Loss: 0.605272\n",
            "\tTraining batch 1449 Loss: 0.339139\n",
            "\tTraining batch 1450 Loss: 0.591085\n",
            "\tTraining batch 1451 Loss: 0.542547\n",
            "\tTraining batch 1452 Loss: 0.785576\n",
            "\tTraining batch 1453 Loss: 0.307752\n",
            "\tTraining batch 1454 Loss: 0.715410\n",
            "\tTraining batch 1455 Loss: 0.382884\n",
            "\tTraining batch 1456 Loss: 0.642724\n",
            "\tTraining batch 1457 Loss: 0.600505\n",
            "\tTraining batch 1458 Loss: 0.620234\n",
            "\tTraining batch 1459 Loss: 0.255197\n",
            "\tTraining batch 1460 Loss: 0.383998\n",
            "\tTraining batch 1461 Loss: 0.298756\n",
            "\tTraining batch 1462 Loss: 0.998246\n",
            "\tTraining batch 1463 Loss: 0.637937\n",
            "\tTraining batch 1464 Loss: 0.138993\n",
            "\tTraining batch 1465 Loss: 0.455149\n",
            "\tTraining batch 1466 Loss: 0.360929\n",
            "\tTraining batch 1467 Loss: 0.464745\n",
            "\tTraining batch 1468 Loss: 0.479150\n",
            "\tTraining batch 1469 Loss: 0.313334\n",
            "\tTraining batch 1470 Loss: 0.387529\n",
            "\tTraining batch 1471 Loss: 0.227423\n",
            "\tTraining batch 1472 Loss: 0.546635\n",
            "\tTraining batch 1473 Loss: 0.463046\n",
            "\tTraining batch 1474 Loss: 0.231516\n",
            "\tTraining batch 1475 Loss: 0.546135\n",
            "\tTraining batch 1476 Loss: 0.523460\n",
            "\tTraining batch 1477 Loss: 0.411656\n",
            "\tTraining batch 1478 Loss: 0.411124\n",
            "\tTraining batch 1479 Loss: 0.570293\n",
            "\tTraining batch 1480 Loss: 0.411238\n",
            "\tTraining batch 1481 Loss: 0.169103\n",
            "\tTraining batch 1482 Loss: 0.906556\n",
            "\tTraining batch 1483 Loss: 0.583286\n",
            "\tTraining batch 1484 Loss: 0.544940\n",
            "\tTraining batch 1485 Loss: 0.819486\n",
            "\tTraining batch 1486 Loss: 0.402809\n",
            "\tTraining batch 1487 Loss: 0.440572\n",
            "\tTraining batch 1488 Loss: 0.296313\n",
            "\tTraining batch 1489 Loss: 0.512873\n",
            "\tTraining batch 1490 Loss: 0.414782\n",
            "\tTraining batch 1491 Loss: 0.623193\n",
            "\tTraining batch 1492 Loss: 0.610436\n",
            "\tTraining batch 1493 Loss: 0.296249\n",
            "\tTraining batch 1494 Loss: 0.440869\n",
            "\tTraining batch 1495 Loss: 0.335190\n",
            "\tTraining batch 1496 Loss: 0.545067\n",
            "\tTraining batch 1497 Loss: 0.486249\n",
            "\tTraining batch 1498 Loss: 0.216578\n",
            "\tTraining batch 1499 Loss: 0.463895\n",
            "\tTraining batch 1500 Loss: 0.431038\n",
            "\tTraining batch 1501 Loss: 0.379003\n",
            "\tTraining batch 1502 Loss: 0.359284\n",
            "\tTraining batch 1503 Loss: 0.960769\n",
            "\tTraining batch 1504 Loss: 0.380982\n",
            "\tTraining batch 1505 Loss: 0.480087\n",
            "\tTraining batch 1506 Loss: 0.586334\n",
            "\tTraining batch 1507 Loss: 0.832725\n",
            "\tTraining batch 1508 Loss: 0.752949\n",
            "\tTraining batch 1509 Loss: 0.371732\n",
            "\tTraining batch 1510 Loss: 0.305058\n",
            "\tTraining batch 1511 Loss: 0.507463\n",
            "\tTraining batch 1512 Loss: 0.510275\n",
            "\tTraining batch 1513 Loss: 0.216425\n",
            "\tTraining batch 1514 Loss: 0.570459\n",
            "\tTraining batch 1515 Loss: 0.453685\n",
            "\tTraining batch 1516 Loss: 0.701432\n",
            "\tTraining batch 1517 Loss: 0.498429\n",
            "\tTraining batch 1518 Loss: 0.488661\n",
            "\tTraining batch 1519 Loss: 0.249285\n",
            "\tTraining batch 1520 Loss: 0.281756\n",
            "\tTraining batch 1521 Loss: 0.367289\n",
            "\tTraining batch 1522 Loss: 0.387085\n",
            "\tTraining batch 1523 Loss: 0.384931\n",
            "\tTraining batch 1524 Loss: 0.566601\n",
            "\tTraining batch 1525 Loss: 0.233434\n",
            "\tTraining batch 1526 Loss: 0.291396\n",
            "\tTraining batch 1527 Loss: 1.038020\n",
            "\tTraining batch 1528 Loss: 0.186722\n",
            "\tTraining batch 1529 Loss: 0.630447\n",
            "\tTraining batch 1530 Loss: 0.279158\n",
            "\tTraining batch 1531 Loss: 0.412072\n",
            "\tTraining batch 1532 Loss: 0.625947\n",
            "\tTraining batch 1533 Loss: 0.280754\n",
            "\tTraining batch 1534 Loss: 0.438269\n",
            "\tTraining batch 1535 Loss: 0.846755\n",
            "\tTraining batch 1536 Loss: 0.835592\n",
            "\tTraining batch 1537 Loss: 0.987240\n",
            "\tTraining batch 1538 Loss: 0.383250\n",
            "\tTraining batch 1539 Loss: 0.944938\n",
            "\tTraining batch 1540 Loss: 0.294424\n",
            "\tTraining batch 1541 Loss: 1.170166\n",
            "\tTraining batch 1542 Loss: 0.964833\n",
            "\tTraining batch 1543 Loss: 0.335744\n",
            "\tTraining batch 1544 Loss: 1.227923\n",
            "\tTraining batch 1545 Loss: 0.545644\n",
            "\tTraining batch 1546 Loss: 0.300310\n",
            "\tTraining batch 1547 Loss: 0.499080\n",
            "\tTraining batch 1548 Loss: 0.425855\n",
            "\tTraining batch 1549 Loss: 0.691762\n",
            "\tTraining batch 1550 Loss: 0.567404\n",
            "\tTraining batch 1551 Loss: 0.754235\n",
            "\tTraining batch 1552 Loss: 0.631420\n",
            "\tTraining batch 1553 Loss: 0.311384\n",
            "\tTraining batch 1554 Loss: 0.504386\n",
            "\tTraining batch 1555 Loss: 0.517248\n",
            "\tTraining batch 1556 Loss: 0.422064\n",
            "\tTraining batch 1557 Loss: 0.775165\n",
            "\tTraining batch 1558 Loss: 0.492716\n",
            "\tTraining batch 1559 Loss: 0.593976\n",
            "\tTraining batch 1560 Loss: 0.554556\n",
            "\tTraining batch 1561 Loss: 0.735341\n",
            "\tTraining batch 1562 Loss: 0.661758\n",
            "\tTraining batch 1563 Loss: 0.696318\n",
            "\tTraining batch 1564 Loss: 0.703096\n",
            "\tTraining batch 1565 Loss: 0.199661\n",
            "\tTraining batch 1566 Loss: 0.746021\n",
            "\tTraining batch 1567 Loss: 0.718709\n",
            "\tTraining batch 1568 Loss: 0.809155\n",
            "\tTraining batch 1569 Loss: 0.489294\n",
            "\tTraining batch 1570 Loss: 0.612125\n",
            "\tTraining batch 1571 Loss: 0.542325\n",
            "\tTraining batch 1572 Loss: 0.609312\n",
            "\tTraining batch 1573 Loss: 0.237153\n",
            "\tTraining batch 1574 Loss: 0.373559\n",
            "\tTraining batch 1575 Loss: 0.257127\n",
            "\tTraining batch 1576 Loss: 0.364458\n",
            "\tTraining batch 1577 Loss: 0.921809\n",
            "\tTraining batch 1578 Loss: 1.103262\n",
            "\tTraining batch 1579 Loss: 0.573369\n",
            "\tTraining batch 1580 Loss: 0.584601\n",
            "\tTraining batch 1581 Loss: 0.508527\n",
            "\tTraining batch 1582 Loss: 0.697747\n",
            "\tTraining batch 1583 Loss: 1.318528\n",
            "\tTraining batch 1584 Loss: 0.499226\n",
            "\tTraining batch 1585 Loss: 0.486083\n",
            "\tTraining batch 1586 Loss: 0.402606\n",
            "\tTraining batch 1587 Loss: 0.379783\n",
            "\tTraining batch 1588 Loss: 0.438825\n",
            "\tTraining batch 1589 Loss: 0.345411\n",
            "\tTraining batch 1590 Loss: 0.476892\n",
            "\tTraining batch 1591 Loss: 0.612479\n",
            "\tTraining batch 1592 Loss: 0.517127\n",
            "\tTraining batch 1593 Loss: 0.505430\n",
            "\tTraining batch 1594 Loss: 0.615880\n",
            "\tTraining batch 1595 Loss: 0.673351\n",
            "\tTraining batch 1596 Loss: 0.809444\n",
            "\tTraining batch 1597 Loss: 0.623963\n",
            "\tTraining batch 1598 Loss: 0.810510\n",
            "\tTraining batch 1599 Loss: 0.957203\n",
            "\tTraining batch 1600 Loss: 0.203593\n",
            "\tTraining batch 1601 Loss: 0.539146\n",
            "\tTraining batch 1602 Loss: 0.490191\n",
            "\tTraining batch 1603 Loss: 0.776330\n",
            "\tTraining batch 1604 Loss: 0.637131\n",
            "\tTraining batch 1605 Loss: 0.366069\n",
            "\tTraining batch 1606 Loss: 0.491693\n",
            "\tTraining batch 1607 Loss: 0.626600\n",
            "\tTraining batch 1608 Loss: 0.541908\n",
            "\tTraining batch 1609 Loss: 0.561629\n",
            "\tTraining batch 1610 Loss: 0.635149\n",
            "\tTraining batch 1611 Loss: 0.425495\n",
            "\tTraining batch 1612 Loss: 0.580098\n",
            "\tTraining batch 1613 Loss: 0.300393\n",
            "\tTraining batch 1614 Loss: 0.522752\n",
            "\tTraining batch 1615 Loss: 0.460376\n",
            "\tTraining batch 1616 Loss: 0.712354\n",
            "\tTraining batch 1617 Loss: 0.633216\n",
            "\tTraining batch 1618 Loss: 0.561179\n",
            "\tTraining batch 1619 Loss: 0.627980\n",
            "\tTraining batch 1620 Loss: 0.293583\n",
            "\tTraining batch 1621 Loss: 0.498728\n",
            "\tTraining batch 1622 Loss: 0.403762\n",
            "\tTraining batch 1623 Loss: 0.649085\n",
            "\tTraining batch 1624 Loss: 0.473134\n",
            "\tTraining batch 1625 Loss: 0.182273\n",
            "\tTraining batch 1626 Loss: 0.932774\n",
            "\tTraining batch 1627 Loss: 0.418542\n",
            "\tTraining batch 1628 Loss: 0.493477\n",
            "\tTraining batch 1629 Loss: 0.521629\n",
            "\tTraining batch 1630 Loss: 0.508149\n",
            "\tTraining batch 1631 Loss: 0.485507\n",
            "\tTraining batch 1632 Loss: 0.443427\n",
            "\tTraining batch 1633 Loss: 0.501157\n",
            "\tTraining batch 1634 Loss: 0.260630\n",
            "\tTraining batch 1635 Loss: 0.759750\n",
            "\tTraining batch 1636 Loss: 0.543059\n",
            "\tTraining batch 1637 Loss: 0.279802\n",
            "\tTraining batch 1638 Loss: 0.351008\n",
            "\tTraining batch 1639 Loss: 0.567315\n",
            "\tTraining batch 1640 Loss: 0.570164\n",
            "\tTraining batch 1641 Loss: 0.947806\n",
            "\tTraining batch 1642 Loss: 0.614449\n",
            "\tTraining batch 1643 Loss: 1.110144\n",
            "\tTraining batch 1644 Loss: 0.419746\n",
            "\tTraining batch 1645 Loss: 0.698879\n",
            "\tTraining batch 1646 Loss: 0.381414\n",
            "\tTraining batch 1647 Loss: 0.405811\n",
            "\tTraining batch 1648 Loss: 0.800480\n",
            "\tTraining batch 1649 Loss: 0.212301\n",
            "\tTraining batch 1650 Loss: 0.568609\n",
            "\tTraining batch 1651 Loss: 0.604227\n",
            "\tTraining batch 1652 Loss: 0.945682\n",
            "\tTraining batch 1653 Loss: 0.994140\n",
            "\tTraining batch 1654 Loss: 0.190439\n",
            "\tTraining batch 1655 Loss: 0.463706\n",
            "\tTraining batch 1656 Loss: 0.571954\n",
            "\tTraining batch 1657 Loss: 0.288246\n",
            "\tTraining batch 1658 Loss: 0.354283\n",
            "\tTraining batch 1659 Loss: 0.808423\n",
            "\tTraining batch 1660 Loss: 0.315236\n",
            "\tTraining batch 1661 Loss: 0.477067\n",
            "\tTraining batch 1662 Loss: 1.081083\n",
            "\tTraining batch 1663 Loss: 0.822595\n",
            "\tTraining batch 1664 Loss: 0.545279\n",
            "\tTraining batch 1665 Loss: 0.515011\n",
            "\tTraining batch 1666 Loss: 0.694581\n",
            "\tTraining batch 1667 Loss: 0.526816\n",
            "\tTraining batch 1668 Loss: 0.701638\n",
            "\tTraining batch 1669 Loss: 0.740340\n",
            "\tTraining batch 1670 Loss: 0.300974\n",
            "\tTraining batch 1671 Loss: 0.295530\n",
            "\tTraining batch 1672 Loss: 0.716049\n",
            "\tTraining batch 1673 Loss: 0.602618\n",
            "\tTraining batch 1674 Loss: 0.513149\n",
            "\tTraining batch 1675 Loss: 0.465748\n",
            "\tTraining batch 1676 Loss: 0.768062\n",
            "\tTraining batch 1677 Loss: 0.594665\n",
            "\tTraining batch 1678 Loss: 1.073616\n",
            "\tTraining batch 1679 Loss: 0.716063\n",
            "\tTraining batch 1680 Loss: 0.410021\n",
            "\tTraining batch 1681 Loss: 0.828878\n",
            "\tTraining batch 1682 Loss: 0.512260\n",
            "\tTraining batch 1683 Loss: 0.688315\n",
            "\tTraining batch 1684 Loss: 0.492666\n",
            "\tTraining batch 1685 Loss: 0.401436\n",
            "\tTraining batch 1686 Loss: 0.347380\n",
            "\tTraining batch 1687 Loss: 0.753836\n",
            "\tTraining batch 1688 Loss: 0.828633\n",
            "\tTraining batch 1689 Loss: 0.495220\n",
            "\tTraining batch 1690 Loss: 0.319513\n",
            "\tTraining batch 1691 Loss: 0.465210\n",
            "\tTraining batch 1692 Loss: 0.377847\n",
            "\tTraining batch 1693 Loss: 0.548332\n",
            "\tTraining batch 1694 Loss: 0.701407\n",
            "\tTraining batch 1695 Loss: 0.973360\n",
            "\tTraining batch 1696 Loss: 0.445472\n",
            "\tTraining batch 1697 Loss: 0.299201\n",
            "\tTraining batch 1698 Loss: 0.482104\n",
            "\tTraining batch 1699 Loss: 0.605042\n",
            "\tTraining batch 1700 Loss: 0.471015\n",
            "\tTraining batch 1701 Loss: 0.573935\n",
            "\tTraining batch 1702 Loss: 0.602625\n",
            "\tTraining batch 1703 Loss: 0.891750\n",
            "\tTraining batch 1704 Loss: 0.271591\n",
            "\tTraining batch 1705 Loss: 0.544803\n",
            "\tTraining batch 1706 Loss: 0.512276\n",
            "\tTraining batch 1707 Loss: 0.336201\n",
            "\tTraining batch 1708 Loss: 0.315436\n",
            "\tTraining batch 1709 Loss: 0.167246\n",
            "\tTraining batch 1710 Loss: 0.327232\n",
            "\tTraining batch 1711 Loss: 0.466414\n",
            "\tTraining batch 1712 Loss: 1.285254\n",
            "\tTraining batch 1713 Loss: 0.476351\n",
            "\tTraining batch 1714 Loss: 0.119711\n",
            "\tTraining batch 1715 Loss: 0.613054\n",
            "\tTraining batch 1716 Loss: 0.455801\n",
            "\tTraining batch 1717 Loss: 0.734162\n",
            "\tTraining batch 1718 Loss: 0.574628\n",
            "\tTraining batch 1719 Loss: 0.502916\n",
            "\tTraining batch 1720 Loss: 0.786234\n",
            "\tTraining batch 1721 Loss: 0.620607\n",
            "\tTraining batch 1722 Loss: 0.528914\n",
            "\tTraining batch 1723 Loss: 0.574483\n",
            "\tTraining batch 1724 Loss: 0.650017\n",
            "\tTraining batch 1725 Loss: 0.473277\n",
            "\tTraining batch 1726 Loss: 0.220572\n",
            "\tTraining batch 1727 Loss: 0.950268\n",
            "\tTraining batch 1728 Loss: 0.784119\n",
            "\tTraining batch 1729 Loss: 0.557235\n",
            "\tTraining batch 1730 Loss: 0.416568\n",
            "\tTraining batch 1731 Loss: 0.328671\n",
            "\tTraining batch 1732 Loss: 0.441777\n",
            "\tTraining batch 1733 Loss: 0.926605\n",
            "\tTraining batch 1734 Loss: 0.595120\n",
            "\tTraining batch 1735 Loss: 0.613247\n",
            "\tTraining batch 1736 Loss: 0.624429\n",
            "\tTraining batch 1737 Loss: 0.709723\n",
            "\tTraining batch 1738 Loss: 0.273300\n",
            "\tTraining batch 1739 Loss: 0.979099\n",
            "\tTraining batch 1740 Loss: 0.578472\n",
            "\tTraining batch 1741 Loss: 0.353659\n",
            "\tTraining batch 1742 Loss: 0.480986\n",
            "\tTraining batch 1743 Loss: 0.423673\n",
            "\tTraining batch 1744 Loss: 0.431426\n",
            "\tTraining batch 1745 Loss: 0.894968\n",
            "\tTraining batch 1746 Loss: 0.291559\n",
            "\tTraining batch 1747 Loss: 0.363222\n",
            "\tTraining batch 1748 Loss: 0.527955\n",
            "\tTraining batch 1749 Loss: 0.963584\n",
            "\tTraining batch 1750 Loss: 0.527825\n",
            "\tTraining batch 1751 Loss: 0.488774\n",
            "\tTraining batch 1752 Loss: 0.755462\n",
            "\tTraining batch 1753 Loss: 0.684097\n",
            "\tTraining batch 1754 Loss: 0.387061\n",
            "\tTraining batch 1755 Loss: 0.782708\n",
            "\tTraining batch 1756 Loss: 0.326329\n",
            "\tTraining batch 1757 Loss: 0.707116\n",
            "\tTraining batch 1758 Loss: 0.941797\n",
            "\tTraining batch 1759 Loss: 0.263428\n",
            "\tTraining batch 1760 Loss: 0.589425\n",
            "\tTraining batch 1761 Loss: 0.627308\n",
            "\tTraining batch 1762 Loss: 0.881114\n",
            "\tTraining batch 1763 Loss: 0.178273\n",
            "\tTraining batch 1764 Loss: 0.673960\n",
            "\tTraining batch 1765 Loss: 0.532539\n",
            "\tTraining batch 1766 Loss: 0.838207\n",
            "\tTraining batch 1767 Loss: 0.891606\n",
            "\tTraining batch 1768 Loss: 0.325268\n",
            "\tTraining batch 1769 Loss: 0.149263\n",
            "\tTraining batch 1770 Loss: 0.946724\n",
            "\tTraining batch 1771 Loss: 0.204232\n",
            "\tTraining batch 1772 Loss: 0.537067\n",
            "\tTraining batch 1773 Loss: 0.668693\n",
            "\tTraining batch 1774 Loss: 0.419011\n",
            "\tTraining batch 1775 Loss: 0.306443\n",
            "\tTraining batch 1776 Loss: 0.332554\n",
            "\tTraining batch 1777 Loss: 0.458563\n",
            "\tTraining batch 1778 Loss: 0.629875\n",
            "\tTraining batch 1779 Loss: 0.333803\n",
            "\tTraining batch 1780 Loss: 0.702308\n",
            "\tTraining batch 1781 Loss: 0.600044\n",
            "\tTraining batch 1782 Loss: 0.512046\n",
            "\tTraining batch 1783 Loss: 0.314369\n",
            "\tTraining batch 1784 Loss: 0.643901\n",
            "\tTraining batch 1785 Loss: 0.477067\n",
            "\tTraining batch 1786 Loss: 0.506131\n",
            "\tTraining batch 1787 Loss: 0.319124\n",
            "\tTraining batch 1788 Loss: 0.546104\n",
            "\tTraining batch 1789 Loss: 0.310555\n",
            "\tTraining batch 1790 Loss: 0.435243\n",
            "\tTraining batch 1791 Loss: 0.476015\n",
            "\tTraining batch 1792 Loss: 0.632478\n",
            "\tTraining batch 1793 Loss: 0.147477\n",
            "\tTraining batch 1794 Loss: 0.522167\n",
            "\tTraining batch 1795 Loss: 0.727905\n",
            "\tTraining batch 1796 Loss: 0.555015\n",
            "\tTraining batch 1797 Loss: 0.631534\n",
            "\tTraining batch 1798 Loss: 0.343301\n",
            "\tTraining batch 1799 Loss: 0.472057\n",
            "\tTraining batch 1800 Loss: 0.843037\n",
            "\tTraining batch 1801 Loss: 0.638082\n",
            "\tTraining batch 1802 Loss: 0.499580\n",
            "\tTraining batch 1803 Loss: 0.499226\n",
            "\tTraining batch 1804 Loss: 0.365074\n",
            "\tTraining batch 1805 Loss: 0.568317\n",
            "\tTraining batch 1806 Loss: 0.576718\n",
            "\tTraining batch 1807 Loss: 0.907970\n",
            "\tTraining batch 1808 Loss: 0.328094\n",
            "\tTraining batch 1809 Loss: 0.543451\n",
            "\tTraining batch 1810 Loss: 0.280669\n",
            "\tTraining batch 1811 Loss: 0.793346\n",
            "\tTraining batch 1812 Loss: 0.266243\n",
            "\tTraining batch 1813 Loss: 0.737949\n",
            "\tTraining batch 1814 Loss: 0.640694\n",
            "\tTraining batch 1815 Loss: 1.173316\n",
            "\tTraining batch 1816 Loss: 0.565176\n",
            "\tTraining batch 1817 Loss: 0.678014\n",
            "\tTraining batch 1818 Loss: 0.461451\n",
            "\tTraining batch 1819 Loss: 0.627025\n",
            "\tTraining batch 1820 Loss: 0.320534\n",
            "\tTraining batch 1821 Loss: 0.556219\n",
            "\tTraining batch 1822 Loss: 0.637638\n",
            "\tTraining batch 1823 Loss: 0.519204\n",
            "\tTraining batch 1824 Loss: 0.424914\n",
            "\tTraining batch 1825 Loss: 0.492898\n",
            "\tTraining batch 1826 Loss: 0.720894\n",
            "\tTraining batch 1827 Loss: 0.438405\n",
            "\tTraining batch 1828 Loss: 0.201715\n",
            "\tTraining batch 1829 Loss: 0.663551\n",
            "\tTraining batch 1830 Loss: 0.566056\n",
            "\tTraining batch 1831 Loss: 0.735183\n",
            "\tTraining batch 1832 Loss: 0.643699\n",
            "\tTraining batch 1833 Loss: 0.827276\n",
            "\tTraining batch 1834 Loss: 0.484047\n",
            "\tTraining batch 1835 Loss: 0.666950\n",
            "\tTraining batch 1836 Loss: 0.333963\n",
            "\tTraining batch 1837 Loss: 0.345031\n",
            "\tTraining batch 1838 Loss: 0.526364\n",
            "\tTraining batch 1839 Loss: 0.261990\n",
            "\tTraining batch 1840 Loss: 0.520805\n",
            "\tTraining batch 1841 Loss: 0.523423\n",
            "\tTraining batch 1842 Loss: 0.476065\n",
            "\tTraining batch 1843 Loss: 0.348553\n",
            "\tTraining batch 1844 Loss: 0.821853\n",
            "\tTraining batch 1845 Loss: 0.666303\n",
            "\tTraining batch 1846 Loss: 0.232907\n",
            "\tTraining batch 1847 Loss: 0.964167\n",
            "\tTraining batch 1848 Loss: 0.433387\n",
            "\tTraining batch 1849 Loss: 0.841840\n",
            "\tTraining batch 1850 Loss: 0.319581\n",
            "\tTraining batch 1851 Loss: 0.718624\n",
            "\tTraining batch 1852 Loss: 0.441251\n",
            "\tTraining batch 1853 Loss: 0.373578\n",
            "\tTraining batch 1854 Loss: 0.810114\n",
            "\tTraining batch 1855 Loss: 0.678709\n",
            "\tTraining batch 1856 Loss: 1.044864\n",
            "\tTraining batch 1857 Loss: 0.738052\n",
            "\tTraining batch 1858 Loss: 0.466515\n",
            "\tTraining batch 1859 Loss: 0.451102\n",
            "\tTraining batch 1860 Loss: 0.692380\n",
            "\tTraining batch 1861 Loss: 0.305748\n",
            "\tTraining batch 1862 Loss: 0.503637\n",
            "\tTraining batch 1863 Loss: 0.268797\n",
            "\tTraining batch 1864 Loss: 0.357086\n",
            "\tTraining batch 1865 Loss: 0.608681\n",
            "\tTraining batch 1866 Loss: 0.654488\n",
            "\tTraining batch 1867 Loss: 0.356551\n",
            "\tTraining batch 1868 Loss: 1.005217\n",
            "\tTraining batch 1869 Loss: 0.468163\n",
            "\tTraining batch 1870 Loss: 0.697468\n",
            "\tTraining batch 1871 Loss: 0.380006\n",
            "\tTraining batch 1872 Loss: 0.368194\n",
            "\tTraining batch 1873 Loss: 0.458438\n",
            "\tTraining batch 1874 Loss: 0.568359\n",
            "\tTraining batch 1875 Loss: 0.179827\n",
            "\tTraining batch 1876 Loss: 0.783963\n",
            "\tTraining batch 1877 Loss: 0.463686\n",
            "\tTraining batch 1878 Loss: 0.715578\n",
            "\tTraining batch 1879 Loss: 0.689421\n",
            "\tTraining batch 1880 Loss: 0.552084\n",
            "\tTraining batch 1881 Loss: 0.737864\n",
            "\tTraining batch 1882 Loss: 0.771817\n",
            "\tTraining batch 1883 Loss: 0.500822\n",
            "\tTraining batch 1884 Loss: 0.451106\n",
            "\tTraining batch 1885 Loss: 0.872799\n",
            "\tTraining batch 1886 Loss: 0.265788\n",
            "\tTraining batch 1887 Loss: 0.716553\n",
            "\tTraining batch 1888 Loss: 0.797271\n",
            "\tTraining batch 1889 Loss: 0.954235\n",
            "\tTraining batch 1890 Loss: 0.574782\n",
            "\tTraining batch 1891 Loss: 0.297342\n",
            "\tTraining batch 1892 Loss: 0.733625\n",
            "\tTraining batch 1893 Loss: 0.644555\n",
            "\tTraining batch 1894 Loss: 0.439361\n",
            "\tTraining batch 1895 Loss: 0.630741\n",
            "\tTraining batch 1896 Loss: 1.078049\n",
            "\tTraining batch 1897 Loss: 0.695653\n",
            "\tTraining batch 1898 Loss: 1.004377\n",
            "\tTraining batch 1899 Loss: 0.851847\n",
            "\tTraining batch 1900 Loss: 0.428034\n",
            "\tTraining batch 1901 Loss: 1.372278\n",
            "\tTraining batch 1902 Loss: 0.911490\n",
            "\tTraining batch 1903 Loss: 0.716146\n",
            "\tTraining batch 1904 Loss: 0.749016\n",
            "\tTraining batch 1905 Loss: 0.719681\n",
            "\tTraining batch 1906 Loss: 0.874152\n",
            "\tTraining batch 1907 Loss: 0.643865\n",
            "\tTraining batch 1908 Loss: 0.239330\n",
            "\tTraining batch 1909 Loss: 0.823898\n",
            "\tTraining batch 1910 Loss: 0.428904\n",
            "\tTraining batch 1911 Loss: 0.520806\n",
            "\tTraining batch 1912 Loss: 0.465167\n",
            "\tTraining batch 1913 Loss: 0.787677\n",
            "\tTraining batch 1914 Loss: 0.490560\n",
            "\tTraining batch 1915 Loss: 0.712364\n",
            "\tTraining batch 1916 Loss: 0.354717\n",
            "\tTraining batch 1917 Loss: 0.608793\n",
            "\tTraining batch 1918 Loss: 0.387740\n",
            "\tTraining batch 1919 Loss: 1.095483\n",
            "\tTraining batch 1920 Loss: 0.489091\n",
            "\tTraining batch 1921 Loss: 0.742950\n",
            "\tTraining batch 1922 Loss: 0.672454\n",
            "\tTraining batch 1923 Loss: 1.100450\n",
            "\tTraining batch 1924 Loss: 0.211861\n",
            "\tTraining batch 1925 Loss: 0.346494\n",
            "\tTraining batch 1926 Loss: 0.262148\n",
            "\tTraining batch 1927 Loss: 0.595922\n",
            "\tTraining batch 1928 Loss: 0.395626\n",
            "\tTraining batch 1929 Loss: 0.145961\n",
            "\tTraining batch 1930 Loss: 0.478180\n",
            "\tTraining batch 1931 Loss: 0.689473\n",
            "\tTraining batch 1932 Loss: 0.760873\n",
            "\tTraining batch 1933 Loss: 0.923923\n",
            "\tTraining batch 1934 Loss: 0.570308\n",
            "\tTraining batch 1935 Loss: 0.608511\n",
            "\tTraining batch 1936 Loss: 0.750350\n",
            "\tTraining batch 1937 Loss: 0.513033\n",
            "\tTraining batch 1938 Loss: 0.427179\n",
            "\tTraining batch 1939 Loss: 0.772158\n",
            "\tTraining batch 1940 Loss: 0.462194\n",
            "\tTraining batch 1941 Loss: 0.618724\n",
            "\tTraining batch 1942 Loss: 0.513204\n",
            "\tTraining batch 1943 Loss: 0.262477\n",
            "\tTraining batch 1944 Loss: 0.361193\n",
            "\tTraining batch 1945 Loss: 0.604932\n",
            "\tTraining batch 1946 Loss: 0.242818\n",
            "\tTraining batch 1947 Loss: 0.194098\n",
            "\tTraining batch 1948 Loss: 0.857798\n",
            "\tTraining batch 1949 Loss: 0.266592\n",
            "\tTraining batch 1950 Loss: 0.310915\n",
            "\tTraining batch 1951 Loss: 0.327657\n",
            "\tTraining batch 1952 Loss: 0.607873\n",
            "\tTraining batch 1953 Loss: 0.674905\n",
            "\tTraining batch 1954 Loss: 0.486463\n",
            "\tTraining batch 1955 Loss: 0.523114\n",
            "\tTraining batch 1956 Loss: 0.595860\n",
            "\tTraining batch 1957 Loss: 0.533109\n",
            "\tTraining batch 1958 Loss: 0.478147\n",
            "\tTraining batch 1959 Loss: 0.395967\n",
            "\tTraining batch 1960 Loss: 0.542492\n",
            "\tTraining batch 1961 Loss: 0.370139\n",
            "\tTraining batch 1962 Loss: 0.599272\n",
            "\tTraining batch 1963 Loss: 0.405874\n",
            "\tTraining batch 1964 Loss: 0.395788\n",
            "\tTraining batch 1965 Loss: 0.409084\n",
            "\tTraining batch 1966 Loss: 0.502358\n",
            "\tTraining batch 1967 Loss: 1.134065\n",
            "\tTraining batch 1968 Loss: 0.272554\n",
            "\tTraining batch 1969 Loss: 0.940293\n",
            "\tTraining batch 1970 Loss: 0.837396\n",
            "\tTraining batch 1971 Loss: 0.515189\n",
            "\tTraining batch 1972 Loss: 1.438475\n",
            "\tTraining batch 1973 Loss: 0.439761\n",
            "\tTraining batch 1974 Loss: 0.628093\n",
            "\tTraining batch 1975 Loss: 0.582831\n",
            "\tTraining batch 1976 Loss: 0.201606\n",
            "\tTraining batch 1977 Loss: 0.581292\n",
            "\tTraining batch 1978 Loss: 0.391086\n",
            "\tTraining batch 1979 Loss: 0.659911\n",
            "\tTraining batch 1980 Loss: 0.424028\n",
            "\tTraining batch 1981 Loss: 0.678805\n",
            "\tTraining batch 1982 Loss: 0.397887\n",
            "\tTraining batch 1983 Loss: 0.576304\n",
            "\tTraining batch 1984 Loss: 0.407838\n",
            "\tTraining batch 1985 Loss: 0.533298\n",
            "\tTraining batch 1986 Loss: 0.378177\n",
            "\tTraining batch 1987 Loss: 0.904389\n",
            "\tTraining batch 1988 Loss: 0.839413\n",
            "\tTraining batch 1989 Loss: 0.781430\n",
            "\tTraining batch 1990 Loss: 0.766253\n",
            "\tTraining batch 1991 Loss: 0.356078\n",
            "\tTraining batch 1992 Loss: 0.519078\n",
            "\tTraining batch 1993 Loss: 0.480202\n",
            "\tTraining batch 1994 Loss: 0.623510\n",
            "\tTraining batch 1995 Loss: 0.248602\n",
            "\tTraining batch 1996 Loss: 0.566796\n",
            "\tTraining batch 1997 Loss: 0.357867\n",
            "\tTraining batch 1998 Loss: 0.596114\n",
            "\tTraining batch 1999 Loss: 0.484481\n",
            "\tTraining batch 2000 Loss: 0.399380\n",
            "\tTraining batch 2001 Loss: 0.570141\n",
            "\tTraining batch 2002 Loss: 0.290008\n",
            "\tTraining batch 2003 Loss: 0.421721\n",
            "\tTraining batch 2004 Loss: 0.408708\n",
            "\tTraining batch 2005 Loss: 0.586083\n",
            "\tTraining batch 2006 Loss: 0.356657\n",
            "\tTraining batch 2007 Loss: 0.759635\n",
            "\tTraining batch 2008 Loss: 0.664340\n",
            "\tTraining batch 2009 Loss: 0.775382\n",
            "\tTraining batch 2010 Loss: 0.463010\n",
            "\tTraining batch 2011 Loss: 0.766789\n",
            "\tTraining batch 2012 Loss: 0.650582\n",
            "\tTraining batch 2013 Loss: 0.363429\n",
            "\tTraining batch 2014 Loss: 0.611196\n",
            "\tTraining batch 2015 Loss: 0.443643\n",
            "\tTraining batch 2016 Loss: 0.746646\n",
            "\tTraining batch 2017 Loss: 0.238408\n",
            "\tTraining batch 2018 Loss: 0.553003\n",
            "\tTraining batch 2019 Loss: 0.565127\n",
            "\tTraining batch 2020 Loss: 1.071348\n",
            "\tTraining batch 2021 Loss: 1.014470\n",
            "\tTraining batch 2022 Loss: 0.592264\n",
            "\tTraining batch 2023 Loss: 0.582714\n",
            "\tTraining batch 2024 Loss: 0.851340\n",
            "\tTraining batch 2025 Loss: 0.884970\n",
            "\tTraining batch 2026 Loss: 0.448096\n",
            "\tTraining batch 2027 Loss: 0.802143\n",
            "\tTraining batch 2028 Loss: 0.372932\n",
            "\tTraining batch 2029 Loss: 1.106972\n",
            "\tTraining batch 2030 Loss: 0.554720\n",
            "\tTraining batch 2031 Loss: 0.515767\n",
            "\tTraining batch 2032 Loss: 0.843893\n",
            "\tTraining batch 2033 Loss: 0.454138\n",
            "\tTraining batch 2034 Loss: 0.394429\n",
            "\tTraining batch 2035 Loss: 0.150075\n",
            "\tTraining batch 2036 Loss: 0.526726\n",
            "\tTraining batch 2037 Loss: 0.448744\n",
            "\tTraining batch 2038 Loss: 0.336144\n",
            "\tTraining batch 2039 Loss: 0.614366\n",
            "\tTraining batch 2040 Loss: 0.491155\n",
            "\tTraining batch 2041 Loss: 0.677382\n",
            "\tTraining batch 2042 Loss: 0.686799\n",
            "\tTraining batch 2043 Loss: 0.565716\n",
            "\tTraining batch 2044 Loss: 0.493508\n",
            "\tTraining batch 2045 Loss: 0.669367\n",
            "\tTraining batch 2046 Loss: 0.943093\n",
            "\tTraining batch 2047 Loss: 0.490208\n",
            "\tTraining batch 2048 Loss: 0.744830\n",
            "\tTraining batch 2049 Loss: 0.716251\n",
            "\tTraining batch 2050 Loss: 0.494074\n",
            "\tTraining batch 2051 Loss: 0.286007\n",
            "\tTraining batch 2052 Loss: 0.697416\n",
            "\tTraining batch 2053 Loss: 0.807374\n",
            "\tTraining batch 2054 Loss: 0.639308\n",
            "\tTraining batch 2055 Loss: 0.237815\n",
            "\tTraining batch 2056 Loss: 0.647749\n",
            "\tTraining batch 2057 Loss: 0.562568\n",
            "\tTraining batch 2058 Loss: 0.829536\n",
            "\tTraining batch 2059 Loss: 0.549376\n",
            "\tTraining batch 2060 Loss: 0.518118\n",
            "\tTraining batch 2061 Loss: 0.480490\n",
            "\tTraining batch 2062 Loss: 0.268630\n",
            "\tTraining batch 2063 Loss: 0.297757\n",
            "\tTraining batch 2064 Loss: 0.451080\n",
            "\tTraining batch 2065 Loss: 0.406558\n",
            "\tTraining batch 2066 Loss: 0.712394\n",
            "\tTraining batch 2067 Loss: 0.971804\n",
            "\tTraining batch 2068 Loss: 0.330974\n",
            "\tTraining batch 2069 Loss: 0.463255\n",
            "\tTraining batch 2070 Loss: 0.289513\n",
            "\tTraining batch 2071 Loss: 0.407109\n",
            "\tTraining batch 2072 Loss: 0.595187\n",
            "\tTraining batch 2073 Loss: 0.978836\n",
            "\tTraining batch 2074 Loss: 0.144354\n",
            "\tTraining batch 2075 Loss: 0.453968\n",
            "\tTraining batch 2076 Loss: 0.518703\n",
            "\tTraining batch 2077 Loss: 0.815222\n",
            "\tTraining batch 2078 Loss: 0.372416\n",
            "\tTraining batch 2079 Loss: 0.631684\n",
            "\tTraining batch 2080 Loss: 1.055297\n",
            "\tTraining batch 2081 Loss: 0.473628\n",
            "\tTraining batch 2082 Loss: 0.933809\n",
            "\tTraining batch 2083 Loss: 0.296707\n",
            "\tTraining batch 2084 Loss: 0.381646\n",
            "\tTraining batch 2085 Loss: 0.640078\n",
            "\tTraining batch 2086 Loss: 0.726359\n",
            "\tTraining batch 2087 Loss: 0.351568\n",
            "\tTraining batch 2088 Loss: 0.546723\n",
            "\tTraining batch 2089 Loss: 0.458249\n",
            "\tTraining batch 2090 Loss: 0.703771\n",
            "\tTraining batch 2091 Loss: 0.549278\n",
            "\tTraining batch 2092 Loss: 0.467428\n",
            "\tTraining batch 2093 Loss: 0.765544\n",
            "\tTraining batch 2094 Loss: 0.320189\n",
            "\tTraining batch 2095 Loss: 0.477019\n",
            "\tTraining batch 2096 Loss: 0.343184\n",
            "\tTraining batch 2097 Loss: 1.207983\n",
            "\tTraining batch 2098 Loss: 0.453229\n",
            "\tTraining batch 2099 Loss: 0.558200\n",
            "\tTraining batch 2100 Loss: 0.377428\n",
            "\tTraining batch 2101 Loss: 0.603861\n",
            "\tTraining batch 2102 Loss: 0.576355\n",
            "\tTraining batch 2103 Loss: 0.455833\n",
            "\tTraining batch 2104 Loss: 0.566983\n",
            "\tTraining batch 2105 Loss: 0.242099\n",
            "\tTraining batch 2106 Loss: 0.530594\n",
            "\tTraining batch 2107 Loss: 0.314219\n",
            "\tTraining batch 2108 Loss: 0.613010\n",
            "\tTraining batch 2109 Loss: 0.665418\n",
            "\tTraining batch 2110 Loss: 0.585536\n",
            "\tTraining batch 2111 Loss: 0.235245\n",
            "\tTraining batch 2112 Loss: 0.535574\n",
            "\tTraining batch 2113 Loss: 0.405564\n",
            "\tTraining batch 2114 Loss: 0.440588\n",
            "\tTraining batch 2115 Loss: 0.228635\n",
            "\tTraining batch 2116 Loss: 0.207512\n",
            "\tTraining batch 2117 Loss: 0.854780\n",
            "\tTraining batch 2118 Loss: 0.387652\n",
            "\tTraining batch 2119 Loss: 0.275288\n",
            "\tTraining batch 2120 Loss: 0.850061\n",
            "\tTraining batch 2121 Loss: 0.827585\n",
            "\tTraining batch 2122 Loss: 0.288760\n",
            "\tTraining batch 2123 Loss: 0.579232\n",
            "\tTraining batch 2124 Loss: 0.945919\n",
            "\tTraining batch 2125 Loss: 0.702344\n",
            "\tTraining batch 2126 Loss: 0.884594\n",
            "\tTraining batch 2127 Loss: 0.922756\n",
            "\tTraining batch 2128 Loss: 0.645944\n",
            "\tTraining batch 2129 Loss: 1.061122\n",
            "\tTraining batch 2130 Loss: 0.326918\n",
            "\tTraining batch 2131 Loss: 0.703579\n",
            "\tTraining batch 2132 Loss: 0.697529\n",
            "\tTraining batch 2133 Loss: 0.404862\n",
            "\tTraining batch 2134 Loss: 0.608299\n",
            "\tTraining batch 2135 Loss: 0.463386\n",
            "\tTraining batch 2136 Loss: 0.417197\n",
            "\tTraining batch 2137 Loss: 0.825839\n",
            "\tTraining batch 2138 Loss: 0.446035\n",
            "\tTraining batch 2139 Loss: 0.702047\n",
            "\tTraining batch 2140 Loss: 0.451518\n",
            "\tTraining batch 2141 Loss: 0.436220\n",
            "\tTraining batch 2142 Loss: 0.454559\n",
            "\tTraining batch 2143 Loss: 0.323486\n",
            "\tTraining batch 2144 Loss: 0.222258\n",
            "\tTraining batch 2145 Loss: 0.776907\n",
            "\tTraining batch 2146 Loss: 0.639252\n",
            "\tTraining batch 2147 Loss: 0.609652\n",
            "\tTraining batch 2148 Loss: 0.571004\n",
            "\tTraining batch 2149 Loss: 1.104328\n",
            "\tTraining batch 2150 Loss: 0.554450\n",
            "\tTraining batch 2151 Loss: 0.637999\n",
            "\tTraining batch 2152 Loss: 0.318526\n",
            "\tTraining batch 2153 Loss: 0.547955\n",
            "\tTraining batch 2154 Loss: 0.592315\n",
            "\tTraining batch 2155 Loss: 0.587867\n",
            "\tTraining batch 2156 Loss: 0.608684\n",
            "\tTraining batch 2157 Loss: 0.310782\n",
            "\tTraining batch 2158 Loss: 0.386663\n",
            "\tTraining batch 2159 Loss: 0.709079\n",
            "\tTraining batch 2160 Loss: 0.611736\n",
            "\tTraining batch 2161 Loss: 0.901458\n",
            "\tTraining batch 2162 Loss: 0.815755\n",
            "\tTraining batch 2163 Loss: 0.379809\n",
            "\tTraining batch 2164 Loss: 0.373101\n",
            "\tTraining batch 2165 Loss: 1.007669\n",
            "\tTraining batch 2166 Loss: 0.363457\n",
            "\tTraining batch 2167 Loss: 0.689385\n",
            "\tTraining batch 2168 Loss: 0.291945\n",
            "\tTraining batch 2169 Loss: 0.483812\n",
            "\tTraining batch 2170 Loss: 0.694201\n",
            "\tTraining batch 2171 Loss: 0.446055\n",
            "\tTraining batch 2172 Loss: 0.235102\n",
            "\tTraining batch 2173 Loss: 0.418896\n",
            "\tTraining batch 2174 Loss: 0.503925\n",
            "\tTraining batch 2175 Loss: 1.761296\n",
            "\tTraining batch 2176 Loss: 0.510221\n",
            "\tTraining batch 2177 Loss: 0.866961\n",
            "\tTraining batch 2178 Loss: 0.441585\n",
            "\tTraining batch 2179 Loss: 0.829655\n",
            "\tTraining batch 2180 Loss: 0.265180\n",
            "\tTraining batch 2181 Loss: 0.156832\n",
            "\tTraining batch 2182 Loss: 0.401387\n",
            "\tTraining batch 2183 Loss: 0.271981\n",
            "\tTraining batch 2184 Loss: 0.487426\n",
            "\tTraining batch 2185 Loss: 0.748880\n",
            "\tTraining batch 2186 Loss: 0.481870\n",
            "\tTraining batch 2187 Loss: 0.697249\n",
            "\tTraining batch 2188 Loss: 0.931015\n",
            "\tTraining batch 2189 Loss: 0.334634\n",
            "\tTraining batch 2190 Loss: 0.761738\n",
            "\tTraining batch 2191 Loss: 0.658847\n",
            "\tTraining batch 2192 Loss: 0.524530\n",
            "\tTraining batch 2193 Loss: 0.522666\n",
            "\tTraining batch 2194 Loss: 0.423886\n",
            "\tTraining batch 2195 Loss: 0.510545\n",
            "\tTraining batch 2196 Loss: 0.910904\n",
            "\tTraining batch 2197 Loss: 0.721669\n",
            "\tTraining batch 2198 Loss: 0.340657\n",
            "\tTraining batch 2199 Loss: 0.421824\n",
            "\tTraining batch 2200 Loss: 0.446808\n",
            "\tTraining batch 2201 Loss: 0.637069\n",
            "\tTraining batch 2202 Loss: 0.770484\n",
            "\tTraining batch 2203 Loss: 0.748602\n",
            "\tTraining batch 2204 Loss: 0.651236\n",
            "\tTraining batch 2205 Loss: 0.596001\n",
            "\tTraining batch 2206 Loss: 0.591337\n",
            "\tTraining batch 2207 Loss: 0.721042\n",
            "\tTraining batch 2208 Loss: 0.615954\n",
            "\tTraining batch 2209 Loss: 0.435143\n",
            "\tTraining batch 2210 Loss: 0.519872\n",
            "\tTraining batch 2211 Loss: 0.299700\n",
            "\tTraining batch 2212 Loss: 0.611392\n",
            "\tTraining batch 2213 Loss: 0.661824\n",
            "\tTraining batch 2214 Loss: 0.605409\n",
            "\tTraining batch 2215 Loss: 0.387950\n",
            "\tTraining batch 2216 Loss: 0.600969\n",
            "\tTraining batch 2217 Loss: 0.688203\n",
            "\tTraining batch 2218 Loss: 0.575756\n",
            "\tTraining batch 2219 Loss: 0.510676\n",
            "\tTraining batch 2220 Loss: 0.305614\n",
            "\tTraining batch 2221 Loss: 0.419818\n",
            "\tTraining batch 2222 Loss: 0.260969\n",
            "\tTraining batch 2223 Loss: 0.506920\n",
            "\tTraining batch 2224 Loss: 0.402731\n",
            "\tTraining batch 2225 Loss: 0.700811\n",
            "\tTraining batch 2226 Loss: 0.661908\n",
            "\tTraining batch 2227 Loss: 0.363909\n",
            "\tTraining batch 2228 Loss: 0.645438\n",
            "\tTraining batch 2229 Loss: 0.871993\n",
            "\tTraining batch 2230 Loss: 0.450828\n",
            "\tTraining batch 2231 Loss: 0.432420\n",
            "\tTraining batch 2232 Loss: 0.480950\n",
            "\tTraining batch 2233 Loss: 0.453019\n",
            "\tTraining batch 2234 Loss: 0.640360\n",
            "\tTraining batch 2235 Loss: 0.522689\n",
            "\tTraining batch 2236 Loss: 0.297452\n",
            "\tTraining batch 2237 Loss: 0.401599\n",
            "\tTraining batch 2238 Loss: 0.684551\n",
            "\tTraining batch 2239 Loss: 0.279155\n",
            "\tTraining batch 2240 Loss: 0.519044\n",
            "\tTraining batch 2241 Loss: 0.851512\n",
            "\tTraining batch 2242 Loss: 0.486440\n",
            "\tTraining batch 2243 Loss: 0.299472\n",
            "\tTraining batch 2244 Loss: 0.979385\n",
            "\tTraining batch 2245 Loss: 0.360178\n",
            "\tTraining batch 2246 Loss: 0.562135\n",
            "\tTraining batch 2247 Loss: 0.605206\n",
            "\tTraining batch 2248 Loss: 0.371239\n",
            "\tTraining batch 2249 Loss: 0.382954\n",
            "\tTraining batch 2250 Loss: 1.168789\n",
            "\tTraining batch 2251 Loss: 0.118750\n",
            "\tTraining batch 2252 Loss: 0.320996\n",
            "\tTraining batch 2253 Loss: 0.756188\n",
            "\tTraining batch 2254 Loss: 0.179731\n",
            "\tTraining batch 2255 Loss: 0.538956\n",
            "\tTraining batch 2256 Loss: 0.995993\n",
            "\tTraining batch 2257 Loss: 0.403793\n",
            "\tTraining batch 2258 Loss: 0.485877\n",
            "\tTraining batch 2259 Loss: 0.187847\n",
            "\tTraining batch 2260 Loss: 0.653461\n",
            "\tTraining batch 2261 Loss: 0.270419\n",
            "\tTraining batch 2262 Loss: 0.755077\n",
            "\tTraining batch 2263 Loss: 0.496487\n",
            "\tTraining batch 2264 Loss: 0.911991\n",
            "\tTraining batch 2265 Loss: 0.314510\n",
            "\tTraining batch 2266 Loss: 0.517765\n",
            "\tTraining batch 2267 Loss: 0.525332\n",
            "\tTraining batch 2268 Loss: 0.283717\n",
            "\tTraining batch 2269 Loss: 0.461723\n",
            "\tTraining batch 2270 Loss: 0.243244\n",
            "\tTraining batch 2271 Loss: 0.268266\n",
            "\tTraining batch 2272 Loss: 0.261271\n",
            "\tTraining batch 2273 Loss: 0.504504\n",
            "\tTraining batch 2274 Loss: 0.743379\n",
            "\tTraining batch 2275 Loss: 0.727502\n",
            "\tTraining batch 2276 Loss: 0.831347\n",
            "\tTraining batch 2277 Loss: 0.374236\n",
            "\tTraining batch 2278 Loss: 0.362331\n",
            "\tTraining batch 2279 Loss: 0.320756\n",
            "\tTraining batch 2280 Loss: 0.336408\n",
            "\tTraining batch 2281 Loss: 0.805076\n",
            "\tTraining batch 2282 Loss: 0.366143\n",
            "\tTraining batch 2283 Loss: 0.331439\n",
            "\tTraining batch 2284 Loss: 0.888171\n",
            "\tTraining batch 2285 Loss: 0.294740\n",
            "\tTraining batch 2286 Loss: 0.507071\n",
            "\tTraining batch 2287 Loss: 0.358764\n",
            "\tTraining batch 2288 Loss: 0.442128\n",
            "\tTraining batch 2289 Loss: 0.318040\n",
            "\tTraining batch 2290 Loss: 0.599713\n",
            "\tTraining batch 2291 Loss: 0.302051\n",
            "\tTraining batch 2292 Loss: 0.344496\n",
            "\tTraining batch 2293 Loss: 0.330650\n",
            "\tTraining batch 2294 Loss: 0.442240\n",
            "\tTraining batch 2295 Loss: 0.745008\n",
            "\tTraining batch 2296 Loss: 0.593199\n",
            "\tTraining batch 2297 Loss: 0.365074\n",
            "\tTraining batch 2298 Loss: 0.622127\n",
            "\tTraining batch 2299 Loss: 0.252620\n",
            "\tTraining batch 2300 Loss: 0.503882\n",
            "\tTraining batch 2301 Loss: 0.810070\n",
            "\tTraining batch 2302 Loss: 0.505751\n",
            "\tTraining batch 2303 Loss: 0.402300\n",
            "\tTraining batch 2304 Loss: 0.393028\n",
            "\tTraining batch 2305 Loss: 0.260114\n",
            "\tTraining batch 2306 Loss: 0.418682\n",
            "\tTraining batch 2307 Loss: 0.085718\n",
            "\tTraining batch 2308 Loss: 0.328740\n",
            "\tTraining batch 2309 Loss: 0.581152\n",
            "\tTraining batch 2310 Loss: 0.366961\n",
            "\tTraining batch 2311 Loss: 0.372771\n",
            "\tTraining batch 2312 Loss: 0.740885\n",
            "\tTraining batch 2313 Loss: 0.585859\n",
            "\tTraining batch 2314 Loss: 0.612743\n",
            "\tTraining batch 2315 Loss: 0.835733\n",
            "\tTraining batch 2316 Loss: 0.474178\n",
            "\tTraining batch 2317 Loss: 0.596990\n",
            "\tTraining batch 2318 Loss: 0.339165\n",
            "\tTraining batch 2319 Loss: 0.796094\n",
            "\tTraining batch 2320 Loss: 0.405938\n",
            "\tTraining batch 2321 Loss: 0.556181\n",
            "\tTraining batch 2322 Loss: 0.404852\n",
            "\tTraining batch 2323 Loss: 0.774340\n",
            "\tTraining batch 2324 Loss: 0.899366\n",
            "\tTraining batch 2325 Loss: 0.523534\n",
            "\tTraining batch 2326 Loss: 0.800400\n",
            "\tTraining batch 2327 Loss: 0.307086\n",
            "\tTraining batch 2328 Loss: 0.715194\n",
            "\tTraining batch 2329 Loss: 0.800841\n",
            "\tTraining batch 2330 Loss: 0.535086\n",
            "\tTraining batch 2331 Loss: 0.448290\n",
            "\tTraining batch 2332 Loss: 0.845481\n",
            "\tTraining batch 2333 Loss: 0.424550\n",
            "\tTraining batch 2334 Loss: 0.559955\n",
            "\tTraining batch 2335 Loss: 0.416937\n",
            "\tTraining batch 2336 Loss: 0.555989\n",
            "\tTraining batch 2337 Loss: 0.761678\n",
            "\tTraining batch 2338 Loss: 0.515581\n",
            "\tTraining batch 2339 Loss: 0.463854\n",
            "\tTraining batch 2340 Loss: 0.467687\n",
            "\tTraining batch 2341 Loss: 0.742840\n",
            "\tTraining batch 2342 Loss: 0.461023\n",
            "\tTraining batch 2343 Loss: 0.504566\n",
            "\tTraining batch 2344 Loss: 0.516022\n",
            "\tTraining batch 2345 Loss: 0.510544\n",
            "\tTraining batch 2346 Loss: 0.599240\n",
            "\tTraining batch 2347 Loss: 1.130843\n",
            "\tTraining batch 2348 Loss: 0.692123\n",
            "\tTraining batch 2349 Loss: 0.742089\n",
            "\tTraining batch 2350 Loss: 0.766217\n",
            "\tTraining batch 2351 Loss: 0.549354\n",
            "\tTraining batch 2352 Loss: 0.855351\n",
            "\tTraining batch 2353 Loss: 0.335960\n",
            "\tTraining batch 2354 Loss: 0.952371\n",
            "\tTraining batch 2355 Loss: 0.624642\n",
            "\tTraining batch 2356 Loss: 0.491370\n",
            "\tTraining batch 2357 Loss: 0.368439\n",
            "\tTraining batch 2358 Loss: 0.360774\n",
            "\tTraining batch 2359 Loss: 0.449210\n",
            "\tTraining batch 2360 Loss: 0.545672\n",
            "\tTraining batch 2361 Loss: 0.574316\n",
            "\tTraining batch 2362 Loss: 0.497589\n",
            "\tTraining batch 2363 Loss: 0.745463\n",
            "\tTraining batch 2364 Loss: 0.430873\n",
            "\tTraining batch 2365 Loss: 0.611429\n",
            "\tTraining batch 2366 Loss: 0.877954\n",
            "\tTraining batch 2367 Loss: 0.378597\n",
            "\tTraining batch 2368 Loss: 0.609071\n",
            "\tTraining batch 2369 Loss: 0.675253\n",
            "\tTraining batch 2370 Loss: 0.511708\n",
            "\tTraining batch 2371 Loss: 0.459663\n",
            "\tTraining batch 2372 Loss: 0.459408\n",
            "\tTraining batch 2373 Loss: 0.489472\n",
            "\tTraining batch 2374 Loss: 0.749713\n",
            "\tTraining batch 2375 Loss: 0.835310\n",
            "\tTraining batch 2376 Loss: 0.630070\n",
            "\tTraining batch 2377 Loss: 0.596248\n",
            "\tTraining batch 2378 Loss: 0.439440\n",
            "\tTraining batch 2379 Loss: 0.136105\n",
            "\tTraining batch 2380 Loss: 0.668050\n",
            "\tTraining batch 2381 Loss: 0.227896\n",
            "\tTraining batch 2382 Loss: 0.456842\n",
            "\tTraining batch 2383 Loss: 0.407708\n",
            "\tTraining batch 2384 Loss: 0.387510\n",
            "\tTraining batch 2385 Loss: 0.714536\n",
            "\tTraining batch 2386 Loss: 0.329476\n",
            "\tTraining batch 2387 Loss: 1.257331\n",
            "\tTraining batch 2388 Loss: 0.536066\n",
            "\tTraining batch 2389 Loss: 0.180793\n",
            "\tTraining batch 2390 Loss: 0.501989\n",
            "\tTraining batch 2391 Loss: 0.484354\n",
            "\tTraining batch 2392 Loss: 0.413588\n",
            "\tTraining batch 2393 Loss: 0.525652\n",
            "\tTraining batch 2394 Loss: 0.841439\n",
            "\tTraining batch 2395 Loss: 0.554596\n",
            "\tTraining batch 2396 Loss: 0.545789\n",
            "\tTraining batch 2397 Loss: 0.551159\n",
            "\tTraining batch 2398 Loss: 0.446529\n",
            "\tTraining batch 2399 Loss: 0.603171\n",
            "\tTraining batch 2400 Loss: 0.522988\n",
            "\tTraining batch 2401 Loss: 1.028338\n",
            "\tTraining batch 2402 Loss: 0.816830\n",
            "\tTraining batch 2403 Loss: 0.585184\n",
            "\tTraining batch 2404 Loss: 0.474506\n",
            "\tTraining batch 2405 Loss: 0.967323\n",
            "\tTraining batch 2406 Loss: 0.664928\n",
            "\tTraining batch 2407 Loss: 0.384613\n",
            "\tTraining batch 2408 Loss: 0.811391\n",
            "\tTraining batch 2409 Loss: 0.353192\n",
            "\tTraining batch 2410 Loss: 0.236775\n",
            "\tTraining batch 2411 Loss: 0.540573\n",
            "\tTraining batch 2412 Loss: 0.371355\n",
            "\tTraining batch 2413 Loss: 0.754161\n",
            "\tTraining batch 2414 Loss: 0.441971\n",
            "\tTraining batch 2415 Loss: 0.428093\n",
            "\tTraining batch 2416 Loss: 0.426334\n",
            "\tTraining batch 2417 Loss: 0.518392\n",
            "\tTraining batch 2418 Loss: 0.455558\n",
            "\tTraining batch 2419 Loss: 0.616980\n",
            "\tTraining batch 2420 Loss: 0.438813\n",
            "\tTraining batch 2421 Loss: 0.438111\n",
            "\tTraining batch 2422 Loss: 0.509785\n",
            "\tTraining batch 2423 Loss: 0.414472\n",
            "\tTraining batch 2424 Loss: 0.435608\n",
            "\tTraining batch 2425 Loss: 0.654741\n",
            "\tTraining batch 2426 Loss: 0.313297\n",
            "\tTraining batch 2427 Loss: 0.713629\n",
            "\tTraining batch 2428 Loss: 0.319307\n",
            "\tTraining batch 2429 Loss: 0.085334\n",
            "\tTraining batch 2430 Loss: 0.333241\n",
            "\tTraining batch 2431 Loss: 0.398608\n",
            "\tTraining batch 2432 Loss: 1.005394\n",
            "\tTraining batch 2433 Loss: 0.282501\n",
            "\tTraining batch 2434 Loss: 0.503919\n",
            "\tTraining batch 2435 Loss: 0.354945\n",
            "\tTraining batch 2436 Loss: 0.898742\n",
            "\tTraining batch 2437 Loss: 0.555503\n",
            "\tTraining batch 2438 Loss: 0.401795\n",
            "\tTraining batch 2439 Loss: 0.716874\n",
            "\tTraining batch 2440 Loss: 0.294050\n",
            "\tTraining batch 2441 Loss: 0.557331\n",
            "\tTraining batch 2442 Loss: 0.173322\n",
            "\tTraining batch 2443 Loss: 0.494789\n",
            "\tTraining batch 2444 Loss: 0.446759\n",
            "\tTraining batch 2445 Loss: 0.294679\n",
            "\tTraining batch 2446 Loss: 0.199197\n",
            "\tTraining batch 2447 Loss: 0.532721\n",
            "\tTraining batch 2448 Loss: 0.316974\n",
            "\tTraining batch 2449 Loss: 0.394030\n",
            "\tTraining batch 2450 Loss: 0.541814\n",
            "\tTraining batch 2451 Loss: 0.476139\n",
            "\tTraining batch 2452 Loss: 0.597006\n",
            "\tTraining batch 2453 Loss: 0.247265\n",
            "\tTraining batch 2454 Loss: 0.443364\n",
            "\tTraining batch 2455 Loss: 0.768521\n",
            "\tTraining batch 2456 Loss: 0.384019\n",
            "\tTraining batch 2457 Loss: 0.421785\n",
            "\tTraining batch 2458 Loss: 0.490350\n",
            "\tTraining batch 2459 Loss: 0.555865\n",
            "\tTraining batch 2460 Loss: 0.322922\n",
            "\tTraining batch 2461 Loss: 0.391623\n",
            "\tTraining batch 2462 Loss: 0.346603\n",
            "\tTraining batch 2463 Loss: 0.458520\n",
            "\tTraining batch 2464 Loss: 0.469078\n",
            "\tTraining batch 2465 Loss: 0.565831\n",
            "\tTraining batch 2466 Loss: 0.743542\n",
            "\tTraining batch 2467 Loss: 1.084520\n",
            "\tTraining batch 2468 Loss: 0.627255\n",
            "\tTraining batch 2469 Loss: 0.762423\n",
            "\tTraining batch 2470 Loss: 0.280641\n",
            "\tTraining batch 2471 Loss: 0.658673\n",
            "\tTraining batch 2472 Loss: 0.691355\n",
            "\tTraining batch 2473 Loss: 0.647384\n",
            "\tTraining batch 2474 Loss: 0.251039\n",
            "\tTraining batch 2475 Loss: 0.511945\n",
            "\tTraining batch 2476 Loss: 0.487620\n",
            "\tTraining batch 2477 Loss: 0.235092\n",
            "\tTraining batch 2478 Loss: 0.574087\n",
            "\tTraining batch 2479 Loss: 0.583950\n",
            "\tTraining batch 2480 Loss: 0.276201\n",
            "\tTraining batch 2481 Loss: 0.395235\n",
            "\tTraining batch 2482 Loss: 0.355236\n",
            "\tTraining batch 2483 Loss: 0.519803\n",
            "\tTraining batch 2484 Loss: 0.372261\n",
            "\tTraining batch 2485 Loss: 0.447136\n",
            "\tTraining batch 2486 Loss: 0.439351\n",
            "\tTraining batch 2487 Loss: 0.717654\n",
            "\tTraining batch 2488 Loss: 0.375889\n",
            "\tTraining batch 2489 Loss: 0.164952\n",
            "\tTraining batch 2490 Loss: 0.734431\n",
            "\tTraining batch 2491 Loss: 0.520128\n",
            "\tTraining batch 2492 Loss: 0.467873\n",
            "\tTraining batch 2493 Loss: 0.370231\n",
            "\tTraining batch 2494 Loss: 0.260874\n",
            "\tTraining batch 2495 Loss: 0.738547\n",
            "\tTraining batch 2496 Loss: 0.207026\n",
            "\tTraining batch 2497 Loss: 0.429184\n",
            "\tTraining batch 2498 Loss: 0.709025\n",
            "\tTraining batch 2499 Loss: 0.636384\n",
            "\tTraining batch 2500 Loss: 0.987052\n",
            "Epoca 910 \t Loss de Treinamento: 0.028083933517336845\n",
            "\tTraining batch 1 Loss: 0.223790\n",
            "\tTraining batch 2 Loss: 0.417322\n",
            "\tTraining batch 3 Loss: 0.794605\n",
            "\tTraining batch 4 Loss: 0.338287\n",
            "\tTraining batch 5 Loss: 0.357281\n",
            "\tTraining batch 6 Loss: 0.213975\n",
            "\tTraining batch 7 Loss: 0.289691\n",
            "\tTraining batch 8 Loss: 0.490269\n",
            "\tTraining batch 9 Loss: 0.393366\n",
            "\tTraining batch 10 Loss: 0.922363\n",
            "\tTraining batch 11 Loss: 0.424201\n",
            "\tTraining batch 12 Loss: 0.202175\n",
            "\tTraining batch 13 Loss: 0.946325\n",
            "\tTraining batch 14 Loss: 0.309130\n",
            "\tTraining batch 15 Loss: 0.529651\n",
            "\tTraining batch 16 Loss: 0.479235\n",
            "\tTraining batch 17 Loss: 0.404089\n",
            "\tTraining batch 18 Loss: 0.684972\n",
            "\tTraining batch 19 Loss: 0.371422\n",
            "\tTraining batch 20 Loss: 0.787861\n",
            "\tTraining batch 21 Loss: 0.425036\n",
            "\tTraining batch 22 Loss: 0.328454\n",
            "\tTraining batch 23 Loss: 0.500155\n",
            "\tTraining batch 24 Loss: 0.292345\n",
            "\tTraining batch 25 Loss: 0.301325\n",
            "\tTraining batch 26 Loss: 0.635472\n",
            "\tTraining batch 27 Loss: 0.395935\n",
            "\tTraining batch 28 Loss: 0.407205\n",
            "\tTraining batch 29 Loss: 0.437800\n",
            "\tTraining batch 30 Loss: 0.603136\n",
            "\tTraining batch 31 Loss: 0.799265\n",
            "\tTraining batch 32 Loss: 0.889026\n",
            "\tTraining batch 33 Loss: 0.253516\n",
            "\tTraining batch 34 Loss: 0.439242\n",
            "\tTraining batch 35 Loss: 0.358405\n",
            "\tTraining batch 36 Loss: 0.939708\n",
            "\tTraining batch 37 Loss: 0.873977\n",
            "\tTraining batch 38 Loss: 0.365557\n",
            "\tTraining batch 39 Loss: 0.462092\n",
            "\tTraining batch 40 Loss: 0.285861\n",
            "\tTraining batch 41 Loss: 0.892667\n",
            "\tTraining batch 42 Loss: 0.470120\n",
            "\tTraining batch 43 Loss: 0.744699\n",
            "\tTraining batch 44 Loss: 0.867025\n",
            "\tTraining batch 45 Loss: 0.616632\n",
            "\tTraining batch 46 Loss: 0.169136\n",
            "\tTraining batch 47 Loss: 0.605558\n",
            "\tTraining batch 48 Loss: 0.390531\n",
            "\tTraining batch 49 Loss: 0.159392\n",
            "\tTraining batch 50 Loss: 0.479134\n",
            "\tTraining batch 51 Loss: 0.822282\n",
            "\tTraining batch 52 Loss: 0.706339\n",
            "\tTraining batch 53 Loss: 0.828393\n",
            "\tTraining batch 54 Loss: 0.492757\n",
            "\tTraining batch 55 Loss: 0.328412\n",
            "\tTraining batch 56 Loss: 0.536206\n",
            "\tTraining batch 57 Loss: 0.718864\n",
            "\tTraining batch 58 Loss: 0.436859\n",
            "\tTraining batch 59 Loss: 0.791661\n",
            "\tTraining batch 60 Loss: 0.277365\n",
            "\tTraining batch 61 Loss: 0.466174\n",
            "\tTraining batch 62 Loss: 0.248397\n",
            "\tTraining batch 63 Loss: 0.346095\n",
            "\tTraining batch 64 Loss: 0.493313\n",
            "\tTraining batch 65 Loss: 0.656670\n",
            "\tTraining batch 66 Loss: 0.371890\n",
            "\tTraining batch 67 Loss: 0.594151\n",
            "\tTraining batch 68 Loss: 0.947473\n",
            "\tTraining batch 69 Loss: 0.500890\n",
            "\tTraining batch 70 Loss: 0.314508\n",
            "\tTraining batch 71 Loss: 0.530509\n",
            "\tTraining batch 72 Loss: 0.424053\n",
            "\tTraining batch 73 Loss: 0.478326\n",
            "\tTraining batch 74 Loss: 0.511296\n",
            "\tTraining batch 75 Loss: 0.795435\n",
            "\tTraining batch 76 Loss: 0.640753\n",
            "\tTraining batch 77 Loss: 0.784371\n",
            "\tTraining batch 78 Loss: 0.661719\n",
            "\tTraining batch 79 Loss: 0.805600\n",
            "\tTraining batch 80 Loss: 0.439015\n",
            "\tTraining batch 81 Loss: 0.720379\n",
            "\tTraining batch 82 Loss: 0.579199\n",
            "\tTraining batch 83 Loss: 0.518243\n",
            "\tTraining batch 84 Loss: 0.738432\n",
            "\tTraining batch 85 Loss: 0.302456\n",
            "\tTraining batch 86 Loss: 0.409521\n",
            "\tTraining batch 87 Loss: 0.367069\n",
            "\tTraining batch 88 Loss: 0.713601\n",
            "\tTraining batch 89 Loss: 0.381525\n",
            "\tTraining batch 90 Loss: 0.797963\n",
            "\tTraining batch 91 Loss: 0.389978\n",
            "\tTraining batch 92 Loss: 0.355601\n",
            "\tTraining batch 93 Loss: 0.203280\n",
            "\tTraining batch 94 Loss: 0.207361\n",
            "\tTraining batch 95 Loss: 0.391585\n",
            "\tTraining batch 96 Loss: 0.272471\n",
            "\tTraining batch 97 Loss: 0.657724\n",
            "\tTraining batch 98 Loss: 0.946712\n",
            "\tTraining batch 99 Loss: 0.725585\n",
            "\tTraining batch 100 Loss: 0.747969\n",
            "\tTraining batch 101 Loss: 0.759145\n",
            "\tTraining batch 102 Loss: 0.670995\n",
            "\tTraining batch 103 Loss: 0.569131\n",
            "\tTraining batch 104 Loss: 0.539310\n",
            "\tTraining batch 105 Loss: 0.280506\n",
            "\tTraining batch 106 Loss: 0.481632\n",
            "\tTraining batch 107 Loss: 0.486450\n",
            "\tTraining batch 108 Loss: 0.461793\n",
            "\tTraining batch 109 Loss: 0.393809\n",
            "\tTraining batch 110 Loss: 0.712457\n",
            "\tTraining batch 111 Loss: 0.530696\n",
            "\tTraining batch 112 Loss: 0.689857\n",
            "\tTraining batch 113 Loss: 0.321195\n",
            "\tTraining batch 114 Loss: 0.500403\n",
            "\tTraining batch 115 Loss: 0.392266\n",
            "\tTraining batch 116 Loss: 0.870273\n",
            "\tTraining batch 117 Loss: 0.620622\n",
            "\tTraining batch 118 Loss: 0.480962\n",
            "\tTraining batch 119 Loss: 0.249695\n",
            "\tTraining batch 120 Loss: 0.234885\n",
            "\tTraining batch 121 Loss: 0.619617\n",
            "\tTraining batch 122 Loss: 0.243377\n",
            "\tTraining batch 123 Loss: 0.428530\n",
            "\tTraining batch 124 Loss: 0.172507\n",
            "\tTraining batch 125 Loss: 0.275139\n",
            "\tTraining batch 126 Loss: 0.345949\n",
            "\tTraining batch 127 Loss: 0.739821\n",
            "\tTraining batch 128 Loss: 0.513053\n",
            "\tTraining batch 129 Loss: 0.284039\n",
            "\tTraining batch 130 Loss: 0.288948\n",
            "\tTraining batch 131 Loss: 0.387674\n",
            "\tTraining batch 132 Loss: 0.382166\n",
            "\tTraining batch 133 Loss: 0.106301\n",
            "\tTraining batch 134 Loss: 0.197886\n",
            "\tTraining batch 135 Loss: 0.597360\n",
            "\tTraining batch 136 Loss: 0.617124\n",
            "\tTraining batch 137 Loss: 0.571047\n",
            "\tTraining batch 138 Loss: 0.523777\n",
            "\tTraining batch 139 Loss: 0.390380\n",
            "\tTraining batch 140 Loss: 0.800364\n",
            "\tTraining batch 141 Loss: 0.351763\n",
            "\tTraining batch 142 Loss: 0.454916\n",
            "\tTraining batch 143 Loss: 0.772757\n",
            "\tTraining batch 144 Loss: 0.217465\n",
            "\tTraining batch 145 Loss: 0.675911\n",
            "\tTraining batch 146 Loss: 0.260231\n",
            "\tTraining batch 147 Loss: 0.482937\n",
            "\tTraining batch 148 Loss: 0.541073\n",
            "\tTraining batch 149 Loss: 0.406578\n",
            "\tTraining batch 150 Loss: 0.464615\n",
            "\tTraining batch 151 Loss: 0.803129\n",
            "\tTraining batch 152 Loss: 0.808798\n",
            "\tTraining batch 153 Loss: 0.734159\n",
            "\tTraining batch 154 Loss: 0.965793\n",
            "\tTraining batch 155 Loss: 0.495177\n",
            "\tTraining batch 156 Loss: 0.458138\n",
            "\tTraining batch 157 Loss: 0.689808\n",
            "\tTraining batch 158 Loss: 0.727945\n",
            "\tTraining batch 159 Loss: 0.592919\n",
            "\tTraining batch 160 Loss: 0.890559\n",
            "\tTraining batch 161 Loss: 0.292011\n",
            "\tTraining batch 162 Loss: 0.398188\n",
            "\tTraining batch 163 Loss: 0.482959\n",
            "\tTraining batch 164 Loss: 0.381091\n",
            "\tTraining batch 165 Loss: 0.975356\n",
            "\tTraining batch 166 Loss: 0.342313\n",
            "\tTraining batch 167 Loss: 0.663596\n",
            "\tTraining batch 168 Loss: 0.367531\n",
            "\tTraining batch 169 Loss: 0.818903\n",
            "\tTraining batch 170 Loss: 1.164556\n",
            "\tTraining batch 171 Loss: 0.530358\n",
            "\tTraining batch 172 Loss: 0.475571\n",
            "\tTraining batch 173 Loss: 0.600822\n",
            "\tTraining batch 174 Loss: 0.222636\n",
            "\tTraining batch 175 Loss: 0.687858\n",
            "\tTraining batch 176 Loss: 0.327495\n",
            "\tTraining batch 177 Loss: 0.662664\n",
            "\tTraining batch 178 Loss: 0.594052\n",
            "\tTraining batch 179 Loss: 0.305235\n",
            "\tTraining batch 180 Loss: 0.465076\n",
            "\tTraining batch 181 Loss: 0.615048\n",
            "\tTraining batch 182 Loss: 0.307436\n",
            "\tTraining batch 183 Loss: 0.574425\n",
            "\tTraining batch 184 Loss: 0.301662\n",
            "\tTraining batch 185 Loss: 0.267525\n",
            "\tTraining batch 186 Loss: 0.541311\n",
            "\tTraining batch 187 Loss: 0.159645\n",
            "\tTraining batch 188 Loss: 0.689347\n",
            "\tTraining batch 189 Loss: 0.488436\n",
            "\tTraining batch 190 Loss: 0.542370\n",
            "\tTraining batch 191 Loss: 0.687764\n",
            "\tTraining batch 192 Loss: 0.303876\n",
            "\tTraining batch 193 Loss: 0.518339\n",
            "\tTraining batch 194 Loss: 0.812548\n",
            "\tTraining batch 195 Loss: 0.536015\n",
            "\tTraining batch 196 Loss: 0.737389\n",
            "\tTraining batch 197 Loss: 0.553972\n",
            "\tTraining batch 198 Loss: 0.429223\n",
            "\tTraining batch 199 Loss: 0.124786\n",
            "\tTraining batch 200 Loss: 0.695960\n",
            "\tTraining batch 201 Loss: 0.613085\n",
            "\tTraining batch 202 Loss: 0.240800\n",
            "\tTraining batch 203 Loss: 0.528337\n",
            "\tTraining batch 204 Loss: 0.377817\n",
            "\tTraining batch 205 Loss: 0.371456\n",
            "\tTraining batch 206 Loss: 0.716923\n",
            "\tTraining batch 207 Loss: 0.741634\n",
            "\tTraining batch 208 Loss: 0.777116\n",
            "\tTraining batch 209 Loss: 0.993086\n",
            "\tTraining batch 210 Loss: 0.423920\n",
            "\tTraining batch 211 Loss: 0.475715\n",
            "\tTraining batch 212 Loss: 0.475621\n",
            "\tTraining batch 213 Loss: 0.662870\n",
            "\tTraining batch 214 Loss: 0.390200\n",
            "\tTraining batch 215 Loss: 0.376117\n",
            "\tTraining batch 216 Loss: 0.718758\n",
            "\tTraining batch 217 Loss: 0.606346\n",
            "\tTraining batch 218 Loss: 0.634891\n",
            "\tTraining batch 219 Loss: 0.515402\n",
            "\tTraining batch 220 Loss: 0.249130\n",
            "\tTraining batch 221 Loss: 0.408859\n",
            "\tTraining batch 222 Loss: 0.613355\n",
            "\tTraining batch 223 Loss: 0.691316\n",
            "\tTraining batch 224 Loss: 0.156014\n",
            "\tTraining batch 225 Loss: 1.142652\n",
            "\tTraining batch 226 Loss: 0.377440\n",
            "\tTraining batch 227 Loss: 0.519258\n",
            "\tTraining batch 228 Loss: 0.384602\n",
            "\tTraining batch 229 Loss: 0.438153\n",
            "\tTraining batch 230 Loss: 0.276926\n",
            "\tTraining batch 231 Loss: 0.130513\n",
            "\tTraining batch 232 Loss: 0.294501\n",
            "\tTraining batch 233 Loss: 0.439661\n",
            "\tTraining batch 234 Loss: 0.549048\n",
            "\tTraining batch 235 Loss: 0.434887\n",
            "\tTraining batch 236 Loss: 0.334359\n",
            "\tTraining batch 237 Loss: 0.222538\n",
            "\tTraining batch 238 Loss: 0.498037\n",
            "\tTraining batch 239 Loss: 0.415066\n",
            "\tTraining batch 240 Loss: 0.531621\n",
            "\tTraining batch 241 Loss: 0.495924\n",
            "\tTraining batch 242 Loss: 0.380071\n",
            "\tTraining batch 243 Loss: 1.280493\n",
            "\tTraining batch 244 Loss: 0.223040\n",
            "\tTraining batch 245 Loss: 0.505779\n",
            "\tTraining batch 246 Loss: 0.661862\n",
            "\tTraining batch 247 Loss: 0.598881\n",
            "\tTraining batch 248 Loss: 0.285674\n",
            "\tTraining batch 249 Loss: 0.747929\n",
            "\tTraining batch 250 Loss: 0.703772\n",
            "\tTraining batch 251 Loss: 0.777543\n",
            "\tTraining batch 252 Loss: 0.514078\n",
            "\tTraining batch 253 Loss: 0.308542\n",
            "\tTraining batch 254 Loss: 0.929931\n",
            "\tTraining batch 255 Loss: 1.388463\n",
            "\tTraining batch 256 Loss: 0.755991\n",
            "\tTraining batch 257 Loss: 0.728512\n",
            "\tTraining batch 258 Loss: 0.672211\n",
            "\tTraining batch 259 Loss: 0.526285\n",
            "\tTraining batch 260 Loss: 0.597565\n",
            "\tTraining batch 261 Loss: 0.552889\n",
            "\tTraining batch 262 Loss: 0.556604\n",
            "\tTraining batch 263 Loss: 0.512637\n",
            "\tTraining batch 264 Loss: 0.382077\n",
            "\tTraining batch 265 Loss: 0.548969\n",
            "\tTraining batch 266 Loss: 0.557460\n",
            "\tTraining batch 267 Loss: 0.753018\n",
            "\tTraining batch 268 Loss: 0.609274\n",
            "\tTraining batch 269 Loss: 0.573258\n",
            "\tTraining batch 270 Loss: 0.653679\n",
            "\tTraining batch 271 Loss: 0.728747\n",
            "\tTraining batch 272 Loss: 0.725059\n",
            "\tTraining batch 273 Loss: 0.746655\n",
            "\tTraining batch 274 Loss: 0.289842\n",
            "\tTraining batch 275 Loss: 0.405829\n",
            "\tTraining batch 276 Loss: 0.450640\n",
            "\tTraining batch 277 Loss: 0.803601\n",
            "\tTraining batch 278 Loss: 0.929996\n",
            "\tTraining batch 279 Loss: 0.247139\n",
            "\tTraining batch 280 Loss: 0.778803\n",
            "\tTraining batch 281 Loss: 0.750158\n",
            "\tTraining batch 282 Loss: 0.916964\n",
            "\tTraining batch 283 Loss: 0.500045\n",
            "\tTraining batch 284 Loss: 0.255103\n",
            "\tTraining batch 285 Loss: 0.572005\n",
            "\tTraining batch 286 Loss: 0.435850\n",
            "\tTraining batch 287 Loss: 0.466395\n",
            "\tTraining batch 288 Loss: 0.513106\n",
            "\tTraining batch 289 Loss: 0.219602\n",
            "\tTraining batch 290 Loss: 0.532562\n",
            "\tTraining batch 291 Loss: 0.547669\n",
            "\tTraining batch 292 Loss: 0.716027\n",
            "\tTraining batch 293 Loss: 0.738857\n",
            "\tTraining batch 294 Loss: 0.888242\n",
            "\tTraining batch 295 Loss: 0.200837\n",
            "\tTraining batch 296 Loss: 0.449490\n",
            "\tTraining batch 297 Loss: 0.296337\n",
            "\tTraining batch 298 Loss: 0.426253\n",
            "\tTraining batch 299 Loss: 0.627929\n",
            "\tTraining batch 300 Loss: 0.274395\n",
            "\tTraining batch 301 Loss: 0.513083\n",
            "\tTraining batch 302 Loss: 0.776113\n",
            "\tTraining batch 303 Loss: 0.344449\n",
            "\tTraining batch 304 Loss: 0.510521\n",
            "\tTraining batch 305 Loss: 0.840948\n",
            "\tTraining batch 306 Loss: 0.815346\n",
            "\tTraining batch 307 Loss: 0.669218\n",
            "\tTraining batch 308 Loss: 0.336580\n",
            "\tTraining batch 309 Loss: 0.509784\n",
            "\tTraining batch 310 Loss: 0.518322\n",
            "\tTraining batch 311 Loss: 0.715001\n",
            "\tTraining batch 312 Loss: 0.590647\n",
            "\tTraining batch 313 Loss: 0.473307\n",
            "\tTraining batch 314 Loss: 0.957032\n",
            "\tTraining batch 315 Loss: 0.676403\n",
            "\tTraining batch 316 Loss: 0.770331\n",
            "\tTraining batch 317 Loss: 0.709513\n",
            "\tTraining batch 318 Loss: 0.309690\n",
            "\tTraining batch 319 Loss: 0.563496\n",
            "\tTraining batch 320 Loss: 0.511171\n",
            "\tTraining batch 321 Loss: 0.477145\n",
            "\tTraining batch 322 Loss: 0.557172\n",
            "\tTraining batch 323 Loss: 0.430297\n",
            "\tTraining batch 324 Loss: 0.932523\n",
            "\tTraining batch 325 Loss: 0.444261\n",
            "\tTraining batch 326 Loss: 0.531622\n",
            "\tTraining batch 327 Loss: 0.435193\n",
            "\tTraining batch 328 Loss: 0.610407\n",
            "\tTraining batch 329 Loss: 0.455840\n",
            "\tTraining batch 330 Loss: 0.347097\n",
            "\tTraining batch 331 Loss: 0.590008\n",
            "\tTraining batch 332 Loss: 0.286377\n",
            "\tTraining batch 333 Loss: 0.451693\n",
            "\tTraining batch 334 Loss: 0.162750\n",
            "\tTraining batch 335 Loss: 0.816110\n",
            "\tTraining batch 336 Loss: 0.236698\n",
            "\tTraining batch 337 Loss: 0.204093\n",
            "\tTraining batch 338 Loss: 0.388441\n",
            "\tTraining batch 339 Loss: 0.462237\n",
            "\tTraining batch 340 Loss: 0.321474\n",
            "\tTraining batch 341 Loss: 0.261982\n",
            "\tTraining batch 342 Loss: 0.440075\n",
            "\tTraining batch 343 Loss: 0.418984\n",
            "\tTraining batch 344 Loss: 0.751945\n",
            "\tTraining batch 345 Loss: 0.528872\n",
            "\tTraining batch 346 Loss: 0.613934\n",
            "\tTraining batch 347 Loss: 0.581335\n",
            "\tTraining batch 348 Loss: 0.748846\n",
            "\tTraining batch 349 Loss: 0.348695\n",
            "\tTraining batch 350 Loss: 0.444274\n",
            "\tTraining batch 351 Loss: 0.517790\n",
            "\tTraining batch 352 Loss: 0.475116\n",
            "\tTraining batch 353 Loss: 0.816133\n",
            "\tTraining batch 354 Loss: 1.106801\n",
            "\tTraining batch 355 Loss: 0.623252\n",
            "\tTraining batch 356 Loss: 0.586256\n",
            "\tTraining batch 357 Loss: 0.163668\n",
            "\tTraining batch 358 Loss: 0.423421\n",
            "\tTraining batch 359 Loss: 0.440449\n",
            "\tTraining batch 360 Loss: 1.204094\n",
            "\tTraining batch 361 Loss: 0.661297\n",
            "\tTraining batch 362 Loss: 0.498242\n",
            "\tTraining batch 363 Loss: 0.314621\n",
            "\tTraining batch 364 Loss: 0.636194\n",
            "\tTraining batch 365 Loss: 0.173842\n",
            "\tTraining batch 366 Loss: 0.285475\n",
            "\tTraining batch 367 Loss: 0.087183\n",
            "\tTraining batch 368 Loss: 0.277524\n",
            "\tTraining batch 369 Loss: 0.501786\n",
            "\tTraining batch 370 Loss: 0.945559\n",
            "\tTraining batch 371 Loss: 0.702541\n",
            "\tTraining batch 372 Loss: 0.584279\n",
            "\tTraining batch 373 Loss: 0.790900\n",
            "\tTraining batch 374 Loss: 0.481339\n",
            "\tTraining batch 375 Loss: 0.438595\n",
            "\tTraining batch 376 Loss: 0.531298\n",
            "\tTraining batch 377 Loss: 0.490235\n",
            "\tTraining batch 378 Loss: 0.621337\n",
            "\tTraining batch 379 Loss: 0.413573\n",
            "\tTraining batch 380 Loss: 0.641948\n",
            "\tTraining batch 381 Loss: 0.204669\n",
            "\tTraining batch 382 Loss: 0.522650\n",
            "\tTraining batch 383 Loss: 0.666807\n",
            "\tTraining batch 384 Loss: 0.586826\n",
            "\tTraining batch 385 Loss: 0.745368\n",
            "\tTraining batch 386 Loss: 0.753960\n",
            "\tTraining batch 387 Loss: 0.727361\n",
            "\tTraining batch 388 Loss: 0.428786\n",
            "\tTraining batch 389 Loss: 0.586366\n",
            "\tTraining batch 390 Loss: 0.323961\n",
            "\tTraining batch 391 Loss: 0.518544\n",
            "\tTraining batch 392 Loss: 0.668546\n",
            "\tTraining batch 393 Loss: 0.972312\n",
            "\tTraining batch 394 Loss: 0.249204\n",
            "\tTraining batch 395 Loss: 0.893609\n",
            "\tTraining batch 396 Loss: 0.616062\n",
            "\tTraining batch 397 Loss: 0.196262\n",
            "\tTraining batch 398 Loss: 0.504360\n",
            "\tTraining batch 399 Loss: 0.465816\n",
            "\tTraining batch 400 Loss: 0.395834\n",
            "\tTraining batch 401 Loss: 0.508233\n",
            "\tTraining batch 402 Loss: 0.601935\n",
            "\tTraining batch 403 Loss: 0.780663\n",
            "\tTraining batch 404 Loss: 0.513162\n",
            "\tTraining batch 405 Loss: 0.443879\n",
            "\tTraining batch 406 Loss: 0.482433\n",
            "\tTraining batch 407 Loss: 0.370702\n",
            "\tTraining batch 408 Loss: 0.466281\n",
            "\tTraining batch 409 Loss: 0.636906\n",
            "\tTraining batch 410 Loss: 0.257330\n",
            "\tTraining batch 411 Loss: 0.425172\n",
            "\tTraining batch 412 Loss: 0.535873\n",
            "\tTraining batch 413 Loss: 0.632986\n",
            "\tTraining batch 414 Loss: 0.362139\n",
            "\tTraining batch 415 Loss: 0.574353\n",
            "\tTraining batch 416 Loss: 0.547928\n",
            "\tTraining batch 417 Loss: 0.520081\n",
            "\tTraining batch 418 Loss: 0.729169\n",
            "\tTraining batch 419 Loss: 0.290351\n",
            "\tTraining batch 420 Loss: 0.446882\n",
            "\tTraining batch 421 Loss: 0.543546\n",
            "\tTraining batch 422 Loss: 0.485926\n",
            "\tTraining batch 423 Loss: 1.052664\n",
            "\tTraining batch 424 Loss: 0.789737\n",
            "\tTraining batch 425 Loss: 0.857648\n",
            "\tTraining batch 426 Loss: 0.380748\n",
            "\tTraining batch 427 Loss: 1.165321\n",
            "\tTraining batch 428 Loss: 0.435835\n",
            "\tTraining batch 429 Loss: 0.323885\n",
            "\tTraining batch 430 Loss: 1.090899\n",
            "\tTraining batch 431 Loss: 0.472453\n",
            "\tTraining batch 432 Loss: 0.293795\n",
            "\tTraining batch 433 Loss: 0.792381\n",
            "\tTraining batch 434 Loss: 0.138177\n",
            "\tTraining batch 435 Loss: 0.313139\n",
            "\tTraining batch 436 Loss: 0.539893\n",
            "\tTraining batch 437 Loss: 0.732990\n",
            "\tTraining batch 438 Loss: 0.350477\n",
            "\tTraining batch 439 Loss: 0.655891\n",
            "\tTraining batch 440 Loss: 0.317775\n",
            "\tTraining batch 441 Loss: 0.470611\n",
            "\tTraining batch 442 Loss: 0.235990\n",
            "\tTraining batch 443 Loss: 0.843734\n",
            "\tTraining batch 444 Loss: 0.283811\n",
            "\tTraining batch 445 Loss: 0.392654\n",
            "\tTraining batch 446 Loss: 0.248383\n",
            "\tTraining batch 447 Loss: 0.910522\n",
            "\tTraining batch 448 Loss: 0.764883\n",
            "\tTraining batch 449 Loss: 0.590773\n",
            "\tTraining batch 450 Loss: 1.025297\n",
            "\tTraining batch 451 Loss: 1.091228\n",
            "\tTraining batch 452 Loss: 0.896031\n",
            "\tTraining batch 453 Loss: 0.407277\n",
            "\tTraining batch 454 Loss: 0.516709\n",
            "\tTraining batch 455 Loss: 0.372475\n",
            "\tTraining batch 456 Loss: 0.339896\n",
            "\tTraining batch 457 Loss: 0.680819\n",
            "\tTraining batch 458 Loss: 0.813253\n",
            "\tTraining batch 459 Loss: 0.396260\n",
            "\tTraining batch 460 Loss: 0.710730\n",
            "\tTraining batch 461 Loss: 0.298402\n",
            "\tTraining batch 462 Loss: 0.824797\n",
            "\tTraining batch 463 Loss: 0.498578\n",
            "\tTraining batch 464 Loss: 0.208933\n",
            "\tTraining batch 465 Loss: 1.156780\n",
            "\tTraining batch 466 Loss: 0.435279\n",
            "\tTraining batch 467 Loss: 0.569664\n",
            "\tTraining batch 468 Loss: 0.760094\n",
            "\tTraining batch 469 Loss: 0.589507\n",
            "\tTraining batch 470 Loss: 0.683721\n",
            "\tTraining batch 471 Loss: 0.295513\n",
            "\tTraining batch 472 Loss: 0.158822\n",
            "\tTraining batch 473 Loss: 1.234952\n",
            "\tTraining batch 474 Loss: 0.866636\n",
            "\tTraining batch 475 Loss: 0.454921\n",
            "\tTraining batch 476 Loss: 0.247289\n",
            "\tTraining batch 477 Loss: 0.233054\n",
            "\tTraining batch 478 Loss: 0.235507\n",
            "\tTraining batch 479 Loss: 0.681767\n",
            "\tTraining batch 480 Loss: 0.699395\n",
            "\tTraining batch 481 Loss: 0.352266\n",
            "\tTraining batch 482 Loss: 0.460888\n",
            "\tTraining batch 483 Loss: 0.737000\n",
            "\tTraining batch 484 Loss: 0.199908\n",
            "\tTraining batch 485 Loss: 0.570124\n",
            "\tTraining batch 486 Loss: 0.510102\n",
            "\tTraining batch 487 Loss: 0.387436\n",
            "\tTraining batch 488 Loss: 1.190006\n",
            "\tTraining batch 489 Loss: 0.469625\n",
            "\tTraining batch 490 Loss: 0.646153\n",
            "\tTraining batch 491 Loss: 0.428015\n",
            "\tTraining batch 492 Loss: 0.601772\n",
            "\tTraining batch 493 Loss: 0.557571\n",
            "\tTraining batch 494 Loss: 0.558579\n",
            "\tTraining batch 495 Loss: 0.145613\n",
            "\tTraining batch 496 Loss: 0.471690\n",
            "\tTraining batch 497 Loss: 0.494854\n",
            "\tTraining batch 498 Loss: 0.325143\n",
            "\tTraining batch 499 Loss: 0.386571\n",
            "\tTraining batch 500 Loss: 0.982544\n",
            "\tTraining batch 501 Loss: 0.198153\n",
            "\tTraining batch 502 Loss: 1.400084\n",
            "\tTraining batch 503 Loss: 0.293716\n",
            "\tTraining batch 504 Loss: 0.178759\n",
            "\tTraining batch 505 Loss: 0.556424\n",
            "\tTraining batch 506 Loss: 0.234269\n",
            "\tTraining batch 507 Loss: 0.317587\n",
            "\tTraining batch 508 Loss: 0.421899\n",
            "\tTraining batch 509 Loss: 1.151705\n",
            "\tTraining batch 510 Loss: 0.796695\n",
            "\tTraining batch 511 Loss: 0.752704\n",
            "\tTraining batch 512 Loss: 0.657928\n",
            "\tTraining batch 513 Loss: 0.691386\n",
            "\tTraining batch 514 Loss: 0.367318\n",
            "\tTraining batch 515 Loss: 0.787206\n",
            "\tTraining batch 516 Loss: 0.425678\n",
            "\tTraining batch 517 Loss: 0.455821\n",
            "\tTraining batch 518 Loss: 0.225499\n",
            "\tTraining batch 519 Loss: 0.372704\n",
            "\tTraining batch 520 Loss: 0.611002\n",
            "\tTraining batch 521 Loss: 0.269440\n",
            "\tTraining batch 522 Loss: 1.000945\n",
            "\tTraining batch 523 Loss: 0.452470\n",
            "\tTraining batch 524 Loss: 0.538265\n",
            "\tTraining batch 525 Loss: 0.606317\n",
            "\tTraining batch 526 Loss: 0.573119\n",
            "\tTraining batch 527 Loss: 0.434572\n",
            "\tTraining batch 528 Loss: 0.445494\n",
            "\tTraining batch 529 Loss: 0.566933\n",
            "\tTraining batch 530 Loss: 0.849586\n",
            "\tTraining batch 531 Loss: 0.693303\n",
            "\tTraining batch 532 Loss: 0.474837\n",
            "\tTraining batch 533 Loss: 0.593454\n",
            "\tTraining batch 534 Loss: 0.437020\n",
            "\tTraining batch 535 Loss: 0.725669\n",
            "\tTraining batch 536 Loss: 0.438790\n",
            "\tTraining batch 537 Loss: 0.454845\n",
            "\tTraining batch 538 Loss: 0.616970\n",
            "\tTraining batch 539 Loss: 0.223656\n",
            "\tTraining batch 540 Loss: 0.205038\n",
            "\tTraining batch 541 Loss: 0.512064\n",
            "\tTraining batch 542 Loss: 0.409988\n",
            "\tTraining batch 543 Loss: 1.046099\n",
            "\tTraining batch 544 Loss: 0.480676\n",
            "\tTraining batch 545 Loss: 0.587679\n",
            "\tTraining batch 546 Loss: 0.726274\n",
            "\tTraining batch 547 Loss: 0.628581\n",
            "\tTraining batch 548 Loss: 0.364781\n",
            "\tTraining batch 549 Loss: 0.309759\n",
            "\tTraining batch 550 Loss: 0.550729\n",
            "\tTraining batch 551 Loss: 0.935466\n",
            "\tTraining batch 552 Loss: 0.715362\n",
            "\tTraining batch 553 Loss: 0.416352\n",
            "\tTraining batch 554 Loss: 0.609712\n",
            "\tTraining batch 555 Loss: 0.473729\n",
            "\tTraining batch 556 Loss: 0.581313\n",
            "\tTraining batch 557 Loss: 0.484940\n",
            "\tTraining batch 558 Loss: 0.644229\n",
            "\tTraining batch 559 Loss: 0.535916\n",
            "\tTraining batch 560 Loss: 0.864908\n",
            "\tTraining batch 561 Loss: 0.325014\n",
            "\tTraining batch 562 Loss: 0.664587\n",
            "\tTraining batch 563 Loss: 0.354728\n",
            "\tTraining batch 564 Loss: 0.607551\n",
            "\tTraining batch 565 Loss: 0.715079\n",
            "\tTraining batch 566 Loss: 0.481113\n",
            "\tTraining batch 567 Loss: 0.684517\n",
            "\tTraining batch 568 Loss: 0.511149\n",
            "\tTraining batch 569 Loss: 0.451984\n",
            "\tTraining batch 570 Loss: 0.280093\n",
            "\tTraining batch 571 Loss: 0.400383\n",
            "\tTraining batch 572 Loss: 0.479888\n",
            "\tTraining batch 573 Loss: 0.698083\n",
            "\tTraining batch 574 Loss: 0.226408\n",
            "\tTraining batch 575 Loss: 0.560521\n",
            "\tTraining batch 576 Loss: 1.117178\n",
            "\tTraining batch 577 Loss: 0.404107\n",
            "\tTraining batch 578 Loss: 0.414100\n",
            "\tTraining batch 579 Loss: 0.574325\n",
            "\tTraining batch 580 Loss: 0.475196\n",
            "\tTraining batch 581 Loss: 0.272091\n",
            "\tTraining batch 582 Loss: 0.592838\n",
            "\tTraining batch 583 Loss: 0.791107\n",
            "\tTraining batch 584 Loss: 0.502926\n",
            "\tTraining batch 585 Loss: 0.254144\n",
            "\tTraining batch 586 Loss: 0.608431\n",
            "\tTraining batch 587 Loss: 0.843799\n",
            "\tTraining batch 588 Loss: 0.338702\n",
            "\tTraining batch 589 Loss: 0.991031\n",
            "\tTraining batch 590 Loss: 0.739386\n",
            "\tTraining batch 591 Loss: 0.449236\n",
            "\tTraining batch 592 Loss: 0.790749\n",
            "\tTraining batch 593 Loss: 0.944321\n",
            "\tTraining batch 594 Loss: 0.480198\n",
            "\tTraining batch 595 Loss: 0.681502\n",
            "\tTraining batch 596 Loss: 0.766667\n",
            "\tTraining batch 597 Loss: 0.366970\n",
            "\tTraining batch 598 Loss: 0.477799\n",
            "\tTraining batch 599 Loss: 0.679780\n",
            "\tTraining batch 600 Loss: 0.294421\n",
            "\tTraining batch 601 Loss: 0.390623\n",
            "\tTraining batch 602 Loss: 0.542031\n",
            "\tTraining batch 603 Loss: 0.750491\n",
            "\tTraining batch 604 Loss: 0.695630\n",
            "\tTraining batch 605 Loss: 0.408893\n",
            "\tTraining batch 606 Loss: 0.530068\n",
            "\tTraining batch 607 Loss: 0.300613\n",
            "\tTraining batch 608 Loss: 0.887379\n",
            "\tTraining batch 609 Loss: 0.312605\n",
            "\tTraining batch 610 Loss: 1.292055\n",
            "\tTraining batch 611 Loss: 0.636550\n",
            "\tTraining batch 612 Loss: 0.484882\n",
            "\tTraining batch 613 Loss: 0.354941\n",
            "\tTraining batch 614 Loss: 0.459894\n",
            "\tTraining batch 615 Loss: 0.610929\n",
            "\tTraining batch 616 Loss: 0.844616\n",
            "\tTraining batch 617 Loss: 0.516726\n",
            "\tTraining batch 618 Loss: 0.693791\n",
            "\tTraining batch 619 Loss: 0.402576\n",
            "\tTraining batch 620 Loss: 0.534055\n",
            "\tTraining batch 621 Loss: 0.596253\n",
            "\tTraining batch 622 Loss: 0.642374\n",
            "\tTraining batch 623 Loss: 0.690900\n",
            "\tTraining batch 624 Loss: 0.531930\n",
            "\tTraining batch 625 Loss: 0.967099\n",
            "\tTraining batch 626 Loss: 1.214571\n",
            "\tTraining batch 627 Loss: 0.682034\n",
            "\tTraining batch 628 Loss: 0.271955\n",
            "\tTraining batch 629 Loss: 0.485741\n",
            "\tTraining batch 630 Loss: 1.057675\n",
            "\tTraining batch 631 Loss: 0.337572\n",
            "\tTraining batch 632 Loss: 0.572116\n",
            "\tTraining batch 633 Loss: 0.631869\n",
            "\tTraining batch 634 Loss: 0.577557\n",
            "\tTraining batch 635 Loss: 0.584117\n",
            "\tTraining batch 636 Loss: 0.698175\n",
            "\tTraining batch 637 Loss: 0.518254\n",
            "\tTraining batch 638 Loss: 0.677030\n",
            "\tTraining batch 639 Loss: 0.325371\n",
            "\tTraining batch 640 Loss: 0.708129\n",
            "\tTraining batch 641 Loss: 0.627586\n",
            "\tTraining batch 642 Loss: 0.447995\n",
            "\tTraining batch 643 Loss: 0.250390\n",
            "\tTraining batch 644 Loss: 0.510323\n",
            "\tTraining batch 645 Loss: 0.607518\n",
            "\tTraining batch 646 Loss: 0.532455\n",
            "\tTraining batch 647 Loss: 0.311406\n",
            "\tTraining batch 648 Loss: 0.710539\n",
            "\tTraining batch 649 Loss: 0.636859\n",
            "\tTraining batch 650 Loss: 0.269282\n",
            "\tTraining batch 651 Loss: 0.713753\n",
            "\tTraining batch 652 Loss: 0.765713\n",
            "\tTraining batch 653 Loss: 0.437648\n",
            "\tTraining batch 654 Loss: 0.670508\n",
            "\tTraining batch 655 Loss: 0.751690\n",
            "\tTraining batch 656 Loss: 0.311288\n",
            "\tTraining batch 657 Loss: 0.265242\n",
            "\tTraining batch 658 Loss: 0.361892\n",
            "\tTraining batch 659 Loss: 0.578832\n",
            "\tTraining batch 660 Loss: 0.638470\n",
            "\tTraining batch 661 Loss: 0.439910\n",
            "\tTraining batch 662 Loss: 0.488931\n",
            "\tTraining batch 663 Loss: 0.721105\n",
            "\tTraining batch 664 Loss: 0.607541\n",
            "\tTraining batch 665 Loss: 0.464144\n",
            "\tTraining batch 666 Loss: 0.911223\n",
            "\tTraining batch 667 Loss: 0.297081\n",
            "\tTraining batch 668 Loss: 0.556447\n",
            "\tTraining batch 669 Loss: 0.736479\n",
            "\tTraining batch 670 Loss: 0.738504\n",
            "\tTraining batch 671 Loss: 0.727967\n",
            "\tTraining batch 672 Loss: 0.471300\n",
            "\tTraining batch 673 Loss: 0.241975\n",
            "\tTraining batch 674 Loss: 0.827498\n",
            "\tTraining batch 675 Loss: 0.317956\n",
            "\tTraining batch 676 Loss: 0.517641\n",
            "\tTraining batch 677 Loss: 0.465499\n",
            "\tTraining batch 678 Loss: 0.394820\n",
            "\tTraining batch 679 Loss: 0.165598\n",
            "\tTraining batch 680 Loss: 0.406111\n",
            "\tTraining batch 681 Loss: 0.459612\n",
            "\tTraining batch 682 Loss: 0.446985\n",
            "\tTraining batch 683 Loss: 0.667127\n",
            "\tTraining batch 684 Loss: 0.656785\n",
            "\tTraining batch 685 Loss: 1.112800\n",
            "\tTraining batch 686 Loss: 0.229807\n",
            "\tTraining batch 687 Loss: 0.554545\n",
            "\tTraining batch 688 Loss: 0.448209\n",
            "\tTraining batch 689 Loss: 0.688513\n",
            "\tTraining batch 690 Loss: 0.723634\n",
            "\tTraining batch 691 Loss: 0.643601\n",
            "\tTraining batch 692 Loss: 0.433337\n",
            "\tTraining batch 693 Loss: 0.359093\n",
            "\tTraining batch 694 Loss: 0.714326\n",
            "\tTraining batch 695 Loss: 0.895599\n",
            "\tTraining batch 696 Loss: 0.579722\n",
            "\tTraining batch 697 Loss: 0.560786\n",
            "\tTraining batch 698 Loss: 0.423753\n",
            "\tTraining batch 699 Loss: 0.597318\n",
            "\tTraining batch 700 Loss: 0.958461\n",
            "\tTraining batch 701 Loss: 0.844543\n",
            "\tTraining batch 702 Loss: 0.766480\n",
            "\tTraining batch 703 Loss: 0.240789\n",
            "\tTraining batch 704 Loss: 0.811778\n",
            "\tTraining batch 705 Loss: 0.198449\n",
            "\tTraining batch 706 Loss: 1.045370\n",
            "\tTraining batch 707 Loss: 0.416942\n",
            "\tTraining batch 708 Loss: 0.639624\n",
            "\tTraining batch 709 Loss: 0.741261\n",
            "\tTraining batch 710 Loss: 1.109559\n",
            "\tTraining batch 711 Loss: 0.242398\n",
            "\tTraining batch 712 Loss: 0.505735\n",
            "\tTraining batch 713 Loss: 0.570276\n",
            "\tTraining batch 714 Loss: 0.518743\n",
            "\tTraining batch 715 Loss: 0.503039\n",
            "\tTraining batch 716 Loss: 0.359107\n",
            "\tTraining batch 717 Loss: 0.534939\n",
            "\tTraining batch 718 Loss: 0.753019\n",
            "\tTraining batch 719 Loss: 0.714997\n",
            "\tTraining batch 720 Loss: 0.337433\n",
            "\tTraining batch 721 Loss: 0.937353\n",
            "\tTraining batch 722 Loss: 0.426524\n",
            "\tTraining batch 723 Loss: 0.437664\n",
            "\tTraining batch 724 Loss: 0.534559\n",
            "\tTraining batch 725 Loss: 0.574296\n",
            "\tTraining batch 726 Loss: 0.406937\n",
            "\tTraining batch 727 Loss: 0.513588\n",
            "\tTraining batch 728 Loss: 0.616317\n",
            "\tTraining batch 729 Loss: 0.603461\n",
            "\tTraining batch 730 Loss: 0.358558\n",
            "\tTraining batch 731 Loss: 0.900093\n",
            "\tTraining batch 732 Loss: 0.387910\n",
            "\tTraining batch 733 Loss: 0.450050\n",
            "\tTraining batch 734 Loss: 0.411621\n",
            "\tTraining batch 735 Loss: 0.830183\n",
            "\tTraining batch 736 Loss: 0.911057\n",
            "\tTraining batch 737 Loss: 0.309291\n",
            "\tTraining batch 738 Loss: 0.492894\n",
            "\tTraining batch 739 Loss: 0.431808\n",
            "\tTraining batch 740 Loss: 0.710023\n",
            "\tTraining batch 741 Loss: 0.799070\n",
            "\tTraining batch 742 Loss: 0.600396\n",
            "\tTraining batch 743 Loss: 0.890692\n",
            "\tTraining batch 744 Loss: 0.456739\n",
            "\tTraining batch 745 Loss: 0.342741\n",
            "\tTraining batch 746 Loss: 0.336730\n",
            "\tTraining batch 747 Loss: 0.714631\n",
            "\tTraining batch 748 Loss: 0.650386\n",
            "\tTraining batch 749 Loss: 0.960469\n",
            "\tTraining batch 750 Loss: 0.353104\n",
            "\tTraining batch 751 Loss: 0.172725\n",
            "\tTraining batch 752 Loss: 0.338697\n",
            "\tTraining batch 753 Loss: 0.422321\n",
            "\tTraining batch 754 Loss: 0.344545\n",
            "\tTraining batch 755 Loss: 0.459242\n",
            "\tTraining batch 756 Loss: 0.708202\n",
            "\tTraining batch 757 Loss: 0.675856\n",
            "\tTraining batch 758 Loss: 0.341064\n",
            "\tTraining batch 759 Loss: 0.387385\n",
            "\tTraining batch 760 Loss: 0.399192\n",
            "\tTraining batch 761 Loss: 0.931979\n",
            "\tTraining batch 762 Loss: 0.793672\n",
            "\tTraining batch 763 Loss: 0.437804\n",
            "\tTraining batch 764 Loss: 0.500789\n",
            "\tTraining batch 765 Loss: 0.478211\n",
            "\tTraining batch 766 Loss: 0.457128\n",
            "\tTraining batch 767 Loss: 0.589934\n",
            "\tTraining batch 768 Loss: 0.563513\n",
            "\tTraining batch 769 Loss: 0.381911\n",
            "\tTraining batch 770 Loss: 0.372200\n",
            "\tTraining batch 771 Loss: 0.314835\n",
            "\tTraining batch 772 Loss: 0.586016\n",
            "\tTraining batch 773 Loss: 1.006467\n",
            "\tTraining batch 774 Loss: 0.394185\n",
            "\tTraining batch 775 Loss: 0.887352\n",
            "\tTraining batch 776 Loss: 0.857706\n",
            "\tTraining batch 777 Loss: 0.315971\n",
            "\tTraining batch 778 Loss: 0.331117\n",
            "\tTraining batch 779 Loss: 0.479747\n",
            "\tTraining batch 780 Loss: 0.761556\n",
            "\tTraining batch 781 Loss: 0.836334\n",
            "\tTraining batch 782 Loss: 0.627611\n",
            "\tTraining batch 783 Loss: 0.482227\n",
            "\tTraining batch 784 Loss: 0.414051\n",
            "\tTraining batch 785 Loss: 0.689584\n",
            "\tTraining batch 786 Loss: 0.891351\n",
            "\tTraining batch 787 Loss: 0.820381\n",
            "\tTraining batch 788 Loss: 0.606318\n",
            "\tTraining batch 789 Loss: 0.420819\n",
            "\tTraining batch 790 Loss: 0.468704\n",
            "\tTraining batch 791 Loss: 0.576813\n",
            "\tTraining batch 792 Loss: 0.980091\n",
            "\tTraining batch 793 Loss: 0.727723\n",
            "\tTraining batch 794 Loss: 0.689727\n",
            "\tTraining batch 795 Loss: 0.484747\n",
            "\tTraining batch 796 Loss: 0.787317\n",
            "\tTraining batch 797 Loss: 0.335452\n",
            "\tTraining batch 798 Loss: 1.129700\n",
            "\tTraining batch 799 Loss: 0.559742\n",
            "\tTraining batch 800 Loss: 0.634324\n",
            "\tTraining batch 801 Loss: 0.543793\n",
            "\tTraining batch 802 Loss: 0.729242\n",
            "\tTraining batch 803 Loss: 0.443860\n",
            "\tTraining batch 804 Loss: 0.368160\n",
            "\tTraining batch 805 Loss: 0.954101\n",
            "\tTraining batch 806 Loss: 0.719540\n",
            "\tTraining batch 807 Loss: 0.520563\n",
            "\tTraining batch 808 Loss: 0.519483\n",
            "\tTraining batch 809 Loss: 0.491591\n",
            "\tTraining batch 810 Loss: 0.381456\n",
            "\tTraining batch 811 Loss: 0.363750\n",
            "\tTraining batch 812 Loss: 0.335936\n",
            "\tTraining batch 813 Loss: 0.518142\n",
            "\tTraining batch 814 Loss: 0.520829\n",
            "\tTraining batch 815 Loss: 0.774581\n",
            "\tTraining batch 816 Loss: 0.421861\n",
            "\tTraining batch 817 Loss: 0.095902\n",
            "\tTraining batch 818 Loss: 0.538990\n",
            "\tTraining batch 819 Loss: 0.858669\n",
            "\tTraining batch 820 Loss: 0.647470\n",
            "\tTraining batch 821 Loss: 0.471963\n",
            "\tTraining batch 822 Loss: 0.826996\n",
            "\tTraining batch 823 Loss: 0.355942\n",
            "\tTraining batch 824 Loss: 0.508226\n",
            "\tTraining batch 825 Loss: 0.723385\n",
            "\tTraining batch 826 Loss: 0.323415\n",
            "\tTraining batch 827 Loss: 0.586766\n",
            "\tTraining batch 828 Loss: 0.576311\n",
            "\tTraining batch 829 Loss: 0.704391\n",
            "\tTraining batch 830 Loss: 0.686893\n",
            "\tTraining batch 831 Loss: 0.527143\n",
            "\tTraining batch 832 Loss: 0.583986\n",
            "\tTraining batch 833 Loss: 0.828894\n",
            "\tTraining batch 834 Loss: 0.372910\n",
            "\tTraining batch 835 Loss: 0.829594\n",
            "\tTraining batch 836 Loss: 0.434133\n",
            "\tTraining batch 837 Loss: 0.443341\n",
            "\tTraining batch 838 Loss: 0.437872\n",
            "\tTraining batch 839 Loss: 0.166598\n",
            "\tTraining batch 840 Loss: 0.319456\n",
            "\tTraining batch 841 Loss: 0.904211\n",
            "\tTraining batch 842 Loss: 0.330221\n",
            "\tTraining batch 843 Loss: 0.488584\n",
            "\tTraining batch 844 Loss: 0.728359\n",
            "\tTraining batch 845 Loss: 0.437435\n",
            "\tTraining batch 846 Loss: 0.717146\n",
            "\tTraining batch 847 Loss: 0.380433\n",
            "\tTraining batch 848 Loss: 0.308047\n",
            "\tTraining batch 849 Loss: 0.438989\n",
            "\tTraining batch 850 Loss: 0.559129\n",
            "\tTraining batch 851 Loss: 0.311707\n",
            "\tTraining batch 852 Loss: 0.846502\n",
            "\tTraining batch 853 Loss: 0.525757\n",
            "\tTraining batch 854 Loss: 0.587365\n",
            "\tTraining batch 855 Loss: 0.712750\n",
            "\tTraining batch 856 Loss: 0.320340\n",
            "\tTraining batch 857 Loss: 0.502892\n",
            "\tTraining batch 858 Loss: 0.449939\n",
            "\tTraining batch 859 Loss: 0.558454\n",
            "\tTraining batch 860 Loss: 0.509335\n",
            "\tTraining batch 861 Loss: 0.610227\n",
            "\tTraining batch 862 Loss: 0.739385\n",
            "\tTraining batch 863 Loss: 0.347072\n",
            "\tTraining batch 864 Loss: 0.495032\n",
            "\tTraining batch 865 Loss: 0.518850\n",
            "\tTraining batch 866 Loss: 0.815239\n",
            "\tTraining batch 867 Loss: 0.339100\n",
            "\tTraining batch 868 Loss: 0.600664\n",
            "\tTraining batch 869 Loss: 0.677981\n",
            "\tTraining batch 870 Loss: 0.459190\n",
            "\tTraining batch 871 Loss: 0.766646\n",
            "\tTraining batch 872 Loss: 0.456444\n",
            "\tTraining batch 873 Loss: 0.441935\n",
            "\tTraining batch 874 Loss: 0.445603\n",
            "\tTraining batch 875 Loss: 0.485959\n",
            "\tTraining batch 876 Loss: 0.504349\n",
            "\tTraining batch 877 Loss: 0.434917\n",
            "\tTraining batch 878 Loss: 0.498486\n",
            "\tTraining batch 879 Loss: 0.794499\n",
            "\tTraining batch 880 Loss: 0.658234\n",
            "\tTraining batch 881 Loss: 0.442567\n",
            "\tTraining batch 882 Loss: 0.302097\n",
            "\tTraining batch 883 Loss: 0.861539\n",
            "\tTraining batch 884 Loss: 0.417205\n",
            "\tTraining batch 885 Loss: 0.653773\n",
            "\tTraining batch 886 Loss: 0.718284\n",
            "\tTraining batch 887 Loss: 0.578994\n",
            "\tTraining batch 888 Loss: 0.512661\n",
            "\tTraining batch 889 Loss: 0.731933\n",
            "\tTraining batch 890 Loss: 0.637908\n",
            "\tTraining batch 891 Loss: 0.220801\n",
            "\tTraining batch 892 Loss: 0.656897\n",
            "\tTraining batch 893 Loss: 0.392145\n",
            "\tTraining batch 894 Loss: 0.460537\n",
            "\tTraining batch 895 Loss: 0.361417\n",
            "\tTraining batch 896 Loss: 0.423445\n",
            "\tTraining batch 897 Loss: 0.832474\n",
            "\tTraining batch 898 Loss: 0.181517\n",
            "\tTraining batch 899 Loss: 0.246105\n",
            "\tTraining batch 900 Loss: 0.432067\n",
            "\tTraining batch 901 Loss: 0.667913\n",
            "\tTraining batch 902 Loss: 0.448733\n",
            "\tTraining batch 903 Loss: 0.855662\n",
            "\tTraining batch 904 Loss: 0.320425\n",
            "\tTraining batch 905 Loss: 0.917176\n",
            "\tTraining batch 906 Loss: 0.557556\n",
            "\tTraining batch 907 Loss: 0.368121\n",
            "\tTraining batch 908 Loss: 0.841831\n",
            "\tTraining batch 909 Loss: 0.789014\n",
            "\tTraining batch 910 Loss: 0.443004\n",
            "\tTraining batch 911 Loss: 0.375574\n",
            "\tTraining batch 912 Loss: 0.530252\n",
            "\tTraining batch 913 Loss: 0.589773\n",
            "\tTraining batch 914 Loss: 0.549779\n",
            "\tTraining batch 915 Loss: 0.754231\n",
            "\tTraining batch 916 Loss: 0.947189\n",
            "\tTraining batch 917 Loss: 0.470217\n",
            "\tTraining batch 918 Loss: 0.195911\n",
            "\tTraining batch 919 Loss: 0.503803\n",
            "\tTraining batch 920 Loss: 0.603640\n",
            "\tTraining batch 921 Loss: 0.524975\n",
            "\tTraining batch 922 Loss: 0.495313\n",
            "\tTraining batch 923 Loss: 0.302180\n",
            "\tTraining batch 924 Loss: 0.568079\n",
            "\tTraining batch 925 Loss: 0.540388\n",
            "\tTraining batch 926 Loss: 0.292900\n",
            "\tTraining batch 927 Loss: 0.464010\n",
            "\tTraining batch 928 Loss: 0.778758\n",
            "\tTraining batch 929 Loss: 0.634633\n",
            "\tTraining batch 930 Loss: 0.246641\n",
            "\tTraining batch 931 Loss: 0.507923\n",
            "\tTraining batch 932 Loss: 0.545941\n",
            "\tTraining batch 933 Loss: 0.485716\n",
            "\tTraining batch 934 Loss: 0.953088\n",
            "\tTraining batch 935 Loss: 0.328188\n",
            "\tTraining batch 936 Loss: 0.507319\n",
            "\tTraining batch 937 Loss: 0.577211\n",
            "\tTraining batch 938 Loss: 0.286373\n",
            "\tTraining batch 939 Loss: 0.958694\n",
            "\tTraining batch 940 Loss: 1.043482\n",
            "\tTraining batch 941 Loss: 0.423336\n",
            "\tTraining batch 942 Loss: 0.594547\n",
            "\tTraining batch 943 Loss: 0.901405\n",
            "\tTraining batch 944 Loss: 0.475471\n",
            "\tTraining batch 945 Loss: 0.665450\n",
            "\tTraining batch 946 Loss: 0.417053\n",
            "\tTraining batch 947 Loss: 0.317005\n",
            "\tTraining batch 948 Loss: 0.460723\n",
            "\tTraining batch 949 Loss: 0.661948\n",
            "\tTraining batch 950 Loss: 0.393755\n",
            "\tTraining batch 951 Loss: 0.522321\n",
            "\tTraining batch 952 Loss: 0.439315\n",
            "\tTraining batch 953 Loss: 0.718883\n",
            "\tTraining batch 954 Loss: 0.232048\n",
            "\tTraining batch 955 Loss: 0.675343\n",
            "\tTraining batch 956 Loss: 0.365531\n",
            "\tTraining batch 957 Loss: 0.472507\n",
            "\tTraining batch 958 Loss: 0.806123\n",
            "\tTraining batch 959 Loss: 0.354869\n",
            "\tTraining batch 960 Loss: 0.798364\n",
            "\tTraining batch 961 Loss: 0.695958\n",
            "\tTraining batch 962 Loss: 0.799499\n",
            "\tTraining batch 963 Loss: 0.743872\n",
            "\tTraining batch 964 Loss: 0.643198\n",
            "\tTraining batch 965 Loss: 0.839910\n",
            "\tTraining batch 966 Loss: 0.423501\n",
            "\tTraining batch 967 Loss: 0.357086\n",
            "\tTraining batch 968 Loss: 0.451542\n",
            "\tTraining batch 969 Loss: 0.327881\n",
            "\tTraining batch 970 Loss: 0.831028\n",
            "\tTraining batch 971 Loss: 0.424560\n",
            "\tTraining batch 972 Loss: 0.381923\n",
            "\tTraining batch 973 Loss: 0.673605\n",
            "\tTraining batch 974 Loss: 0.652520\n",
            "\tTraining batch 975 Loss: 0.612454\n",
            "\tTraining batch 976 Loss: 0.304772\n",
            "\tTraining batch 977 Loss: 0.732128\n",
            "\tTraining batch 978 Loss: 0.821449\n",
            "\tTraining batch 979 Loss: 0.861519\n",
            "\tTraining batch 980 Loss: 0.365331\n",
            "\tTraining batch 981 Loss: 0.436491\n",
            "\tTraining batch 982 Loss: 0.278566\n",
            "\tTraining batch 983 Loss: 0.312687\n",
            "\tTraining batch 984 Loss: 0.383847\n",
            "\tTraining batch 985 Loss: 0.467673\n",
            "\tTraining batch 986 Loss: 0.885327\n",
            "\tTraining batch 987 Loss: 0.745450\n",
            "\tTraining batch 988 Loss: 0.632077\n",
            "\tTraining batch 989 Loss: 0.224227\n",
            "\tTraining batch 990 Loss: 0.862934\n",
            "\tTraining batch 991 Loss: 0.590205\n",
            "\tTraining batch 992 Loss: 0.672802\n",
            "\tTraining batch 993 Loss: 0.536901\n",
            "\tTraining batch 994 Loss: 0.321545\n",
            "\tTraining batch 995 Loss: 0.330457\n",
            "\tTraining batch 996 Loss: 0.801562\n",
            "\tTraining batch 997 Loss: 0.318096\n",
            "\tTraining batch 998 Loss: 0.760409\n",
            "\tTraining batch 999 Loss: 0.446298\n",
            "\tTraining batch 1000 Loss: 0.475067\n",
            "\tTraining batch 1001 Loss: 0.546802\n",
            "\tTraining batch 1002 Loss: 0.488035\n",
            "\tTraining batch 1003 Loss: 0.364100\n",
            "\tTraining batch 1004 Loss: 0.693822\n",
            "\tTraining batch 1005 Loss: 0.374914\n",
            "\tTraining batch 1006 Loss: 0.507982\n",
            "\tTraining batch 1007 Loss: 0.593248\n",
            "\tTraining batch 1008 Loss: 0.510506\n",
            "\tTraining batch 1009 Loss: 0.714990\n",
            "\tTraining batch 1010 Loss: 0.392403\n",
            "\tTraining batch 1011 Loss: 0.685321\n",
            "\tTraining batch 1012 Loss: 0.493877\n",
            "\tTraining batch 1013 Loss: 0.573546\n",
            "\tTraining batch 1014 Loss: 0.858576\n",
            "\tTraining batch 1015 Loss: 0.461931\n",
            "\tTraining batch 1016 Loss: 0.320669\n",
            "\tTraining batch 1017 Loss: 0.533444\n",
            "\tTraining batch 1018 Loss: 0.453521\n",
            "\tTraining batch 1019 Loss: 0.383091\n",
            "\tTraining batch 1020 Loss: 0.194591\n",
            "\tTraining batch 1021 Loss: 0.767950\n",
            "\tTraining batch 1022 Loss: 0.532789\n",
            "\tTraining batch 1023 Loss: 0.293198\n",
            "\tTraining batch 1024 Loss: 0.700265\n",
            "\tTraining batch 1025 Loss: 0.619868\n",
            "\tTraining batch 1026 Loss: 0.755403\n",
            "\tTraining batch 1027 Loss: 0.388754\n",
            "\tTraining batch 1028 Loss: 0.224300\n",
            "\tTraining batch 1029 Loss: 0.565427\n",
            "\tTraining batch 1030 Loss: 0.290285\n",
            "\tTraining batch 1031 Loss: 0.484307\n",
            "\tTraining batch 1032 Loss: 0.214789\n",
            "\tTraining batch 1033 Loss: 0.591523\n",
            "\tTraining batch 1034 Loss: 0.562567\n",
            "\tTraining batch 1035 Loss: 0.363038\n",
            "\tTraining batch 1036 Loss: 0.513118\n",
            "\tTraining batch 1037 Loss: 0.339157\n",
            "\tTraining batch 1038 Loss: 0.283046\n",
            "\tTraining batch 1039 Loss: 0.473073\n",
            "\tTraining batch 1040 Loss: 0.591163\n",
            "\tTraining batch 1041 Loss: 0.608923\n",
            "\tTraining batch 1042 Loss: 0.365878\n",
            "\tTraining batch 1043 Loss: 0.537685\n",
            "\tTraining batch 1044 Loss: 0.733289\n",
            "\tTraining batch 1045 Loss: 0.670838\n",
            "\tTraining batch 1046 Loss: 0.198577\n",
            "\tTraining batch 1047 Loss: 0.231363\n",
            "\tTraining batch 1048 Loss: 0.828125\n",
            "\tTraining batch 1049 Loss: 0.200763\n",
            "\tTraining batch 1050 Loss: 0.950118\n",
            "\tTraining batch 1051 Loss: 0.209466\n",
            "\tTraining batch 1052 Loss: 0.524181\n",
            "\tTraining batch 1053 Loss: 0.646089\n",
            "\tTraining batch 1054 Loss: 0.518209\n",
            "\tTraining batch 1055 Loss: 0.442908\n",
            "\tTraining batch 1056 Loss: 0.217594\n",
            "\tTraining batch 1057 Loss: 0.225547\n",
            "\tTraining batch 1058 Loss: 0.172922\n",
            "\tTraining batch 1059 Loss: 0.723737\n",
            "\tTraining batch 1060 Loss: 0.337795\n",
            "\tTraining batch 1061 Loss: 0.656093\n",
            "\tTraining batch 1062 Loss: 0.287368\n",
            "\tTraining batch 1063 Loss: 0.646663\n",
            "\tTraining batch 1064 Loss: 0.720746\n",
            "\tTraining batch 1065 Loss: 0.291706\n",
            "\tTraining batch 1066 Loss: 0.579938\n",
            "\tTraining batch 1067 Loss: 0.224102\n",
            "\tTraining batch 1068 Loss: 0.550291\n",
            "\tTraining batch 1069 Loss: 0.306313\n",
            "\tTraining batch 1070 Loss: 0.844925\n",
            "\tTraining batch 1071 Loss: 0.681179\n",
            "\tTraining batch 1072 Loss: 0.351453\n",
            "\tTraining batch 1073 Loss: 0.556546\n",
            "\tTraining batch 1074 Loss: 0.415514\n",
            "\tTraining batch 1075 Loss: 0.836907\n",
            "\tTraining batch 1076 Loss: 0.334185\n",
            "\tTraining batch 1077 Loss: 1.137080\n",
            "\tTraining batch 1078 Loss: 0.314347\n",
            "\tTraining batch 1079 Loss: 0.375809\n",
            "\tTraining batch 1080 Loss: 0.799439\n",
            "\tTraining batch 1081 Loss: 0.285301\n",
            "\tTraining batch 1082 Loss: 0.290893\n",
            "\tTraining batch 1083 Loss: 0.681126\n",
            "\tTraining batch 1084 Loss: 0.574502\n",
            "\tTraining batch 1085 Loss: 0.556515\n",
            "\tTraining batch 1086 Loss: 0.623089\n",
            "\tTraining batch 1087 Loss: 0.901761\n",
            "\tTraining batch 1088 Loss: 0.847484\n",
            "\tTraining batch 1089 Loss: 0.576721\n",
            "\tTraining batch 1090 Loss: 0.279216\n",
            "\tTraining batch 1091 Loss: 0.285332\n",
            "\tTraining batch 1092 Loss: 0.378709\n",
            "\tTraining batch 1093 Loss: 0.654649\n",
            "\tTraining batch 1094 Loss: 0.556874\n",
            "\tTraining batch 1095 Loss: 0.332440\n",
            "\tTraining batch 1096 Loss: 0.557593\n",
            "\tTraining batch 1097 Loss: 1.012016\n",
            "\tTraining batch 1098 Loss: 0.277307\n",
            "\tTraining batch 1099 Loss: 0.579198\n",
            "\tTraining batch 1100 Loss: 0.525361\n",
            "\tTraining batch 1101 Loss: 0.437413\n",
            "\tTraining batch 1102 Loss: 0.442801\n",
            "\tTraining batch 1103 Loss: 0.461085\n",
            "\tTraining batch 1104 Loss: 0.300600\n",
            "\tTraining batch 1105 Loss: 0.579034\n",
            "\tTraining batch 1106 Loss: 0.765437\n",
            "\tTraining batch 1107 Loss: 0.526057\n",
            "\tTraining batch 1108 Loss: 0.392450\n",
            "\tTraining batch 1109 Loss: 0.202718\n",
            "\tTraining batch 1110 Loss: 0.562998\n",
            "\tTraining batch 1111 Loss: 0.320694\n",
            "\tTraining batch 1112 Loss: 0.545105\n",
            "\tTraining batch 1113 Loss: 0.618451\n",
            "\tTraining batch 1114 Loss: 0.323165\n",
            "\tTraining batch 1115 Loss: 0.341693\n",
            "\tTraining batch 1116 Loss: 0.291582\n",
            "\tTraining batch 1117 Loss: 0.350220\n",
            "\tTraining batch 1118 Loss: 0.548182\n",
            "\tTraining batch 1119 Loss: 0.825327\n",
            "\tTraining batch 1120 Loss: 0.505393\n",
            "\tTraining batch 1121 Loss: 0.472892\n",
            "\tTraining batch 1122 Loss: 0.437770\n",
            "\tTraining batch 1123 Loss: 0.684876\n",
            "\tTraining batch 1124 Loss: 0.938227\n",
            "\tTraining batch 1125 Loss: 0.472438\n",
            "\tTraining batch 1126 Loss: 0.689966\n",
            "\tTraining batch 1127 Loss: 0.310023\n",
            "\tTraining batch 1128 Loss: 0.387383\n",
            "\tTraining batch 1129 Loss: 0.366433\n",
            "\tTraining batch 1130 Loss: 0.355265\n",
            "\tTraining batch 1131 Loss: 0.870997\n",
            "\tTraining batch 1132 Loss: 0.540289\n",
            "\tTraining batch 1133 Loss: 0.164688\n",
            "\tTraining batch 1134 Loss: 0.547117\n",
            "\tTraining batch 1135 Loss: 0.383132\n",
            "\tTraining batch 1136 Loss: 0.541191\n",
            "\tTraining batch 1137 Loss: 0.623126\n",
            "\tTraining batch 1138 Loss: 0.244629\n",
            "\tTraining batch 1139 Loss: 0.255589\n",
            "\tTraining batch 1140 Loss: 0.746870\n",
            "\tTraining batch 1141 Loss: 0.262567\n",
            "\tTraining batch 1142 Loss: 0.600632\n",
            "\tTraining batch 1143 Loss: 0.539102\n",
            "\tTraining batch 1144 Loss: 0.743630\n",
            "\tTraining batch 1145 Loss: 0.290172\n",
            "\tTraining batch 1146 Loss: 0.570325\n",
            "\tTraining batch 1147 Loss: 0.475128\n",
            "\tTraining batch 1148 Loss: 0.640555\n",
            "\tTraining batch 1149 Loss: 0.609950\n",
            "\tTraining batch 1150 Loss: 0.431721\n",
            "\tTraining batch 1151 Loss: 0.526997\n",
            "\tTraining batch 1152 Loss: 0.560362\n",
            "\tTraining batch 1153 Loss: 0.500750\n",
            "\tTraining batch 1154 Loss: 0.476238\n",
            "\tTraining batch 1155 Loss: 0.363752\n",
            "\tTraining batch 1156 Loss: 0.357601\n",
            "\tTraining batch 1157 Loss: 0.687412\n",
            "\tTraining batch 1158 Loss: 0.298007\n",
            "\tTraining batch 1159 Loss: 0.468639\n",
            "\tTraining batch 1160 Loss: 0.419194\n",
            "\tTraining batch 1161 Loss: 0.608826\n",
            "\tTraining batch 1162 Loss: 0.495023\n",
            "\tTraining batch 1163 Loss: 0.410049\n",
            "\tTraining batch 1164 Loss: 0.666720\n",
            "\tTraining batch 1165 Loss: 0.532855\n",
            "\tTraining batch 1166 Loss: 0.209304\n",
            "\tTraining batch 1167 Loss: 0.297298\n",
            "\tTraining batch 1168 Loss: 0.423526\n",
            "\tTraining batch 1169 Loss: 0.386080\n",
            "\tTraining batch 1170 Loss: 0.862346\n",
            "\tTraining batch 1171 Loss: 0.380807\n",
            "\tTraining batch 1172 Loss: 0.700054\n",
            "\tTraining batch 1173 Loss: 0.466965\n",
            "\tTraining batch 1174 Loss: 0.400106\n",
            "\tTraining batch 1175 Loss: 0.634940\n",
            "\tTraining batch 1176 Loss: 0.690643\n",
            "\tTraining batch 1177 Loss: 1.116195\n",
            "\tTraining batch 1178 Loss: 0.368365\n",
            "\tTraining batch 1179 Loss: 0.347858\n",
            "\tTraining batch 1180 Loss: 0.625995\n",
            "\tTraining batch 1181 Loss: 0.621112\n",
            "\tTraining batch 1182 Loss: 0.550032\n",
            "\tTraining batch 1183 Loss: 0.657092\n",
            "\tTraining batch 1184 Loss: 0.281712\n",
            "\tTraining batch 1185 Loss: 0.296294\n",
            "\tTraining batch 1186 Loss: 0.431096\n",
            "\tTraining batch 1187 Loss: 0.847000\n",
            "\tTraining batch 1188 Loss: 0.232839\n",
            "\tTraining batch 1189 Loss: 0.326779\n",
            "\tTraining batch 1190 Loss: 0.272177\n",
            "\tTraining batch 1191 Loss: 0.524291\n",
            "\tTraining batch 1192 Loss: 0.652667\n",
            "\tTraining batch 1193 Loss: 0.691066\n",
            "\tTraining batch 1194 Loss: 0.326619\n",
            "\tTraining batch 1195 Loss: 0.374255\n",
            "\tTraining batch 1196 Loss: 0.392197\n",
            "\tTraining batch 1197 Loss: 0.711695\n",
            "\tTraining batch 1198 Loss: 0.891622\n",
            "\tTraining batch 1199 Loss: 0.621179\n",
            "\tTraining batch 1200 Loss: 0.555121\n",
            "\tTraining batch 1201 Loss: 0.473735\n",
            "\tTraining batch 1202 Loss: 0.450041\n",
            "\tTraining batch 1203 Loss: 0.494961\n",
            "\tTraining batch 1204 Loss: 1.256657\n",
            "\tTraining batch 1205 Loss: 0.510175\n",
            "\tTraining batch 1206 Loss: 0.487878\n",
            "\tTraining batch 1207 Loss: 0.647041\n",
            "\tTraining batch 1208 Loss: 0.287605\n",
            "\tTraining batch 1209 Loss: 0.675292\n",
            "\tTraining batch 1210 Loss: 0.525602\n",
            "\tTraining batch 1211 Loss: 0.420249\n",
            "\tTraining batch 1212 Loss: 0.572057\n",
            "\tTraining batch 1213 Loss: 0.325318\n",
            "\tTraining batch 1214 Loss: 0.402905\n",
            "\tTraining batch 1215 Loss: 0.545612\n",
            "\tTraining batch 1216 Loss: 0.712775\n",
            "\tTraining batch 1217 Loss: 0.535272\n",
            "\tTraining batch 1218 Loss: 0.477171\n",
            "\tTraining batch 1219 Loss: 0.569060\n",
            "\tTraining batch 1220 Loss: 0.568516\n",
            "\tTraining batch 1221 Loss: 0.394006\n",
            "\tTraining batch 1222 Loss: 0.511447\n",
            "\tTraining batch 1223 Loss: 0.254524\n",
            "\tTraining batch 1224 Loss: 0.585103\n",
            "\tTraining batch 1225 Loss: 0.435237\n",
            "\tTraining batch 1226 Loss: 0.346091\n",
            "\tTraining batch 1227 Loss: 0.298240\n",
            "\tTraining batch 1228 Loss: 0.606192\n",
            "\tTraining batch 1229 Loss: 0.841833\n",
            "\tTraining batch 1230 Loss: 0.442344\n",
            "\tTraining batch 1231 Loss: 0.238875\n",
            "\tTraining batch 1232 Loss: 0.579817\n",
            "\tTraining batch 1233 Loss: 0.313634\n",
            "\tTraining batch 1234 Loss: 0.404620\n",
            "\tTraining batch 1235 Loss: 0.598360\n",
            "\tTraining batch 1236 Loss: 0.337451\n",
            "\tTraining batch 1237 Loss: 0.680404\n",
            "\tTraining batch 1238 Loss: 0.441825\n",
            "\tTraining batch 1239 Loss: 0.726479\n",
            "\tTraining batch 1240 Loss: 0.243615\n",
            "\tTraining batch 1241 Loss: 0.511356\n",
            "\tTraining batch 1242 Loss: 0.329691\n",
            "\tTraining batch 1243 Loss: 0.913294\n",
            "\tTraining batch 1244 Loss: 0.724572\n",
            "\tTraining batch 1245 Loss: 0.529017\n",
            "\tTraining batch 1246 Loss: 0.888467\n",
            "\tTraining batch 1247 Loss: 0.343767\n",
            "\tTraining batch 1248 Loss: 0.187664\n",
            "\tTraining batch 1249 Loss: 0.274022\n",
            "\tTraining batch 1250 Loss: 0.348861\n",
            "\tTraining batch 1251 Loss: 0.777643\n",
            "\tTraining batch 1252 Loss: 0.254080\n",
            "\tTraining batch 1253 Loss: 0.362524\n",
            "\tTraining batch 1254 Loss: 0.339816\n",
            "\tTraining batch 1255 Loss: 0.934824\n",
            "\tTraining batch 1256 Loss: 0.398137\n",
            "\tTraining batch 1257 Loss: 0.093721\n",
            "\tTraining batch 1258 Loss: 0.768656\n",
            "\tTraining batch 1259 Loss: 0.525097\n",
            "\tTraining batch 1260 Loss: 0.635845\n",
            "\tTraining batch 1261 Loss: 0.559128\n",
            "\tTraining batch 1262 Loss: 0.722166\n",
            "\tTraining batch 1263 Loss: 0.632076\n",
            "\tTraining batch 1264 Loss: 0.757427\n",
            "\tTraining batch 1265 Loss: 0.299609\n",
            "\tTraining batch 1266 Loss: 0.427797\n",
            "\tTraining batch 1267 Loss: 0.739977\n",
            "\tTraining batch 1268 Loss: 0.348532\n",
            "\tTraining batch 1269 Loss: 0.650330\n",
            "\tTraining batch 1270 Loss: 0.475243\n",
            "\tTraining batch 1271 Loss: 0.641033\n",
            "\tTraining batch 1272 Loss: 1.005694\n",
            "\tTraining batch 1273 Loss: 0.442818\n",
            "\tTraining batch 1274 Loss: 0.487000\n",
            "\tTraining batch 1275 Loss: 0.490326\n",
            "\tTraining batch 1276 Loss: 0.292929\n",
            "\tTraining batch 1277 Loss: 0.666900\n",
            "\tTraining batch 1278 Loss: 0.344500\n",
            "\tTraining batch 1279 Loss: 0.489702\n",
            "\tTraining batch 1280 Loss: 0.526698\n",
            "\tTraining batch 1281 Loss: 0.319831\n",
            "\tTraining batch 1282 Loss: 0.308893\n",
            "\tTraining batch 1283 Loss: 0.545567\n",
            "\tTraining batch 1284 Loss: 0.521714\n",
            "\tTraining batch 1285 Loss: 0.749737\n",
            "\tTraining batch 1286 Loss: 0.829273\n",
            "\tTraining batch 1287 Loss: 0.316976\n",
            "\tTraining batch 1288 Loss: 0.298670\n",
            "\tTraining batch 1289 Loss: 0.846648\n",
            "\tTraining batch 1290 Loss: 0.333028\n",
            "\tTraining batch 1291 Loss: 0.288777\n",
            "\tTraining batch 1292 Loss: 0.328836\n",
            "\tTraining batch 1293 Loss: 0.330553\n",
            "\tTraining batch 1294 Loss: 0.215691\n",
            "\tTraining batch 1295 Loss: 0.725646\n",
            "\tTraining batch 1296 Loss: 0.343923\n",
            "\tTraining batch 1297 Loss: 0.273181\n",
            "\tTraining batch 1298 Loss: 0.535505\n",
            "\tTraining batch 1299 Loss: 0.481158\n",
            "\tTraining batch 1300 Loss: 0.519857\n",
            "\tTraining batch 1301 Loss: 0.335722\n",
            "\tTraining batch 1302 Loss: 0.459942\n",
            "\tTraining batch 1303 Loss: 0.296410\n",
            "\tTraining batch 1304 Loss: 0.897255\n",
            "\tTraining batch 1305 Loss: 0.209657\n",
            "\tTraining batch 1306 Loss: 0.523145\n",
            "\tTraining batch 1307 Loss: 0.371134\n",
            "\tTraining batch 1308 Loss: 0.290388\n",
            "\tTraining batch 1309 Loss: 0.263227\n",
            "\tTraining batch 1310 Loss: 0.471300\n",
            "\tTraining batch 1311 Loss: 0.239937\n",
            "\tTraining batch 1312 Loss: 0.196281\n",
            "\tTraining batch 1313 Loss: 0.456889\n",
            "\tTraining batch 1314 Loss: 0.921584\n",
            "\tTraining batch 1315 Loss: 0.586749\n",
            "\tTraining batch 1316 Loss: 0.138195\n",
            "\tTraining batch 1317 Loss: 0.685318\n",
            "\tTraining batch 1318 Loss: 0.242972\n",
            "\tTraining batch 1319 Loss: 0.297242\n",
            "\tTraining batch 1320 Loss: 0.383094\n",
            "\tTraining batch 1321 Loss: 0.681795\n",
            "\tTraining batch 1322 Loss: 0.496469\n",
            "\tTraining batch 1323 Loss: 0.538867\n",
            "\tTraining batch 1324 Loss: 0.356885\n",
            "\tTraining batch 1325 Loss: 0.483025\n",
            "\tTraining batch 1326 Loss: 0.511240\n",
            "\tTraining batch 1327 Loss: 0.446343\n",
            "\tTraining batch 1328 Loss: 0.411348\n",
            "\tTraining batch 1329 Loss: 0.471534\n",
            "\tTraining batch 1330 Loss: 0.369938\n",
            "\tTraining batch 1331 Loss: 0.515898\n",
            "\tTraining batch 1332 Loss: 0.599838\n",
            "\tTraining batch 1333 Loss: 0.539506\n",
            "\tTraining batch 1334 Loss: 0.381962\n",
            "\tTraining batch 1335 Loss: 0.829498\n",
            "\tTraining batch 1336 Loss: 0.386357\n",
            "\tTraining batch 1337 Loss: 0.147569\n",
            "\tTraining batch 1338 Loss: 0.515587\n",
            "\tTraining batch 1339 Loss: 0.486940\n",
            "\tTraining batch 1340 Loss: 0.316445\n",
            "\tTraining batch 1341 Loss: 0.456313\n",
            "\tTraining batch 1342 Loss: 0.741997\n",
            "\tTraining batch 1343 Loss: 0.504193\n",
            "\tTraining batch 1344 Loss: 0.535205\n",
            "\tTraining batch 1345 Loss: 0.599588\n",
            "\tTraining batch 1346 Loss: 0.576790\n",
            "\tTraining batch 1347 Loss: 0.842711\n",
            "\tTraining batch 1348 Loss: 0.209501\n",
            "\tTraining batch 1349 Loss: 0.475197\n",
            "\tTraining batch 1350 Loss: 0.372028\n",
            "\tTraining batch 1351 Loss: 0.223499\n",
            "\tTraining batch 1352 Loss: 0.444442\n",
            "\tTraining batch 1353 Loss: 0.747122\n",
            "\tTraining batch 1354 Loss: 0.563380\n",
            "\tTraining batch 1355 Loss: 0.694653\n",
            "\tTraining batch 1356 Loss: 0.747982\n",
            "\tTraining batch 1357 Loss: 0.157681\n",
            "\tTraining batch 1358 Loss: 0.715912\n",
            "\tTraining batch 1359 Loss: 0.224844\n",
            "\tTraining batch 1360 Loss: 0.408792\n",
            "\tTraining batch 1361 Loss: 0.749400\n",
            "\tTraining batch 1362 Loss: 0.841983\n",
            "\tTraining batch 1363 Loss: 0.362804\n",
            "\tTraining batch 1364 Loss: 0.378203\n",
            "\tTraining batch 1365 Loss: 1.007663\n",
            "\tTraining batch 1366 Loss: 0.238773\n",
            "\tTraining batch 1367 Loss: 0.244653\n",
            "\tTraining batch 1368 Loss: 0.373601\n",
            "\tTraining batch 1369 Loss: 0.310698\n",
            "\tTraining batch 1370 Loss: 0.656305\n",
            "\tTraining batch 1371 Loss: 0.939515\n",
            "\tTraining batch 1372 Loss: 0.412412\n",
            "\tTraining batch 1373 Loss: 0.434756\n",
            "\tTraining batch 1374 Loss: 0.335621\n",
            "\tTraining batch 1375 Loss: 0.314688\n",
            "\tTraining batch 1376 Loss: 0.814922\n",
            "\tTraining batch 1377 Loss: 0.448092\n",
            "\tTraining batch 1378 Loss: 0.710168\n",
            "\tTraining batch 1379 Loss: 0.216539\n",
            "\tTraining batch 1380 Loss: 0.391654\n",
            "\tTraining batch 1381 Loss: 0.710211\n",
            "\tTraining batch 1382 Loss: 0.275373\n",
            "\tTraining batch 1383 Loss: 0.303437\n",
            "\tTraining batch 1384 Loss: 0.241610\n",
            "\tTraining batch 1385 Loss: 0.425590\n",
            "\tTraining batch 1386 Loss: 0.504255\n",
            "\tTraining batch 1387 Loss: 0.399609\n",
            "\tTraining batch 1388 Loss: 0.396274\n",
            "\tTraining batch 1389 Loss: 0.401824\n",
            "\tTraining batch 1390 Loss: 0.371559\n",
            "\tTraining batch 1391 Loss: 0.835189\n",
            "\tTraining batch 1392 Loss: 0.232803\n",
            "\tTraining batch 1393 Loss: 0.639785\n",
            "\tTraining batch 1394 Loss: 0.445251\n",
            "\tTraining batch 1395 Loss: 0.689921\n",
            "\tTraining batch 1396 Loss: 0.347394\n",
            "\tTraining batch 1397 Loss: 0.451714\n",
            "\tTraining batch 1398 Loss: 0.255608\n",
            "\tTraining batch 1399 Loss: 0.699228\n",
            "\tTraining batch 1400 Loss: 0.281737\n",
            "\tTraining batch 1401 Loss: 0.475289\n",
            "\tTraining batch 1402 Loss: 0.409707\n",
            "\tTraining batch 1403 Loss: 0.506955\n",
            "\tTraining batch 1404 Loss: 0.340621\n",
            "\tTraining batch 1405 Loss: 0.322301\n",
            "\tTraining batch 1406 Loss: 0.477113\n",
            "\tTraining batch 1407 Loss: 0.376969\n",
            "\tTraining batch 1408 Loss: 0.331293\n",
            "\tTraining batch 1409 Loss: 0.433767\n",
            "\tTraining batch 1410 Loss: 0.563230\n",
            "\tTraining batch 1411 Loss: 0.238228\n",
            "\tTraining batch 1412 Loss: 0.715869\n",
            "\tTraining batch 1413 Loss: 0.126200\n",
            "\tTraining batch 1414 Loss: 0.506626\n",
            "\tTraining batch 1415 Loss: 0.607574\n",
            "\tTraining batch 1416 Loss: 0.717183\n",
            "\tTraining batch 1417 Loss: 0.547460\n",
            "\tTraining batch 1418 Loss: 0.740885\n",
            "\tTraining batch 1419 Loss: 0.417048\n",
            "\tTraining batch 1420 Loss: 0.313813\n",
            "\tTraining batch 1421 Loss: 0.596272\n",
            "\tTraining batch 1422 Loss: 0.541164\n",
            "\tTraining batch 1423 Loss: 0.403282\n",
            "\tTraining batch 1424 Loss: 0.946271\n",
            "\tTraining batch 1425 Loss: 0.392317\n",
            "\tTraining batch 1426 Loss: 0.597070\n",
            "\tTraining batch 1427 Loss: 0.261881\n",
            "\tTraining batch 1428 Loss: 0.562338\n",
            "\tTraining batch 1429 Loss: 0.418824\n",
            "\tTraining batch 1430 Loss: 0.738097\n",
            "\tTraining batch 1431 Loss: 0.821158\n",
            "\tTraining batch 1432 Loss: 0.307921\n",
            "\tTraining batch 1433 Loss: 0.665133\n",
            "\tTraining batch 1434 Loss: 0.714055\n",
            "\tTraining batch 1435 Loss: 0.668460\n",
            "\tTraining batch 1436 Loss: 0.344807\n",
            "\tTraining batch 1437 Loss: 0.608727\n",
            "\tTraining batch 1438 Loss: 0.558994\n",
            "\tTraining batch 1439 Loss: 0.210222\n",
            "\tTraining batch 1440 Loss: 0.482785\n",
            "\tTraining batch 1441 Loss: 0.312192\n",
            "\tTraining batch 1442 Loss: 0.547324\n",
            "\tTraining batch 1443 Loss: 0.533604\n",
            "\tTraining batch 1444 Loss: 0.533980\n",
            "\tTraining batch 1445 Loss: 0.174538\n",
            "\tTraining batch 1446 Loss: 0.552510\n",
            "\tTraining batch 1447 Loss: 0.641687\n",
            "\tTraining batch 1448 Loss: 0.366636\n",
            "\tTraining batch 1449 Loss: 0.496755\n",
            "\tTraining batch 1450 Loss: 0.491919\n",
            "\tTraining batch 1451 Loss: 0.827700\n",
            "\tTraining batch 1452 Loss: 0.823333\n",
            "\tTraining batch 1453 Loss: 0.164005\n",
            "\tTraining batch 1454 Loss: 0.842210\n",
            "\tTraining batch 1455 Loss: 0.388006\n",
            "\tTraining batch 1456 Loss: 0.503997\n",
            "\tTraining batch 1457 Loss: 0.662820\n",
            "\tTraining batch 1458 Loss: 0.585616\n",
            "\tTraining batch 1459 Loss: 0.327308\n",
            "\tTraining batch 1460 Loss: 0.583219\n",
            "\tTraining batch 1461 Loss: 0.283744\n",
            "\tTraining batch 1462 Loss: 0.447311\n",
            "\tTraining batch 1463 Loss: 0.498825\n",
            "\tTraining batch 1464 Loss: 0.165453\n",
            "\tTraining batch 1465 Loss: 0.328438\n",
            "\tTraining batch 1466 Loss: 0.428126\n",
            "\tTraining batch 1467 Loss: 0.715976\n",
            "\tTraining batch 1468 Loss: 0.381072\n",
            "\tTraining batch 1469 Loss: 0.518825\n",
            "\tTraining batch 1470 Loss: 0.458209\n",
            "\tTraining batch 1471 Loss: 0.278589\n",
            "\tTraining batch 1472 Loss: 0.367281\n",
            "\tTraining batch 1473 Loss: 0.596329\n",
            "\tTraining batch 1474 Loss: 0.367821\n",
            "\tTraining batch 1475 Loss: 0.358536\n",
            "\tTraining batch 1476 Loss: 0.367768\n",
            "\tTraining batch 1477 Loss: 0.243035\n",
            "\tTraining batch 1478 Loss: 0.436729\n",
            "\tTraining batch 1479 Loss: 0.385162\n",
            "\tTraining batch 1480 Loss: 0.582395\n",
            "\tTraining batch 1481 Loss: 0.379480\n",
            "\tTraining batch 1482 Loss: 0.406716\n",
            "\tTraining batch 1483 Loss: 1.103854\n",
            "\tTraining batch 1484 Loss: 0.532166\n",
            "\tTraining batch 1485 Loss: 0.646845\n",
            "\tTraining batch 1486 Loss: 0.609444\n",
            "\tTraining batch 1487 Loss: 0.325609\n",
            "\tTraining batch 1488 Loss: 0.324793\n",
            "\tTraining batch 1489 Loss: 0.344880\n",
            "\tTraining batch 1490 Loss: 0.458518\n",
            "\tTraining batch 1491 Loss: 0.569211\n",
            "\tTraining batch 1492 Loss: 0.303878\n",
            "\tTraining batch 1493 Loss: 0.284844\n",
            "\tTraining batch 1494 Loss: 0.821676\n",
            "\tTraining batch 1495 Loss: 0.571523\n",
            "\tTraining batch 1496 Loss: 0.525600\n",
            "\tTraining batch 1497 Loss: 0.477955\n",
            "\tTraining batch 1498 Loss: 0.308207\n",
            "\tTraining batch 1499 Loss: 0.222389\n",
            "\tTraining batch 1500 Loss: 0.553297\n",
            "\tTraining batch 1501 Loss: 0.578546\n",
            "\tTraining batch 1502 Loss: 0.096931\n",
            "\tTraining batch 1503 Loss: 1.064809\n",
            "\tTraining batch 1504 Loss: 0.419294\n",
            "\tTraining batch 1505 Loss: 0.389828\n",
            "\tTraining batch 1506 Loss: 0.543859\n",
            "\tTraining batch 1507 Loss: 0.636739\n",
            "\tTraining batch 1508 Loss: 0.624586\n",
            "\tTraining batch 1509 Loss: 0.394102\n",
            "\tTraining batch 1510 Loss: 0.376646\n",
            "\tTraining batch 1511 Loss: 0.741800\n",
            "\tTraining batch 1512 Loss: 0.298075\n",
            "\tTraining batch 1513 Loss: 0.183981\n",
            "\tTraining batch 1514 Loss: 0.598660\n",
            "\tTraining batch 1515 Loss: 0.407765\n",
            "\tTraining batch 1516 Loss: 0.674926\n",
            "\tTraining batch 1517 Loss: 0.597059\n",
            "\tTraining batch 1518 Loss: 0.660546\n",
            "\tTraining batch 1519 Loss: 0.388737\n",
            "\tTraining batch 1520 Loss: 0.246158\n",
            "\tTraining batch 1521 Loss: 0.629402\n",
            "\tTraining batch 1522 Loss: 0.384853\n",
            "\tTraining batch 1523 Loss: 0.758580\n",
            "\tTraining batch 1524 Loss: 0.598029\n",
            "\tTraining batch 1525 Loss: 0.158990\n",
            "\tTraining batch 1526 Loss: 0.291471\n",
            "\tTraining batch 1527 Loss: 0.966530\n",
            "\tTraining batch 1528 Loss: 0.423115\n",
            "\tTraining batch 1529 Loss: 0.497277\n",
            "\tTraining batch 1530 Loss: 0.238506\n",
            "\tTraining batch 1531 Loss: 0.337254\n",
            "\tTraining batch 1532 Loss: 0.623077\n",
            "\tTraining batch 1533 Loss: 0.231492\n",
            "\tTraining batch 1534 Loss: 0.479741\n",
            "\tTraining batch 1535 Loss: 1.021151\n",
            "\tTraining batch 1536 Loss: 0.929292\n",
            "\tTraining batch 1537 Loss: 0.566022\n",
            "\tTraining batch 1538 Loss: 0.457582\n",
            "\tTraining batch 1539 Loss: 0.623584\n",
            "\tTraining batch 1540 Loss: 0.265448\n",
            "\tTraining batch 1541 Loss: 0.887100\n",
            "\tTraining batch 1542 Loss: 0.878697\n",
            "\tTraining batch 1543 Loss: 0.321635\n",
            "\tTraining batch 1544 Loss: 0.759494\n",
            "\tTraining batch 1545 Loss: 0.712781\n",
            "\tTraining batch 1546 Loss: 0.150211\n",
            "\tTraining batch 1547 Loss: 0.462292\n",
            "\tTraining batch 1548 Loss: 0.262465\n",
            "\tTraining batch 1549 Loss: 0.668037\n",
            "\tTraining batch 1550 Loss: 0.477361\n",
            "\tTraining batch 1551 Loss: 0.645038\n",
            "\tTraining batch 1552 Loss: 0.533444\n",
            "\tTraining batch 1553 Loss: 0.176086\n",
            "\tTraining batch 1554 Loss: 0.453359\n",
            "\tTraining batch 1555 Loss: 0.616581\n",
            "\tTraining batch 1556 Loss: 0.466265\n",
            "\tTraining batch 1557 Loss: 0.393372\n",
            "\tTraining batch 1558 Loss: 0.249809\n",
            "\tTraining batch 1559 Loss: 0.547445\n",
            "\tTraining batch 1560 Loss: 0.729250\n",
            "\tTraining batch 1561 Loss: 0.538675\n",
            "\tTraining batch 1562 Loss: 0.465892\n",
            "\tTraining batch 1563 Loss: 0.652275\n",
            "\tTraining batch 1564 Loss: 0.185729\n",
            "\tTraining batch 1565 Loss: 0.436980\n",
            "\tTraining batch 1566 Loss: 0.449708\n",
            "\tTraining batch 1567 Loss: 0.137622\n",
            "\tTraining batch 1568 Loss: 0.596924\n",
            "\tTraining batch 1569 Loss: 0.482976\n",
            "\tTraining batch 1570 Loss: 0.420025\n",
            "\tTraining batch 1571 Loss: 0.414118\n",
            "\tTraining batch 1572 Loss: 0.588214\n",
            "\tTraining batch 1573 Loss: 0.264427\n",
            "\tTraining batch 1574 Loss: 0.578164\n",
            "\tTraining batch 1575 Loss: 0.451440\n",
            "\tTraining batch 1576 Loss: 0.244342\n",
            "\tTraining batch 1577 Loss: 0.831899\n",
            "\tTraining batch 1578 Loss: 0.617518\n",
            "\tTraining batch 1579 Loss: 0.943390\n",
            "\tTraining batch 1580 Loss: 0.257108\n",
            "\tTraining batch 1581 Loss: 0.437314\n",
            "\tTraining batch 1582 Loss: 0.758884\n",
            "\tTraining batch 1583 Loss: 1.037788\n",
            "\tTraining batch 1584 Loss: 0.586407\n",
            "\tTraining batch 1585 Loss: 0.586614\n",
            "\tTraining batch 1586 Loss: 0.552788\n",
            "\tTraining batch 1587 Loss: 0.255123\n",
            "\tTraining batch 1588 Loss: 0.591120\n",
            "\tTraining batch 1589 Loss: 0.448062\n",
            "\tTraining batch 1590 Loss: 0.336993\n",
            "\tTraining batch 1591 Loss: 0.613702\n",
            "\tTraining batch 1592 Loss: 0.668371\n",
            "\tTraining batch 1593 Loss: 0.499277\n",
            "\tTraining batch 1594 Loss: 0.587673\n",
            "\tTraining batch 1595 Loss: 0.575146\n",
            "\tTraining batch 1596 Loss: 0.623858\n",
            "\tTraining batch 1597 Loss: 0.553143\n",
            "\tTraining batch 1598 Loss: 0.595353\n",
            "\tTraining batch 1599 Loss: 0.412842\n",
            "\tTraining batch 1600 Loss: 0.237781\n",
            "\tTraining batch 1601 Loss: 0.713260\n",
            "\tTraining batch 1602 Loss: 0.620901\n",
            "\tTraining batch 1603 Loss: 1.033293\n",
            "\tTraining batch 1604 Loss: 0.434165\n",
            "\tTraining batch 1605 Loss: 0.478414\n",
            "\tTraining batch 1606 Loss: 0.261511\n",
            "\tTraining batch 1607 Loss: 0.374869\n",
            "\tTraining batch 1608 Loss: 0.449716\n",
            "\tTraining batch 1609 Loss: 0.604895\n",
            "\tTraining batch 1610 Loss: 0.323338\n",
            "\tTraining batch 1611 Loss: 0.698347\n",
            "\tTraining batch 1612 Loss: 0.559378\n",
            "\tTraining batch 1613 Loss: 0.639682\n",
            "\tTraining batch 1614 Loss: 0.490842\n",
            "\tTraining batch 1615 Loss: 0.608983\n",
            "\tTraining batch 1616 Loss: 0.835987\n",
            "\tTraining batch 1617 Loss: 0.588951\n",
            "\tTraining batch 1618 Loss: 0.566482\n",
            "\tTraining batch 1619 Loss: 0.402550\n",
            "\tTraining batch 1620 Loss: 0.258553\n",
            "\tTraining batch 1621 Loss: 0.630054\n",
            "\tTraining batch 1622 Loss: 0.538196\n",
            "\tTraining batch 1623 Loss: 0.517152\n",
            "\tTraining batch 1624 Loss: 0.531700\n",
            "\tTraining batch 1625 Loss: 0.295315\n",
            "\tTraining batch 1626 Loss: 0.768978\n",
            "\tTraining batch 1627 Loss: 0.737309\n",
            "\tTraining batch 1628 Loss: 0.711017\n",
            "\tTraining batch 1629 Loss: 0.593887\n",
            "\tTraining batch 1630 Loss: 0.604556\n",
            "\tTraining batch 1631 Loss: 0.661746\n",
            "\tTraining batch 1632 Loss: 0.531592\n",
            "\tTraining batch 1633 Loss: 0.438172\n",
            "\tTraining batch 1634 Loss: 0.475591\n",
            "\tTraining batch 1635 Loss: 0.757150\n",
            "\tTraining batch 1636 Loss: 1.004348\n",
            "\tTraining batch 1637 Loss: 0.287187\n",
            "\tTraining batch 1638 Loss: 0.229292\n",
            "\tTraining batch 1639 Loss: 0.512100\n",
            "\tTraining batch 1640 Loss: 0.841976\n",
            "\tTraining batch 1641 Loss: 0.619744\n",
            "\tTraining batch 1642 Loss: 0.658859\n",
            "\tTraining batch 1643 Loss: 1.061220\n",
            "\tTraining batch 1644 Loss: 0.551109\n",
            "\tTraining batch 1645 Loss: 0.561960\n",
            "\tTraining batch 1646 Loss: 0.500787\n",
            "\tTraining batch 1647 Loss: 0.651685\n",
            "\tTraining batch 1648 Loss: 0.626982\n",
            "\tTraining batch 1649 Loss: 0.667027\n",
            "\tTraining batch 1650 Loss: 0.738383\n",
            "\tTraining batch 1651 Loss: 0.559256\n",
            "\tTraining batch 1652 Loss: 0.932847\n",
            "\tTraining batch 1653 Loss: 0.905995\n",
            "\tTraining batch 1654 Loss: 0.382080\n",
            "\tTraining batch 1655 Loss: 0.517015\n",
            "\tTraining batch 1656 Loss: 0.270499\n",
            "\tTraining batch 1657 Loss: 0.404510\n",
            "\tTraining batch 1658 Loss: 0.364737\n",
            "\tTraining batch 1659 Loss: 0.759953\n",
            "\tTraining batch 1660 Loss: 0.344136\n",
            "\tTraining batch 1661 Loss: 0.663101\n",
            "\tTraining batch 1662 Loss: 0.954637\n",
            "\tTraining batch 1663 Loss: 0.676753\n",
            "\tTraining batch 1664 Loss: 0.416543\n",
            "\tTraining batch 1665 Loss: 0.747433\n",
            "\tTraining batch 1666 Loss: 0.753646\n",
            "\tTraining batch 1667 Loss: 0.539520\n",
            "\tTraining batch 1668 Loss: 0.282359\n",
            "\tTraining batch 1669 Loss: 0.387719\n",
            "\tTraining batch 1670 Loss: 0.679128\n",
            "\tTraining batch 1671 Loss: 0.428242\n",
            "\tTraining batch 1672 Loss: 1.014199\n",
            "\tTraining batch 1673 Loss: 0.654482\n",
            "\tTraining batch 1674 Loss: 0.459357\n",
            "\tTraining batch 1675 Loss: 0.600381\n",
            "\tTraining batch 1676 Loss: 0.667188\n",
            "\tTraining batch 1677 Loss: 0.372009\n",
            "\tTraining batch 1678 Loss: 1.005933\n",
            "\tTraining batch 1679 Loss: 0.676852\n",
            "\tTraining batch 1680 Loss: 0.643529\n",
            "\tTraining batch 1681 Loss: 0.526792\n",
            "\tTraining batch 1682 Loss: 0.505541\n",
            "\tTraining batch 1683 Loss: 0.476685\n",
            "\tTraining batch 1684 Loss: 0.786493\n",
            "\tTraining batch 1685 Loss: 0.470992\n",
            "\tTraining batch 1686 Loss: 0.382512\n",
            "\tTraining batch 1687 Loss: 0.563960\n",
            "\tTraining batch 1688 Loss: 0.747777\n",
            "\tTraining batch 1689 Loss: 0.397907\n",
            "\tTraining batch 1690 Loss: 0.295966\n",
            "\tTraining batch 1691 Loss: 0.314533\n",
            "\tTraining batch 1692 Loss: 0.303699\n",
            "\tTraining batch 1693 Loss: 0.477849\n",
            "\tTraining batch 1694 Loss: 0.528846\n",
            "\tTraining batch 1695 Loss: 0.745726\n",
            "\tTraining batch 1696 Loss: 0.580496\n",
            "\tTraining batch 1697 Loss: 0.330366\n",
            "\tTraining batch 1698 Loss: 0.464116\n",
            "\tTraining batch 1699 Loss: 0.509167\n",
            "\tTraining batch 1700 Loss: 0.470874\n",
            "\tTraining batch 1701 Loss: 0.547552\n",
            "\tTraining batch 1702 Loss: 0.312633\n",
            "\tTraining batch 1703 Loss: 0.997978\n",
            "\tTraining batch 1704 Loss: 0.222478\n",
            "\tTraining batch 1705 Loss: 0.655300\n",
            "\tTraining batch 1706 Loss: 0.284367\n",
            "\tTraining batch 1707 Loss: 0.274506\n",
            "\tTraining batch 1708 Loss: 0.172230\n",
            "\tTraining batch 1709 Loss: 0.123030\n",
            "\tTraining batch 1710 Loss: 0.707572\n",
            "\tTraining batch 1711 Loss: 0.535607\n",
            "\tTraining batch 1712 Loss: 0.846102\n",
            "\tTraining batch 1713 Loss: 0.542419\n",
            "\tTraining batch 1714 Loss: 0.397692\n",
            "\tTraining batch 1715 Loss: 0.274572\n",
            "\tTraining batch 1716 Loss: 0.455807\n",
            "\tTraining batch 1717 Loss: 0.931584\n",
            "\tTraining batch 1718 Loss: 0.543186\n",
            "\tTraining batch 1719 Loss: 0.583537\n",
            "\tTraining batch 1720 Loss: 0.777090\n",
            "\tTraining batch 1721 Loss: 0.878367\n",
            "\tTraining batch 1722 Loss: 0.605421\n",
            "\tTraining batch 1723 Loss: 0.358250\n",
            "\tTraining batch 1724 Loss: 0.441307\n",
            "\tTraining batch 1725 Loss: 0.325561\n",
            "\tTraining batch 1726 Loss: 0.239385\n",
            "\tTraining batch 1727 Loss: 0.434816\n",
            "\tTraining batch 1728 Loss: 0.510501\n",
            "\tTraining batch 1729 Loss: 0.580324\n",
            "\tTraining batch 1730 Loss: 0.330756\n",
            "\tTraining batch 1731 Loss: 0.223428\n",
            "\tTraining batch 1732 Loss: 0.450182\n",
            "\tTraining batch 1733 Loss: 1.128065\n",
            "\tTraining batch 1734 Loss: 0.545689\n",
            "\tTraining batch 1735 Loss: 0.260615\n",
            "\tTraining batch 1736 Loss: 0.339801\n",
            "\tTraining batch 1737 Loss: 0.827033\n",
            "\tTraining batch 1738 Loss: 0.433490\n",
            "\tTraining batch 1739 Loss: 0.868160\n",
            "\tTraining batch 1740 Loss: 0.268658\n",
            "\tTraining batch 1741 Loss: 0.501642\n",
            "\tTraining batch 1742 Loss: 0.346121\n",
            "\tTraining batch 1743 Loss: 0.380362\n",
            "\tTraining batch 1744 Loss: 0.370212\n",
            "\tTraining batch 1745 Loss: 0.535756\n",
            "\tTraining batch 1746 Loss: 0.585416\n",
            "\tTraining batch 1747 Loss: 0.451180\n",
            "\tTraining batch 1748 Loss: 0.547641\n",
            "\tTraining batch 1749 Loss: 1.164028\n",
            "\tTraining batch 1750 Loss: 0.352533\n",
            "\tTraining batch 1751 Loss: 0.274131\n",
            "\tTraining batch 1752 Loss: 0.792438\n",
            "\tTraining batch 1753 Loss: 0.488596\n",
            "\tTraining batch 1754 Loss: 0.245190\n",
            "\tTraining batch 1755 Loss: 0.522418\n",
            "\tTraining batch 1756 Loss: 0.638160\n",
            "\tTraining batch 1757 Loss: 0.444734\n",
            "\tTraining batch 1758 Loss: 0.832471\n",
            "\tTraining batch 1759 Loss: 0.180142\n",
            "\tTraining batch 1760 Loss: 0.422494\n",
            "\tTraining batch 1761 Loss: 0.601798\n",
            "\tTraining batch 1762 Loss: 0.883084\n",
            "\tTraining batch 1763 Loss: 0.170972\n",
            "\tTraining batch 1764 Loss: 0.372643\n",
            "\tTraining batch 1765 Loss: 0.412669\n",
            "\tTraining batch 1766 Loss: 0.365546\n",
            "\tTraining batch 1767 Loss: 0.358921\n",
            "\tTraining batch 1768 Loss: 0.323684\n",
            "\tTraining batch 1769 Loss: 0.266335\n",
            "\tTraining batch 1770 Loss: 0.515414\n",
            "\tTraining batch 1771 Loss: 0.442198\n",
            "\tTraining batch 1772 Loss: 0.792722\n",
            "\tTraining batch 1773 Loss: 0.491898\n",
            "\tTraining batch 1774 Loss: 0.415978\n",
            "\tTraining batch 1775 Loss: 0.485013\n",
            "\tTraining batch 1776 Loss: 0.209270\n",
            "\tTraining batch 1777 Loss: 0.324137\n",
            "\tTraining batch 1778 Loss: 0.504157\n",
            "\tTraining batch 1779 Loss: 0.424480\n",
            "\tTraining batch 1780 Loss: 0.556484\n",
            "\tTraining batch 1781 Loss: 0.609963\n",
            "\tTraining batch 1782 Loss: 0.246392\n",
            "\tTraining batch 1783 Loss: 0.316358\n",
            "\tTraining batch 1784 Loss: 0.391246\n",
            "\tTraining batch 1785 Loss: 0.336507\n",
            "\tTraining batch 1786 Loss: 0.382124\n",
            "\tTraining batch 1787 Loss: 0.323363\n",
            "\tTraining batch 1788 Loss: 0.565741\n",
            "\tTraining batch 1789 Loss: 0.237222\n",
            "\tTraining batch 1790 Loss: 0.843184\n",
            "\tTraining batch 1791 Loss: 0.544743\n",
            "\tTraining batch 1792 Loss: 0.677802\n",
            "\tTraining batch 1793 Loss: 0.160422\n",
            "\tTraining batch 1794 Loss: 0.559688\n",
            "\tTraining batch 1795 Loss: 0.357000\n",
            "\tTraining batch 1796 Loss: 0.441758\n",
            "\tTraining batch 1797 Loss: 0.849369\n",
            "\tTraining batch 1798 Loss: 0.378992\n",
            "\tTraining batch 1799 Loss: 0.311544\n",
            "\tTraining batch 1800 Loss: 0.586382\n",
            "\tTraining batch 1801 Loss: 0.305277\n",
            "\tTraining batch 1802 Loss: 0.647492\n",
            "\tTraining batch 1803 Loss: 0.388605\n",
            "\tTraining batch 1804 Loss: 0.442587\n",
            "\tTraining batch 1805 Loss: 0.373582\n",
            "\tTraining batch 1806 Loss: 0.398093\n",
            "\tTraining batch 1807 Loss: 1.225806\n",
            "\tTraining batch 1808 Loss: 0.352067\n",
            "\tTraining batch 1809 Loss: 0.536411\n",
            "\tTraining batch 1810 Loss: 0.478902\n",
            "\tTraining batch 1811 Loss: 0.595341\n",
            "\tTraining batch 1812 Loss: 0.403757\n",
            "\tTraining batch 1813 Loss: 0.793860\n",
            "\tTraining batch 1814 Loss: 0.729996\n",
            "\tTraining batch 1815 Loss: 1.047335\n",
            "\tTraining batch 1816 Loss: 0.569302\n",
            "\tTraining batch 1817 Loss: 0.129789\n",
            "\tTraining batch 1818 Loss: 0.278423\n",
            "\tTraining batch 1819 Loss: 0.663198\n",
            "\tTraining batch 1820 Loss: 0.286882\n",
            "\tTraining batch 1821 Loss: 0.578226\n",
            "\tTraining batch 1822 Loss: 0.487089\n",
            "\tTraining batch 1823 Loss: 0.435621\n",
            "\tTraining batch 1824 Loss: 0.488068\n",
            "\tTraining batch 1825 Loss: 0.520276\n",
            "\tTraining batch 1826 Loss: 0.655582\n",
            "\tTraining batch 1827 Loss: 0.399984\n",
            "\tTraining batch 1828 Loss: 0.502134\n",
            "\tTraining batch 1829 Loss: 0.462762\n",
            "\tTraining batch 1830 Loss: 0.745936\n",
            "\tTraining batch 1831 Loss: 0.630252\n",
            "\tTraining batch 1832 Loss: 0.687430\n",
            "\tTraining batch 1833 Loss: 0.607645\n",
            "\tTraining batch 1834 Loss: 0.685214\n",
            "\tTraining batch 1835 Loss: 0.658773\n",
            "\tTraining batch 1836 Loss: 0.775405\n",
            "\tTraining batch 1837 Loss: 0.256940\n",
            "\tTraining batch 1838 Loss: 0.504326\n",
            "\tTraining batch 1839 Loss: 0.427000\n",
            "\tTraining batch 1840 Loss: 0.302517\n",
            "\tTraining batch 1841 Loss: 0.441060\n",
            "\tTraining batch 1842 Loss: 0.405087\n",
            "\tTraining batch 1843 Loss: 0.544285\n",
            "\tTraining batch 1844 Loss: 1.090526\n",
            "\tTraining batch 1845 Loss: 0.966354\n",
            "\tTraining batch 1846 Loss: 0.263117\n",
            "\tTraining batch 1847 Loss: 0.651308\n",
            "\tTraining batch 1848 Loss: 0.334445\n",
            "\tTraining batch 1849 Loss: 1.284653\n",
            "\tTraining batch 1850 Loss: 0.653806\n",
            "\tTraining batch 1851 Loss: 0.660289\n",
            "\tTraining batch 1852 Loss: 0.183278\n",
            "\tTraining batch 1853 Loss: 0.368396\n",
            "\tTraining batch 1854 Loss: 0.833455\n",
            "\tTraining batch 1855 Loss: 0.785040\n",
            "\tTraining batch 1856 Loss: 0.788714\n",
            "\tTraining batch 1857 Loss: 0.543883\n",
            "\tTraining batch 1858 Loss: 0.496933\n",
            "\tTraining batch 1859 Loss: 0.670708\n",
            "\tTraining batch 1860 Loss: 0.741876\n",
            "\tTraining batch 1861 Loss: 0.372996\n",
            "\tTraining batch 1862 Loss: 0.485380\n",
            "\tTraining batch 1863 Loss: 0.589382\n",
            "\tTraining batch 1864 Loss: 0.437622\n",
            "\tTraining batch 1865 Loss: 0.419873\n",
            "\tTraining batch 1866 Loss: 0.308590\n",
            "\tTraining batch 1867 Loss: 0.202445\n",
            "\tTraining batch 1868 Loss: 0.990427\n",
            "\tTraining batch 1869 Loss: 0.460514\n",
            "\tTraining batch 1870 Loss: 0.330885\n",
            "\tTraining batch 1871 Loss: 0.770757\n",
            "\tTraining batch 1872 Loss: 0.339726\n",
            "\tTraining batch 1873 Loss: 0.626197\n",
            "\tTraining batch 1874 Loss: 0.518486\n",
            "\tTraining batch 1875 Loss: 0.069202\n",
            "\tTraining batch 1876 Loss: 0.869054\n",
            "\tTraining batch 1877 Loss: 0.290887\n",
            "\tTraining batch 1878 Loss: 0.629169\n",
            "\tTraining batch 1879 Loss: 0.554270\n",
            "\tTraining batch 1880 Loss: 0.553512\n",
            "\tTraining batch 1881 Loss: 0.424836\n",
            "\tTraining batch 1882 Loss: 0.779868\n",
            "\tTraining batch 1883 Loss: 0.630422\n",
            "\tTraining batch 1884 Loss: 0.588393\n",
            "\tTraining batch 1885 Loss: 0.696345\n",
            "\tTraining batch 1886 Loss: 0.298369\n",
            "\tTraining batch 1887 Loss: 0.324939\n",
            "\tTraining batch 1888 Loss: 0.568404\n",
            "\tTraining batch 1889 Loss: 1.073948\n",
            "\tTraining batch 1890 Loss: 0.393593\n",
            "\tTraining batch 1891 Loss: 0.562686\n",
            "\tTraining batch 1892 Loss: 0.546939\n",
            "\tTraining batch 1893 Loss: 0.614457\n",
            "\tTraining batch 1894 Loss: 0.341560\n",
            "\tTraining batch 1895 Loss: 0.426106\n",
            "\tTraining batch 1896 Loss: 0.867479\n",
            "\tTraining batch 1897 Loss: 0.477773\n",
            "\tTraining batch 1898 Loss: 0.657565\n",
            "\tTraining batch 1899 Loss: 0.597621\n",
            "\tTraining batch 1900 Loss: 0.808778\n",
            "\tTraining batch 1901 Loss: 0.833649\n",
            "\tTraining batch 1902 Loss: 0.553802\n",
            "\tTraining batch 1903 Loss: 0.486972\n",
            "\tTraining batch 1904 Loss: 0.392863\n",
            "\tTraining batch 1905 Loss: 0.489396\n",
            "\tTraining batch 1906 Loss: 0.617639\n",
            "\tTraining batch 1907 Loss: 0.755277\n",
            "\tTraining batch 1908 Loss: 0.375839\n",
            "\tTraining batch 1909 Loss: 0.498345\n",
            "\tTraining batch 1910 Loss: 0.618961\n",
            "\tTraining batch 1911 Loss: 0.632457\n",
            "\tTraining batch 1912 Loss: 0.393486\n",
            "\tTraining batch 1913 Loss: 0.650568\n",
            "\tTraining batch 1914 Loss: 0.302777\n",
            "\tTraining batch 1915 Loss: 0.998012\n",
            "\tTraining batch 1916 Loss: 0.478669\n",
            "\tTraining batch 1917 Loss: 0.284550\n",
            "\tTraining batch 1918 Loss: 0.793271\n",
            "\tTraining batch 1919 Loss: 0.773390\n",
            "\tTraining batch 1920 Loss: 0.414198\n",
            "\tTraining batch 1921 Loss: 0.246718\n",
            "\tTraining batch 1922 Loss: 0.628382\n",
            "\tTraining batch 1923 Loss: 0.605719\n",
            "\tTraining batch 1924 Loss: 0.216084\n",
            "\tTraining batch 1925 Loss: 0.342270\n",
            "\tTraining batch 1926 Loss: 0.460060\n",
            "\tTraining batch 1927 Loss: 0.749488\n",
            "\tTraining batch 1928 Loss: 0.306645\n",
            "\tTraining batch 1929 Loss: 0.200808\n",
            "\tTraining batch 1930 Loss: 0.355177\n",
            "\tTraining batch 1931 Loss: 0.476414\n",
            "\tTraining batch 1932 Loss: 0.487760\n",
            "\tTraining batch 1933 Loss: 0.853091\n",
            "\tTraining batch 1934 Loss: 0.513783\n",
            "\tTraining batch 1935 Loss: 0.602144\n",
            "\tTraining batch 1936 Loss: 0.181453\n",
            "\tTraining batch 1937 Loss: 0.741801\n",
            "\tTraining batch 1938 Loss: 0.518695\n",
            "\tTraining batch 1939 Loss: 0.518776\n",
            "\tTraining batch 1940 Loss: 0.536317\n",
            "\tTraining batch 1941 Loss: 0.425694\n",
            "\tTraining batch 1942 Loss: 0.430249\n",
            "\tTraining batch 1943 Loss: 0.123022\n",
            "\tTraining batch 1944 Loss: 0.257876\n",
            "\tTraining batch 1945 Loss: 0.379715\n",
            "\tTraining batch 1946 Loss: 0.356862\n",
            "\tTraining batch 1947 Loss: 0.497822\n",
            "\tTraining batch 1948 Loss: 0.677909\n",
            "\tTraining batch 1949 Loss: 0.323789\n",
            "\tTraining batch 1950 Loss: 0.777965\n",
            "\tTraining batch 1951 Loss: 0.534442\n",
            "\tTraining batch 1952 Loss: 0.595428\n",
            "\tTraining batch 1953 Loss: 0.386171\n",
            "\tTraining batch 1954 Loss: 0.447872\n",
            "\tTraining batch 1955 Loss: 0.163634\n",
            "\tTraining batch 1956 Loss: 0.523199\n",
            "\tTraining batch 1957 Loss: 0.216560\n",
            "\tTraining batch 1958 Loss: 0.436190\n",
            "\tTraining batch 1959 Loss: 0.371208\n",
            "\tTraining batch 1960 Loss: 0.502374\n",
            "\tTraining batch 1961 Loss: 0.589316\n",
            "\tTraining batch 1962 Loss: 0.587627\n",
            "\tTraining batch 1963 Loss: 0.298042\n",
            "\tTraining batch 1964 Loss: 0.650670\n",
            "\tTraining batch 1965 Loss: 0.403170\n",
            "\tTraining batch 1966 Loss: 0.663043\n",
            "\tTraining batch 1967 Loss: 0.620930\n",
            "\tTraining batch 1968 Loss: 0.272492\n",
            "\tTraining batch 1969 Loss: 0.671629\n",
            "\tTraining batch 1970 Loss: 0.314981\n",
            "\tTraining batch 1971 Loss: 1.062628\n",
            "\tTraining batch 1972 Loss: 0.641372\n",
            "\tTraining batch 1973 Loss: 0.487061\n",
            "\tTraining batch 1974 Loss: 0.470891\n",
            "\tTraining batch 1975 Loss: 0.601505\n",
            "\tTraining batch 1976 Loss: 0.261991\n",
            "\tTraining batch 1977 Loss: 0.714861\n",
            "\tTraining batch 1978 Loss: 0.296906\n",
            "\tTraining batch 1979 Loss: 0.593802\n",
            "\tTraining batch 1980 Loss: 0.358175\n",
            "\tTraining batch 1981 Loss: 1.128644\n",
            "\tTraining batch 1982 Loss: 0.811437\n",
            "\tTraining batch 1983 Loss: 0.519313\n",
            "\tTraining batch 1984 Loss: 0.497222\n",
            "\tTraining batch 1985 Loss: 0.539682\n",
            "\tTraining batch 1986 Loss: 0.517475\n",
            "\tTraining batch 1987 Loss: 0.609680\n",
            "\tTraining batch 1988 Loss: 0.673039\n",
            "\tTraining batch 1989 Loss: 0.458213\n",
            "\tTraining batch 1990 Loss: 0.666473\n",
            "\tTraining batch 1991 Loss: 0.273626\n",
            "\tTraining batch 1992 Loss: 0.859697\n",
            "\tTraining batch 1993 Loss: 0.715941\n",
            "\tTraining batch 1994 Loss: 0.928882\n",
            "\tTraining batch 1995 Loss: 0.590030\n",
            "\tTraining batch 1996 Loss: 0.279579\n",
            "\tTraining batch 1997 Loss: 0.326568\n",
            "\tTraining batch 1998 Loss: 0.564472\n",
            "\tTraining batch 1999 Loss: 0.848890\n",
            "\tTraining batch 2000 Loss: 0.625746\n",
            "\tTraining batch 2001 Loss: 0.726302\n",
            "\tTraining batch 2002 Loss: 0.626984\n",
            "\tTraining batch 2003 Loss: 0.458163\n",
            "\tTraining batch 2004 Loss: 0.360827\n",
            "\tTraining batch 2005 Loss: 0.416717\n",
            "\tTraining batch 2006 Loss: 0.485467\n",
            "\tTraining batch 2007 Loss: 0.588213\n",
            "\tTraining batch 2008 Loss: 0.418624\n",
            "\tTraining batch 2009 Loss: 0.741281\n",
            "\tTraining batch 2010 Loss: 0.618873\n",
            "\tTraining batch 2011 Loss: 0.517009\n",
            "\tTraining batch 2012 Loss: 0.614592\n",
            "\tTraining batch 2013 Loss: 0.606534\n",
            "\tTraining batch 2014 Loss: 0.357085\n",
            "\tTraining batch 2015 Loss: 0.273184\n",
            "\tTraining batch 2016 Loss: 0.494422\n",
            "\tTraining batch 2017 Loss: 0.195993\n",
            "\tTraining batch 2018 Loss: 0.492752\n",
            "\tTraining batch 2019 Loss: 0.623569\n",
            "\tTraining batch 2020 Loss: 0.761094\n",
            "\tTraining batch 2021 Loss: 0.205331\n",
            "\tTraining batch 2022 Loss: 0.615449\n",
            "\tTraining batch 2023 Loss: 0.551080\n",
            "\tTraining batch 2024 Loss: 1.312772\n",
            "\tTraining batch 2025 Loss: 0.937962\n",
            "\tTraining batch 2026 Loss: 0.565448\n",
            "\tTraining batch 2027 Loss: 0.468263\n",
            "\tTraining batch 2028 Loss: 0.257441\n",
            "\tTraining batch 2029 Loss: 0.496212\n",
            "\tTraining batch 2030 Loss: 0.332740\n",
            "\tTraining batch 2031 Loss: 0.703451\n",
            "\tTraining batch 2032 Loss: 1.140904\n",
            "\tTraining batch 2033 Loss: 0.511653\n",
            "\tTraining batch 2034 Loss: 0.268969\n",
            "\tTraining batch 2035 Loss: 0.143373\n",
            "\tTraining batch 2036 Loss: 0.296755\n",
            "\tTraining batch 2037 Loss: 0.453407\n",
            "\tTraining batch 2038 Loss: 0.339183\n",
            "\tTraining batch 2039 Loss: 0.566925\n",
            "\tTraining batch 2040 Loss: 0.356625\n",
            "\tTraining batch 2041 Loss: 0.630945\n",
            "\tTraining batch 2042 Loss: 0.600695\n",
            "\tTraining batch 2043 Loss: 0.976932\n",
            "\tTraining batch 2044 Loss: 0.516967\n",
            "\tTraining batch 2045 Loss: 0.610197\n",
            "\tTraining batch 2046 Loss: 0.935452\n",
            "\tTraining batch 2047 Loss: 0.649903\n",
            "\tTraining batch 2048 Loss: 0.682901\n",
            "\tTraining batch 2049 Loss: 0.892874\n",
            "\tTraining batch 2050 Loss: 0.983535\n",
            "\tTraining batch 2051 Loss: 0.201175\n",
            "\tTraining batch 2052 Loss: 1.010154\n",
            "\tTraining batch 2053 Loss: 0.681631\n",
            "\tTraining batch 2054 Loss: 0.271820\n",
            "\tTraining batch 2055 Loss: 0.459537\n",
            "\tTraining batch 2056 Loss: 0.736394\n",
            "\tTraining batch 2057 Loss: 0.638471\n",
            "\tTraining batch 2058 Loss: 0.706176\n",
            "\tTraining batch 2059 Loss: 0.362825\n",
            "\tTraining batch 2060 Loss: 0.372908\n",
            "\tTraining batch 2061 Loss: 0.436128\n",
            "\tTraining batch 2062 Loss: 0.394886\n",
            "\tTraining batch 2063 Loss: 0.565746\n",
            "\tTraining batch 2064 Loss: 0.569837\n",
            "\tTraining batch 2065 Loss: 0.300272\n",
            "\tTraining batch 2066 Loss: 1.013261\n",
            "\tTraining batch 2067 Loss: 0.727605\n",
            "\tTraining batch 2068 Loss: 0.445983\n",
            "\tTraining batch 2069 Loss: 0.660952\n",
            "\tTraining batch 2070 Loss: 0.380748\n",
            "\tTraining batch 2071 Loss: 0.292755\n",
            "\tTraining batch 2072 Loss: 0.369106\n",
            "\tTraining batch 2073 Loss: 0.576435\n",
            "\tTraining batch 2074 Loss: 0.230898\n",
            "\tTraining batch 2075 Loss: 0.569934\n",
            "\tTraining batch 2076 Loss: 0.461539\n",
            "\tTraining batch 2077 Loss: 0.705014\n",
            "\tTraining batch 2078 Loss: 0.430221\n",
            "\tTraining batch 2079 Loss: 0.229477\n",
            "\tTraining batch 2080 Loss: 1.055801\n",
            "\tTraining batch 2081 Loss: 0.396003\n",
            "\tTraining batch 2082 Loss: 0.454817\n",
            "\tTraining batch 2083 Loss: 0.476266\n",
            "\tTraining batch 2084 Loss: 0.288119\n",
            "\tTraining batch 2085 Loss: 0.336456\n",
            "\tTraining batch 2086 Loss: 0.716111\n",
            "\tTraining batch 2087 Loss: 0.388495\n",
            "\tTraining batch 2088 Loss: 0.598588\n",
            "\tTraining batch 2089 Loss: 0.382760\n",
            "\tTraining batch 2090 Loss: 0.620343\n",
            "\tTraining batch 2091 Loss: 0.444944\n",
            "\tTraining batch 2092 Loss: 0.619672\n",
            "\tTraining batch 2093 Loss: 0.647748\n",
            "\tTraining batch 2094 Loss: 0.384251\n",
            "\tTraining batch 2095 Loss: 0.419896\n",
            "\tTraining batch 2096 Loss: 0.316650\n",
            "\tTraining batch 2097 Loss: 0.962740\n",
            "\tTraining batch 2098 Loss: 0.601175\n",
            "\tTraining batch 2099 Loss: 0.404861\n",
            "\tTraining batch 2100 Loss: 0.426113\n",
            "\tTraining batch 2101 Loss: 0.413188\n",
            "\tTraining batch 2102 Loss: 0.406765\n",
            "\tTraining batch 2103 Loss: 0.586594\n",
            "\tTraining batch 2104 Loss: 0.566561\n",
            "\tTraining batch 2105 Loss: 0.249808\n",
            "\tTraining batch 2106 Loss: 0.487014\n",
            "\tTraining batch 2107 Loss: 0.325050\n",
            "\tTraining batch 2108 Loss: 0.523331\n",
            "\tTraining batch 2109 Loss: 0.690782\n",
            "\tTraining batch 2110 Loss: 0.555206\n",
            "\tTraining batch 2111 Loss: 0.333205\n",
            "\tTraining batch 2112 Loss: 0.366539\n",
            "\tTraining batch 2113 Loss: 0.452190\n",
            "\tTraining batch 2114 Loss: 0.480300\n",
            "\tTraining batch 2115 Loss: 0.289774\n",
            "\tTraining batch 2116 Loss: 0.189645\n",
            "\tTraining batch 2117 Loss: 0.756047\n",
            "\tTraining batch 2118 Loss: 0.690401\n",
            "\tTraining batch 2119 Loss: 0.240561\n",
            "\tTraining batch 2120 Loss: 0.942169\n",
            "\tTraining batch 2121 Loss: 0.563344\n",
            "\tTraining batch 2122 Loss: 0.423809\n",
            "\tTraining batch 2123 Loss: 0.378379\n",
            "\tTraining batch 2124 Loss: 0.329754\n",
            "\tTraining batch 2125 Loss: 0.284140\n",
            "\tTraining batch 2126 Loss: 0.915213\n",
            "\tTraining batch 2127 Loss: 0.643811\n",
            "\tTraining batch 2128 Loss: 0.376582\n",
            "\tTraining batch 2129 Loss: 0.644397\n",
            "\tTraining batch 2130 Loss: 0.672799\n",
            "\tTraining batch 2131 Loss: 0.624156\n",
            "\tTraining batch 2132 Loss: 0.401217\n",
            "\tTraining batch 2133 Loss: 0.441986\n",
            "\tTraining batch 2134 Loss: 0.731737\n",
            "\tTraining batch 2135 Loss: 0.740678\n",
            "\tTraining batch 2136 Loss: 0.362897\n",
            "\tTraining batch 2137 Loss: 0.935018\n",
            "\tTraining batch 2138 Loss: 0.574636\n",
            "\tTraining batch 2139 Loss: 0.527422\n",
            "\tTraining batch 2140 Loss: 0.903681\n",
            "\tTraining batch 2141 Loss: 0.447326\n",
            "\tTraining batch 2142 Loss: 0.585354\n",
            "\tTraining batch 2143 Loss: 0.435439\n",
            "\tTraining batch 2144 Loss: 0.399222\n",
            "\tTraining batch 2145 Loss: 0.675942\n",
            "\tTraining batch 2146 Loss: 0.596970\n",
            "\tTraining batch 2147 Loss: 0.397553\n",
            "\tTraining batch 2148 Loss: 0.299818\n",
            "\tTraining batch 2149 Loss: 0.767429\n",
            "\tTraining batch 2150 Loss: 0.568592\n",
            "\tTraining batch 2151 Loss: 0.766524\n",
            "\tTraining batch 2152 Loss: 0.165935\n",
            "\tTraining batch 2153 Loss: 0.418294\n",
            "\tTraining batch 2154 Loss: 0.648277\n",
            "\tTraining batch 2155 Loss: 0.969932\n",
            "\tTraining batch 2156 Loss: 0.403655\n",
            "\tTraining batch 2157 Loss: 0.387438\n",
            "\tTraining batch 2158 Loss: 0.786207\n",
            "\tTraining batch 2159 Loss: 0.351557\n",
            "\tTraining batch 2160 Loss: 0.728709\n",
            "\tTraining batch 2161 Loss: 0.574908\n",
            "\tTraining batch 2162 Loss: 0.524459\n",
            "\tTraining batch 2163 Loss: 0.416301\n",
            "\tTraining batch 2164 Loss: 0.252849\n",
            "\tTraining batch 2165 Loss: 0.812161\n",
            "\tTraining batch 2166 Loss: 0.360910\n",
            "\tTraining batch 2167 Loss: 0.486633\n",
            "\tTraining batch 2168 Loss: 0.258141\n",
            "\tTraining batch 2169 Loss: 0.522043\n",
            "\tTraining batch 2170 Loss: 0.577230\n",
            "\tTraining batch 2171 Loss: 0.406761\n",
            "\tTraining batch 2172 Loss: 0.525847\n",
            "\tTraining batch 2173 Loss: 0.309801\n",
            "\tTraining batch 2174 Loss: 0.626326\n",
            "\tTraining batch 2175 Loss: 1.271581\n",
            "\tTraining batch 2176 Loss: 0.710335\n",
            "\tTraining batch 2177 Loss: 0.369725\n",
            "\tTraining batch 2178 Loss: 0.539237\n",
            "\tTraining batch 2179 Loss: 0.623663\n",
            "\tTraining batch 2180 Loss: 0.243938\n",
            "\tTraining batch 2181 Loss: 0.315903\n",
            "\tTraining batch 2182 Loss: 0.202569\n",
            "\tTraining batch 2183 Loss: 0.269430\n",
            "\tTraining batch 2184 Loss: 0.289595\n",
            "\tTraining batch 2185 Loss: 0.548266\n",
            "\tTraining batch 2186 Loss: 0.472133\n",
            "\tTraining batch 2187 Loss: 0.788526\n",
            "\tTraining batch 2188 Loss: 1.012092\n",
            "\tTraining batch 2189 Loss: 0.383266\n",
            "\tTraining batch 2190 Loss: 0.476859\n",
            "\tTraining batch 2191 Loss: 0.491699\n",
            "\tTraining batch 2192 Loss: 0.716746\n",
            "\tTraining batch 2193 Loss: 0.592996\n",
            "\tTraining batch 2194 Loss: 0.634312\n",
            "\tTraining batch 2195 Loss: 0.556924\n",
            "\tTraining batch 2196 Loss: 0.836063\n",
            "\tTraining batch 2197 Loss: 0.528055\n",
            "\tTraining batch 2198 Loss: 0.176943\n",
            "\tTraining batch 2199 Loss: 0.502669\n",
            "\tTraining batch 2200 Loss: 0.331262\n",
            "\tTraining batch 2201 Loss: 0.261246\n",
            "\tTraining batch 2202 Loss: 0.523623\n",
            "\tTraining batch 2203 Loss: 0.218695\n",
            "\tTraining batch 2204 Loss: 0.339882\n",
            "\tTraining batch 2205 Loss: 0.311069\n",
            "\tTraining batch 2206 Loss: 1.022751\n",
            "\tTraining batch 2207 Loss: 0.372437\n",
            "\tTraining batch 2208 Loss: 0.383422\n",
            "\tTraining batch 2209 Loss: 0.355093\n",
            "\tTraining batch 2210 Loss: 0.765658\n",
            "\tTraining batch 2211 Loss: 0.622409\n",
            "\tTraining batch 2212 Loss: 0.722888\n",
            "\tTraining batch 2213 Loss: 0.740533\n",
            "\tTraining batch 2214 Loss: 0.678489\n",
            "\tTraining batch 2215 Loss: 0.238913\n",
            "\tTraining batch 2216 Loss: 0.297683\n",
            "\tTraining batch 2217 Loss: 1.018176\n",
            "\tTraining batch 2218 Loss: 0.353335\n",
            "\tTraining batch 2219 Loss: 0.535100\n",
            "\tTraining batch 2220 Loss: 0.247680\n",
            "\tTraining batch 2221 Loss: 0.516019\n",
            "\tTraining batch 2222 Loss: 0.418856\n",
            "\tTraining batch 2223 Loss: 0.325145\n",
            "\tTraining batch 2224 Loss: 0.530237\n",
            "\tTraining batch 2225 Loss: 0.616174\n",
            "\tTraining batch 2226 Loss: 0.416767\n",
            "\tTraining batch 2227 Loss: 0.280208\n",
            "\tTraining batch 2228 Loss: 0.870433\n",
            "\tTraining batch 2229 Loss: 0.750389\n",
            "\tTraining batch 2230 Loss: 0.320217\n",
            "\tTraining batch 2231 Loss: 0.435891\n",
            "\tTraining batch 2232 Loss: 0.506244\n",
            "\tTraining batch 2233 Loss: 0.407516\n",
            "\tTraining batch 2234 Loss: 0.480495\n",
            "\tTraining batch 2235 Loss: 0.602331\n",
            "\tTraining batch 2236 Loss: 0.179406\n",
            "\tTraining batch 2237 Loss: 0.514869\n",
            "\tTraining batch 2238 Loss: 0.477325\n",
            "\tTraining batch 2239 Loss: 0.481711\n",
            "\tTraining batch 2240 Loss: 0.629643\n",
            "\tTraining batch 2241 Loss: 0.545585\n",
            "\tTraining batch 2242 Loss: 0.585261\n",
            "\tTraining batch 2243 Loss: 0.348329\n",
            "\tTraining batch 2244 Loss: 0.798860\n",
            "\tTraining batch 2245 Loss: 0.505767\n",
            "\tTraining batch 2246 Loss: 0.357752\n",
            "\tTraining batch 2247 Loss: 0.695081\n",
            "\tTraining batch 2248 Loss: 0.270445\n",
            "\tTraining batch 2249 Loss: 0.692129\n",
            "\tTraining batch 2250 Loss: 0.555660\n",
            "\tTraining batch 2251 Loss: 0.204333\n",
            "\tTraining batch 2252 Loss: 0.295242\n",
            "\tTraining batch 2253 Loss: 0.620155\n",
            "\tTraining batch 2254 Loss: 0.610006\n",
            "\tTraining batch 2255 Loss: 0.267496\n",
            "\tTraining batch 2256 Loss: 0.598530\n",
            "\tTraining batch 2257 Loss: 0.562751\n",
            "\tTraining batch 2258 Loss: 0.368597\n",
            "\tTraining batch 2259 Loss: 0.131197\n",
            "\tTraining batch 2260 Loss: 0.358720\n",
            "\tTraining batch 2261 Loss: 0.405583\n",
            "\tTraining batch 2262 Loss: 0.645788\n",
            "\tTraining batch 2263 Loss: 0.461310\n",
            "\tTraining batch 2264 Loss: 0.411693\n",
            "\tTraining batch 2265 Loss: 0.264863\n",
            "\tTraining batch 2266 Loss: 0.621682\n",
            "\tTraining batch 2267 Loss: 0.600070\n",
            "\tTraining batch 2268 Loss: 0.166383\n",
            "\tTraining batch 2269 Loss: 0.163041\n",
            "\tTraining batch 2270 Loss: 0.454253\n",
            "\tTraining batch 2271 Loss: 0.405802\n",
            "\tTraining batch 2272 Loss: 0.243936\n",
            "\tTraining batch 2273 Loss: 0.437327\n",
            "\tTraining batch 2274 Loss: 0.710972\n",
            "\tTraining batch 2275 Loss: 0.518021\n",
            "\tTraining batch 2276 Loss: 0.579779\n",
            "\tTraining batch 2277 Loss: 0.488373\n",
            "\tTraining batch 2278 Loss: 0.524551\n",
            "\tTraining batch 2279 Loss: 0.448554\n",
            "\tTraining batch 2280 Loss: 0.426203\n",
            "\tTraining batch 2281 Loss: 0.722203\n",
            "\tTraining batch 2282 Loss: 0.687666\n",
            "\tTraining batch 2283 Loss: 0.520578\n",
            "\tTraining batch 2284 Loss: 0.645228\n",
            "\tTraining batch 2285 Loss: 0.205990\n",
            "\tTraining batch 2286 Loss: 0.439950\n",
            "\tTraining batch 2287 Loss: 0.762326\n",
            "\tTraining batch 2288 Loss: 0.500695\n",
            "\tTraining batch 2289 Loss: 0.820555\n",
            "\tTraining batch 2290 Loss: 0.917326\n",
            "\tTraining batch 2291 Loss: 0.230767\n",
            "\tTraining batch 2292 Loss: 0.504107\n",
            "\tTraining batch 2293 Loss: 0.518917\n",
            "\tTraining batch 2294 Loss: 0.407506\n",
            "\tTraining batch 2295 Loss: 0.491238\n",
            "\tTraining batch 2296 Loss: 0.538203\n",
            "\tTraining batch 2297 Loss: 0.357276\n",
            "\tTraining batch 2298 Loss: 0.482467\n",
            "\tTraining batch 2299 Loss: 0.078568\n",
            "\tTraining batch 2300 Loss: 0.430976\n",
            "\tTraining batch 2301 Loss: 0.504782\n",
            "\tTraining batch 2302 Loss: 0.373359\n",
            "\tTraining batch 2303 Loss: 0.600474\n",
            "\tTraining batch 2304 Loss: 0.784244\n",
            "\tTraining batch 2305 Loss: 0.441798\n",
            "\tTraining batch 2306 Loss: 0.652357\n",
            "\tTraining batch 2307 Loss: 0.277426\n",
            "\tTraining batch 2308 Loss: 0.274373\n",
            "\tTraining batch 2309 Loss: 0.474795\n",
            "\tTraining batch 2310 Loss: 0.312017\n",
            "\tTraining batch 2311 Loss: 0.298847\n",
            "\tTraining batch 2312 Loss: 0.823588\n",
            "\tTraining batch 2313 Loss: 0.607000\n",
            "\tTraining batch 2314 Loss: 0.370690\n",
            "\tTraining batch 2315 Loss: 0.968559\n",
            "\tTraining batch 2316 Loss: 0.417075\n",
            "\tTraining batch 2317 Loss: 0.437369\n",
            "\tTraining batch 2318 Loss: 0.343430\n",
            "\tTraining batch 2319 Loss: 0.835234\n",
            "\tTraining batch 2320 Loss: 0.589780\n",
            "\tTraining batch 2321 Loss: 0.235130\n",
            "\tTraining batch 2322 Loss: 0.366136\n",
            "\tTraining batch 2323 Loss: 0.382005\n",
            "\tTraining batch 2324 Loss: 0.602359\n",
            "\tTraining batch 2325 Loss: 0.701931\n",
            "\tTraining batch 2326 Loss: 0.682299\n",
            "\tTraining batch 2327 Loss: 0.541112\n",
            "\tTraining batch 2328 Loss: 0.483140\n",
            "\tTraining batch 2329 Loss: 0.749811\n",
            "\tTraining batch 2330 Loss: 0.428485\n",
            "\tTraining batch 2331 Loss: 0.234932\n",
            "\tTraining batch 2332 Loss: 0.672603\n",
            "\tTraining batch 2333 Loss: 0.524624\n",
            "\tTraining batch 2334 Loss: 0.809216\n",
            "\tTraining batch 2335 Loss: 0.434131\n",
            "\tTraining batch 2336 Loss: 0.354440\n",
            "\tTraining batch 2337 Loss: 0.703757\n",
            "\tTraining batch 2338 Loss: 0.379308\n",
            "\tTraining batch 2339 Loss: 0.300514\n",
            "\tTraining batch 2340 Loss: 0.416095\n",
            "\tTraining batch 2341 Loss: 0.778263\n",
            "\tTraining batch 2342 Loss: 0.370728\n",
            "\tTraining batch 2343 Loss: 0.608461\n",
            "\tTraining batch 2344 Loss: 0.339230\n",
            "\tTraining batch 2345 Loss: 0.617418\n",
            "\tTraining batch 2346 Loss: 0.454244\n",
            "\tTraining batch 2347 Loss: 0.902865\n",
            "\tTraining batch 2348 Loss: 1.010701\n",
            "\tTraining batch 2349 Loss: 0.561719\n",
            "\tTraining batch 2350 Loss: 0.771290\n",
            "\tTraining batch 2351 Loss: 0.878730\n",
            "\tTraining batch 2352 Loss: 0.652028\n",
            "\tTraining batch 2353 Loss: 0.340391\n",
            "\tTraining batch 2354 Loss: 0.993876\n",
            "\tTraining batch 2355 Loss: 0.387373\n",
            "\tTraining batch 2356 Loss: 0.948689\n",
            "\tTraining batch 2357 Loss: 0.335234\n",
            "\tTraining batch 2358 Loss: 0.438995\n",
            "\tTraining batch 2359 Loss: 0.321864\n",
            "\tTraining batch 2360 Loss: 0.474589\n",
            "\tTraining batch 2361 Loss: 0.492744\n",
            "\tTraining batch 2362 Loss: 0.534021\n",
            "\tTraining batch 2363 Loss: 0.510978\n",
            "\tTraining batch 2364 Loss: 0.748629\n",
            "\tTraining batch 2365 Loss: 0.402107\n",
            "\tTraining batch 2366 Loss: 0.335260\n",
            "\tTraining batch 2367 Loss: 0.784734\n",
            "\tTraining batch 2368 Loss: 0.534313\n",
            "\tTraining batch 2369 Loss: 0.264788\n",
            "\tTraining batch 2370 Loss: 0.544408\n",
            "\tTraining batch 2371 Loss: 0.569399\n",
            "\tTraining batch 2372 Loss: 0.405313\n",
            "\tTraining batch 2373 Loss: 0.727301\n",
            "\tTraining batch 2374 Loss: 0.631709\n",
            "\tTraining batch 2375 Loss: 0.615456\n",
            "\tTraining batch 2376 Loss: 0.630049\n",
            "\tTraining batch 2377 Loss: 0.418669\n",
            "\tTraining batch 2378 Loss: 0.215049\n",
            "\tTraining batch 2379 Loss: 0.167664\n",
            "\tTraining batch 2380 Loss: 0.413912\n",
            "\tTraining batch 2381 Loss: 0.368102\n",
            "\tTraining batch 2382 Loss: 0.469452\n",
            "\tTraining batch 2383 Loss: 0.285778\n",
            "\tTraining batch 2384 Loss: 0.406982\n",
            "\tTraining batch 2385 Loss: 0.290795\n",
            "\tTraining batch 2386 Loss: 0.297414\n",
            "\tTraining batch 2387 Loss: 0.919112\n",
            "\tTraining batch 2388 Loss: 0.479309\n",
            "\tTraining batch 2389 Loss: 0.427423\n",
            "\tTraining batch 2390 Loss: 0.591623\n",
            "\tTraining batch 2391 Loss: 0.557043\n",
            "\tTraining batch 2392 Loss: 0.242172\n",
            "\tTraining batch 2393 Loss: 0.653110\n",
            "\tTraining batch 2394 Loss: 0.773681\n",
            "\tTraining batch 2395 Loss: 0.367508\n",
            "\tTraining batch 2396 Loss: 0.745899\n",
            "\tTraining batch 2397 Loss: 0.473783\n",
            "\tTraining batch 2398 Loss: 0.367781\n",
            "\tTraining batch 2399 Loss: 0.419376\n",
            "\tTraining batch 2400 Loss: 0.633613\n",
            "\tTraining batch 2401 Loss: 0.733826\n",
            "\tTraining batch 2402 Loss: 0.605849\n",
            "\tTraining batch 2403 Loss: 0.721540\n",
            "\tTraining batch 2404 Loss: 0.606301\n",
            "\tTraining batch 2405 Loss: 0.497780\n",
            "\tTraining batch 2406 Loss: 0.707132\n",
            "\tTraining batch 2407 Loss: 0.643490\n",
            "\tTraining batch 2408 Loss: 0.274943\n",
            "\tTraining batch 2409 Loss: 0.293041\n",
            "\tTraining batch 2410 Loss: 0.238986\n",
            "\tTraining batch 2411 Loss: 0.496070\n",
            "\tTraining batch 2412 Loss: 0.364719\n",
            "\tTraining batch 2413 Loss: 0.629085\n",
            "\tTraining batch 2414 Loss: 0.532432\n",
            "\tTraining batch 2415 Loss: 0.500986\n",
            "\tTraining batch 2416 Loss: 0.427757\n",
            "\tTraining batch 2417 Loss: 0.134189\n",
            "\tTraining batch 2418 Loss: 0.563642\n",
            "\tTraining batch 2419 Loss: 0.531645\n",
            "\tTraining batch 2420 Loss: 0.216289\n",
            "\tTraining batch 2421 Loss: 0.363363\n",
            "\tTraining batch 2422 Loss: 0.854240\n",
            "\tTraining batch 2423 Loss: 0.378163\n",
            "\tTraining batch 2424 Loss: 0.136102\n",
            "\tTraining batch 2425 Loss: 0.619595\n",
            "\tTraining batch 2426 Loss: 0.423104\n",
            "\tTraining batch 2427 Loss: 0.837247\n",
            "\tTraining batch 2428 Loss: 0.385282\n",
            "\tTraining batch 2429 Loss: 0.486476\n",
            "\tTraining batch 2430 Loss: 0.237086\n",
            "\tTraining batch 2431 Loss: 0.283710\n",
            "\tTraining batch 2432 Loss: 0.825338\n",
            "\tTraining batch 2433 Loss: 0.244513\n",
            "\tTraining batch 2434 Loss: 0.599890\n",
            "\tTraining batch 2435 Loss: 0.418902\n",
            "\tTraining batch 2436 Loss: 0.815107\n",
            "\tTraining batch 2437 Loss: 0.578025\n",
            "\tTraining batch 2438 Loss: 0.323824\n",
            "\tTraining batch 2439 Loss: 0.639935\n",
            "\tTraining batch 2440 Loss: 0.502488\n",
            "\tTraining batch 2441 Loss: 0.287255\n",
            "\tTraining batch 2442 Loss: 0.363379\n",
            "\tTraining batch 2443 Loss: 0.468523\n",
            "\tTraining batch 2444 Loss: 0.504029\n",
            "\tTraining batch 2445 Loss: 0.368932\n",
            "\tTraining batch 2446 Loss: 0.150495\n",
            "\tTraining batch 2447 Loss: 0.330119\n",
            "\tTraining batch 2448 Loss: 0.219250\n",
            "\tTraining batch 2449 Loss: 0.424357\n",
            "\tTraining batch 2450 Loss: 0.484413\n",
            "\tTraining batch 2451 Loss: 0.428866\n",
            "\tTraining batch 2452 Loss: 0.570720\n",
            "\tTraining batch 2453 Loss: 0.237918\n",
            "\tTraining batch 2454 Loss: 0.076515\n",
            "\tTraining batch 2455 Loss: 0.450611\n",
            "\tTraining batch 2456 Loss: 0.452524\n",
            "\tTraining batch 2457 Loss: 0.176555\n",
            "\tTraining batch 2458 Loss: 0.644826\n",
            "\tTraining batch 2459 Loss: 0.347207\n",
            "\tTraining batch 2460 Loss: 0.502159\n",
            "\tTraining batch 2461 Loss: 0.422207\n",
            "\tTraining batch 2462 Loss: 0.284128\n",
            "\tTraining batch 2463 Loss: 0.246858\n",
            "\tTraining batch 2464 Loss: 0.301185\n",
            "\tTraining batch 2465 Loss: 0.333016\n",
            "\tTraining batch 2466 Loss: 0.522186\n",
            "\tTraining batch 2467 Loss: 0.772311\n",
            "\tTraining batch 2468 Loss: 0.437594\n",
            "\tTraining batch 2469 Loss: 0.807619\n",
            "\tTraining batch 2470 Loss: 0.462597\n",
            "\tTraining batch 2471 Loss: 0.542031\n",
            "\tTraining batch 2472 Loss: 0.707038\n",
            "\tTraining batch 2473 Loss: 0.530384\n",
            "\tTraining batch 2474 Loss: 0.426679\n",
            "\tTraining batch 2475 Loss: 0.564274\n",
            "\tTraining batch 2476 Loss: 0.313374\n",
            "\tTraining batch 2477 Loss: 0.279536\n",
            "\tTraining batch 2478 Loss: 0.535845\n",
            "\tTraining batch 2479 Loss: 0.431960\n",
            "\tTraining batch 2480 Loss: 0.251989\n",
            "\tTraining batch 2481 Loss: 0.400621\n",
            "\tTraining batch 2482 Loss: 0.266863\n",
            "\tTraining batch 2483 Loss: 0.312591\n",
            "\tTraining batch 2484 Loss: 0.365875\n",
            "\tTraining batch 2485 Loss: 0.662229\n",
            "\tTraining batch 2486 Loss: 0.490514\n",
            "\tTraining batch 2487 Loss: 0.754280\n",
            "\tTraining batch 2488 Loss: 0.465412\n",
            "\tTraining batch 2489 Loss: 0.368676\n",
            "\tTraining batch 2490 Loss: 0.338198\n",
            "\tTraining batch 2491 Loss: 0.580577\n",
            "\tTraining batch 2492 Loss: 0.690097\n",
            "\tTraining batch 2493 Loss: 0.221603\n",
            "\tTraining batch 2494 Loss: 0.263010\n",
            "\tTraining batch 2495 Loss: 0.505810\n",
            "\tTraining batch 2496 Loss: 0.684442\n",
            "\tTraining batch 2497 Loss: 0.671388\n",
            "\tTraining batch 2498 Loss: 0.559868\n",
            "\tTraining batch 2499 Loss: 0.259213\n",
            "\tTraining batch 2500 Loss: 0.640305\n",
            "Epoca 1010 \t Loss de Treinamento: 0.02625296823680401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "N5h1T_qAVZ6d"
      },
      "source": [
        "**Teste**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rpAXF75VaVs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b78d3eb-30fe-4a71-8593-e2bf35a86467"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "modelo.eval()\n",
        "loss_teste = 0\n",
        "\n",
        "batch_count = 0\n",
        "correct = 0\n",
        "\n",
        "for x, y in test_loader:\n",
        "    \n",
        "    batch_count += 1\n",
        "    \n",
        "  # mover tensores para GPU, se aplicável\n",
        "    if usar_gpu:\n",
        "        x, y = x.cuda(), y.cuda()\n",
        "\n",
        "  # TODO: 1. Faça um forward no modelo passando x como entrada \n",
        "  #          (não deixe de capturar a saída)\n",
        "  #DONE\n",
        "    output = modelo(x)\n",
        "\n",
        "  # TODO: 2. Calcule a loss do batch (utilize o criterion para comparar a \n",
        "  #          saída do modelo com os targets y)\n",
        "  #DONE\n",
        "    loss = criterion(output, y)\n",
        "  \n",
        "  # atualização da loss de teste\n",
        "    loss_teste += loss\n",
        "\n",
        "  # TODO: 3. Defina a classificação de cada amostra do pertencente x.\n",
        "  #          Considere a classe com maior probabilidade na saída da rede.\n",
        "  #          Sugestão: utilize a função torch.max\n",
        "  #DONE\n",
        "    _, predicted = torch.max(output.data, 1)\n",
        "    correct += torch.sum(y==predicted)\n",
        "\n",
        "\n",
        "  # TODO: 4. Compare a predição com os targets corretos (y) e compute a acurácia\n",
        "  #          (Faça como achar melhor: dentro ou fora do loop, mas não deixe de\n",
        "  #           exibir o valor da métrica)\n",
        "  #          Sugestão: utilizar algum pacote para computar a métrica \n",
        "  #          (e.g., sklearn accuracy_score)\n",
        "  #DONE\n",
        "    #print(accuracy_score(y, predicted))\n",
        "    \n",
        "    print('accuracy: ' + str(correct) + '/10000' + '\\n')\n",
        "\n",
        "loss_teste = loss_teste/len(test_loader.dataset)\n",
        "print(f\"Loss de Teste: {loss_teste}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: tensor(18)/10000\n",
            "\n",
            "accuracy: tensor(31)/10000\n",
            "\n",
            "accuracy: tensor(45)/10000\n",
            "\n",
            "accuracy: tensor(60)/10000\n",
            "\n",
            "accuracy: tensor(75)/10000\n",
            "\n",
            "accuracy: tensor(89)/10000\n",
            "\n",
            "accuracy: tensor(105)/10000\n",
            "\n",
            "accuracy: tensor(116)/10000\n",
            "\n",
            "accuracy: tensor(130)/10000\n",
            "\n",
            "accuracy: tensor(143)/10000\n",
            "\n",
            "accuracy: tensor(158)/10000\n",
            "\n",
            "accuracy: tensor(169)/10000\n",
            "\n",
            "accuracy: tensor(182)/10000\n",
            "\n",
            "accuracy: tensor(194)/10000\n",
            "\n",
            "accuracy: tensor(210)/10000\n",
            "\n",
            "accuracy: tensor(222)/10000\n",
            "\n",
            "accuracy: tensor(239)/10000\n",
            "\n",
            "accuracy: tensor(253)/10000\n",
            "\n",
            "accuracy: tensor(266)/10000\n",
            "\n",
            "accuracy: tensor(279)/10000\n",
            "\n",
            "accuracy: tensor(296)/10000\n",
            "\n",
            "accuracy: tensor(308)/10000\n",
            "\n",
            "accuracy: tensor(324)/10000\n",
            "\n",
            "accuracy: tensor(339)/10000\n",
            "\n",
            "accuracy: tensor(355)/10000\n",
            "\n",
            "accuracy: tensor(371)/10000\n",
            "\n",
            "accuracy: tensor(387)/10000\n",
            "\n",
            "accuracy: tensor(399)/10000\n",
            "\n",
            "accuracy: tensor(410)/10000\n",
            "\n",
            "accuracy: tensor(426)/10000\n",
            "\n",
            "accuracy: tensor(442)/10000\n",
            "\n",
            "accuracy: tensor(455)/10000\n",
            "\n",
            "accuracy: tensor(471)/10000\n",
            "\n",
            "accuracy: tensor(483)/10000\n",
            "\n",
            "accuracy: tensor(497)/10000\n",
            "\n",
            "accuracy: tensor(513)/10000\n",
            "\n",
            "accuracy: tensor(526)/10000\n",
            "\n",
            "accuracy: tensor(541)/10000\n",
            "\n",
            "accuracy: tensor(553)/10000\n",
            "\n",
            "accuracy: tensor(565)/10000\n",
            "\n",
            "accuracy: tensor(579)/10000\n",
            "\n",
            "accuracy: tensor(591)/10000\n",
            "\n",
            "accuracy: tensor(610)/10000\n",
            "\n",
            "accuracy: tensor(625)/10000\n",
            "\n",
            "accuracy: tensor(640)/10000\n",
            "\n",
            "accuracy: tensor(656)/10000\n",
            "\n",
            "accuracy: tensor(670)/10000\n",
            "\n",
            "accuracy: tensor(687)/10000\n",
            "\n",
            "accuracy: tensor(704)/10000\n",
            "\n",
            "accuracy: tensor(720)/10000\n",
            "\n",
            "accuracy: tensor(736)/10000\n",
            "\n",
            "accuracy: tensor(749)/10000\n",
            "\n",
            "accuracy: tensor(762)/10000\n",
            "\n",
            "accuracy: tensor(777)/10000\n",
            "\n",
            "accuracy: tensor(792)/10000\n",
            "\n",
            "accuracy: tensor(806)/10000\n",
            "\n",
            "accuracy: tensor(818)/10000\n",
            "\n",
            "accuracy: tensor(832)/10000\n",
            "\n",
            "accuracy: tensor(848)/10000\n",
            "\n",
            "accuracy: tensor(865)/10000\n",
            "\n",
            "accuracy: tensor(881)/10000\n",
            "\n",
            "accuracy: tensor(891)/10000\n",
            "\n",
            "accuracy: tensor(902)/10000\n",
            "\n",
            "accuracy: tensor(913)/10000\n",
            "\n",
            "accuracy: tensor(928)/10000\n",
            "\n",
            "accuracy: tensor(943)/10000\n",
            "\n",
            "accuracy: tensor(953)/10000\n",
            "\n",
            "accuracy: tensor(969)/10000\n",
            "\n",
            "accuracy: tensor(988)/10000\n",
            "\n",
            "accuracy: tensor(1000)/10000\n",
            "\n",
            "accuracy: tensor(1017)/10000\n",
            "\n",
            "accuracy: tensor(1029)/10000\n",
            "\n",
            "accuracy: tensor(1044)/10000\n",
            "\n",
            "accuracy: tensor(1061)/10000\n",
            "\n",
            "accuracy: tensor(1073)/10000\n",
            "\n",
            "accuracy: tensor(1085)/10000\n",
            "\n",
            "accuracy: tensor(1100)/10000\n",
            "\n",
            "accuracy: tensor(1115)/10000\n",
            "\n",
            "accuracy: tensor(1128)/10000\n",
            "\n",
            "accuracy: tensor(1144)/10000\n",
            "\n",
            "accuracy: tensor(1156)/10000\n",
            "\n",
            "accuracy: tensor(1170)/10000\n",
            "\n",
            "accuracy: tensor(1185)/10000\n",
            "\n",
            "accuracy: tensor(1201)/10000\n",
            "\n",
            "accuracy: tensor(1214)/10000\n",
            "\n",
            "accuracy: tensor(1228)/10000\n",
            "\n",
            "accuracy: tensor(1237)/10000\n",
            "\n",
            "accuracy: tensor(1252)/10000\n",
            "\n",
            "accuracy: tensor(1264)/10000\n",
            "\n",
            "accuracy: tensor(1277)/10000\n",
            "\n",
            "accuracy: tensor(1289)/10000\n",
            "\n",
            "accuracy: tensor(1302)/10000\n",
            "\n",
            "accuracy: tensor(1316)/10000\n",
            "\n",
            "accuracy: tensor(1330)/10000\n",
            "\n",
            "accuracy: tensor(1344)/10000\n",
            "\n",
            "accuracy: tensor(1360)/10000\n",
            "\n",
            "accuracy: tensor(1369)/10000\n",
            "\n",
            "accuracy: tensor(1384)/10000\n",
            "\n",
            "accuracy: tensor(1400)/10000\n",
            "\n",
            "accuracy: tensor(1416)/10000\n",
            "\n",
            "accuracy: tensor(1429)/10000\n",
            "\n",
            "accuracy: tensor(1440)/10000\n",
            "\n",
            "accuracy: tensor(1457)/10000\n",
            "\n",
            "accuracy: tensor(1472)/10000\n",
            "\n",
            "accuracy: tensor(1486)/10000\n",
            "\n",
            "accuracy: tensor(1501)/10000\n",
            "\n",
            "accuracy: tensor(1512)/10000\n",
            "\n",
            "accuracy: tensor(1530)/10000\n",
            "\n",
            "accuracy: tensor(1545)/10000\n",
            "\n",
            "accuracy: tensor(1560)/10000\n",
            "\n",
            "accuracy: tensor(1576)/10000\n",
            "\n",
            "accuracy: tensor(1594)/10000\n",
            "\n",
            "accuracy: tensor(1608)/10000\n",
            "\n",
            "accuracy: tensor(1621)/10000\n",
            "\n",
            "accuracy: tensor(1636)/10000\n",
            "\n",
            "accuracy: tensor(1649)/10000\n",
            "\n",
            "accuracy: tensor(1663)/10000\n",
            "\n",
            "accuracy: tensor(1676)/10000\n",
            "\n",
            "accuracy: tensor(1690)/10000\n",
            "\n",
            "accuracy: tensor(1702)/10000\n",
            "\n",
            "accuracy: tensor(1718)/10000\n",
            "\n",
            "accuracy: tensor(1731)/10000\n",
            "\n",
            "accuracy: tensor(1746)/10000\n",
            "\n",
            "accuracy: tensor(1765)/10000\n",
            "\n",
            "accuracy: tensor(1778)/10000\n",
            "\n",
            "accuracy: tensor(1792)/10000\n",
            "\n",
            "accuracy: tensor(1801)/10000\n",
            "\n",
            "accuracy: tensor(1816)/10000\n",
            "\n",
            "accuracy: tensor(1829)/10000\n",
            "\n",
            "accuracy: tensor(1840)/10000\n",
            "\n",
            "accuracy: tensor(1855)/10000\n",
            "\n",
            "accuracy: tensor(1870)/10000\n",
            "\n",
            "accuracy: tensor(1885)/10000\n",
            "\n",
            "accuracy: tensor(1903)/10000\n",
            "\n",
            "accuracy: tensor(1919)/10000\n",
            "\n",
            "accuracy: tensor(1932)/10000\n",
            "\n",
            "accuracy: tensor(1946)/10000\n",
            "\n",
            "accuracy: tensor(1964)/10000\n",
            "\n",
            "accuracy: tensor(1977)/10000\n",
            "\n",
            "accuracy: tensor(1989)/10000\n",
            "\n",
            "accuracy: tensor(2001)/10000\n",
            "\n",
            "accuracy: tensor(2018)/10000\n",
            "\n",
            "accuracy: tensor(2033)/10000\n",
            "\n",
            "accuracy: tensor(2045)/10000\n",
            "\n",
            "accuracy: tensor(2060)/10000\n",
            "\n",
            "accuracy: tensor(2076)/10000\n",
            "\n",
            "accuracy: tensor(2092)/10000\n",
            "\n",
            "accuracy: tensor(2104)/10000\n",
            "\n",
            "accuracy: tensor(2118)/10000\n",
            "\n",
            "accuracy: tensor(2132)/10000\n",
            "\n",
            "accuracy: tensor(2147)/10000\n",
            "\n",
            "accuracy: tensor(2161)/10000\n",
            "\n",
            "accuracy: tensor(2174)/10000\n",
            "\n",
            "accuracy: tensor(2187)/10000\n",
            "\n",
            "accuracy: tensor(2202)/10000\n",
            "\n",
            "accuracy: tensor(2214)/10000\n",
            "\n",
            "accuracy: tensor(2231)/10000\n",
            "\n",
            "accuracy: tensor(2246)/10000\n",
            "\n",
            "accuracy: tensor(2260)/10000\n",
            "\n",
            "accuracy: tensor(2275)/10000\n",
            "\n",
            "accuracy: tensor(2287)/10000\n",
            "\n",
            "accuracy: tensor(2301)/10000\n",
            "\n",
            "accuracy: tensor(2319)/10000\n",
            "\n",
            "accuracy: tensor(2337)/10000\n",
            "\n",
            "accuracy: tensor(2354)/10000\n",
            "\n",
            "accuracy: tensor(2369)/10000\n",
            "\n",
            "accuracy: tensor(2383)/10000\n",
            "\n",
            "accuracy: tensor(2396)/10000\n",
            "\n",
            "accuracy: tensor(2410)/10000\n",
            "\n",
            "accuracy: tensor(2424)/10000\n",
            "\n",
            "accuracy: tensor(2434)/10000\n",
            "\n",
            "accuracy: tensor(2447)/10000\n",
            "\n",
            "accuracy: tensor(2459)/10000\n",
            "\n",
            "accuracy: tensor(2475)/10000\n",
            "\n",
            "accuracy: tensor(2486)/10000\n",
            "\n",
            "accuracy: tensor(2496)/10000\n",
            "\n",
            "accuracy: tensor(2511)/10000\n",
            "\n",
            "accuracy: tensor(2527)/10000\n",
            "\n",
            "accuracy: tensor(2539)/10000\n",
            "\n",
            "accuracy: tensor(2553)/10000\n",
            "\n",
            "accuracy: tensor(2564)/10000\n",
            "\n",
            "accuracy: tensor(2578)/10000\n",
            "\n",
            "accuracy: tensor(2593)/10000\n",
            "\n",
            "accuracy: tensor(2608)/10000\n",
            "\n",
            "accuracy: tensor(2621)/10000\n",
            "\n",
            "accuracy: tensor(2635)/10000\n",
            "\n",
            "accuracy: tensor(2649)/10000\n",
            "\n",
            "accuracy: tensor(2661)/10000\n",
            "\n",
            "accuracy: tensor(2673)/10000\n",
            "\n",
            "accuracy: tensor(2687)/10000\n",
            "\n",
            "accuracy: tensor(2701)/10000\n",
            "\n",
            "accuracy: tensor(2715)/10000\n",
            "\n",
            "accuracy: tensor(2731)/10000\n",
            "\n",
            "accuracy: tensor(2748)/10000\n",
            "\n",
            "accuracy: tensor(2763)/10000\n",
            "\n",
            "accuracy: tensor(2778)/10000\n",
            "\n",
            "accuracy: tensor(2794)/10000\n",
            "\n",
            "accuracy: tensor(2806)/10000\n",
            "\n",
            "accuracy: tensor(2819)/10000\n",
            "\n",
            "accuracy: tensor(2833)/10000\n",
            "\n",
            "accuracy: tensor(2845)/10000\n",
            "\n",
            "accuracy: tensor(2862)/10000\n",
            "\n",
            "accuracy: tensor(2876)/10000\n",
            "\n",
            "accuracy: tensor(2895)/10000\n",
            "\n",
            "accuracy: tensor(2908)/10000\n",
            "\n",
            "accuracy: tensor(2923)/10000\n",
            "\n",
            "accuracy: tensor(2934)/10000\n",
            "\n",
            "accuracy: tensor(2949)/10000\n",
            "\n",
            "accuracy: tensor(2964)/10000\n",
            "\n",
            "accuracy: tensor(2981)/10000\n",
            "\n",
            "accuracy: tensor(2996)/10000\n",
            "\n",
            "accuracy: tensor(3011)/10000\n",
            "\n",
            "accuracy: tensor(3026)/10000\n",
            "\n",
            "accuracy: tensor(3042)/10000\n",
            "\n",
            "accuracy: tensor(3053)/10000\n",
            "\n",
            "accuracy: tensor(3066)/10000\n",
            "\n",
            "accuracy: tensor(3081)/10000\n",
            "\n",
            "accuracy: tensor(3097)/10000\n",
            "\n",
            "accuracy: tensor(3117)/10000\n",
            "\n",
            "accuracy: tensor(3132)/10000\n",
            "\n",
            "accuracy: tensor(3148)/10000\n",
            "\n",
            "accuracy: tensor(3163)/10000\n",
            "\n",
            "accuracy: tensor(3179)/10000\n",
            "\n",
            "accuracy: tensor(3190)/10000\n",
            "\n",
            "accuracy: tensor(3207)/10000\n",
            "\n",
            "accuracy: tensor(3221)/10000\n",
            "\n",
            "accuracy: tensor(3236)/10000\n",
            "\n",
            "accuracy: tensor(3247)/10000\n",
            "\n",
            "accuracy: tensor(3263)/10000\n",
            "\n",
            "accuracy: tensor(3277)/10000\n",
            "\n",
            "accuracy: tensor(3289)/10000\n",
            "\n",
            "accuracy: tensor(3303)/10000\n",
            "\n",
            "accuracy: tensor(3317)/10000\n",
            "\n",
            "accuracy: tensor(3331)/10000\n",
            "\n",
            "accuracy: tensor(3346)/10000\n",
            "\n",
            "accuracy: tensor(3361)/10000\n",
            "\n",
            "accuracy: tensor(3376)/10000\n",
            "\n",
            "accuracy: tensor(3384)/10000\n",
            "\n",
            "accuracy: tensor(3399)/10000\n",
            "\n",
            "accuracy: tensor(3410)/10000\n",
            "\n",
            "accuracy: tensor(3427)/10000\n",
            "\n",
            "accuracy: tensor(3441)/10000\n",
            "\n",
            "accuracy: tensor(3458)/10000\n",
            "\n",
            "accuracy: tensor(3474)/10000\n",
            "\n",
            "accuracy: tensor(3490)/10000\n",
            "\n",
            "accuracy: tensor(3507)/10000\n",
            "\n",
            "accuracy: tensor(3521)/10000\n",
            "\n",
            "accuracy: tensor(3534)/10000\n",
            "\n",
            "accuracy: tensor(3549)/10000\n",
            "\n",
            "accuracy: tensor(3560)/10000\n",
            "\n",
            "accuracy: tensor(3576)/10000\n",
            "\n",
            "accuracy: tensor(3592)/10000\n",
            "\n",
            "accuracy: tensor(3607)/10000\n",
            "\n",
            "accuracy: tensor(3621)/10000\n",
            "\n",
            "accuracy: tensor(3633)/10000\n",
            "\n",
            "accuracy: tensor(3648)/10000\n",
            "\n",
            "accuracy: tensor(3663)/10000\n",
            "\n",
            "accuracy: tensor(3677)/10000\n",
            "\n",
            "accuracy: tensor(3692)/10000\n",
            "\n",
            "accuracy: tensor(3706)/10000\n",
            "\n",
            "accuracy: tensor(3719)/10000\n",
            "\n",
            "accuracy: tensor(3732)/10000\n",
            "\n",
            "accuracy: tensor(3749)/10000\n",
            "\n",
            "accuracy: tensor(3764)/10000\n",
            "\n",
            "accuracy: tensor(3777)/10000\n",
            "\n",
            "accuracy: tensor(3792)/10000\n",
            "\n",
            "accuracy: tensor(3805)/10000\n",
            "\n",
            "accuracy: tensor(3820)/10000\n",
            "\n",
            "accuracy: tensor(3834)/10000\n",
            "\n",
            "accuracy: tensor(3847)/10000\n",
            "\n",
            "accuracy: tensor(3860)/10000\n",
            "\n",
            "accuracy: tensor(3876)/10000\n",
            "\n",
            "accuracy: tensor(3889)/10000\n",
            "\n",
            "accuracy: tensor(3901)/10000\n",
            "\n",
            "accuracy: tensor(3913)/10000\n",
            "\n",
            "accuracy: tensor(3929)/10000\n",
            "\n",
            "accuracy: tensor(3942)/10000\n",
            "\n",
            "accuracy: tensor(3951)/10000\n",
            "\n",
            "accuracy: tensor(3965)/10000\n",
            "\n",
            "accuracy: tensor(3982)/10000\n",
            "\n",
            "accuracy: tensor(3997)/10000\n",
            "\n",
            "accuracy: tensor(4013)/10000\n",
            "\n",
            "accuracy: tensor(4024)/10000\n",
            "\n",
            "accuracy: tensor(4038)/10000\n",
            "\n",
            "accuracy: tensor(4057)/10000\n",
            "\n",
            "accuracy: tensor(4073)/10000\n",
            "\n",
            "accuracy: tensor(4089)/10000\n",
            "\n",
            "accuracy: tensor(4107)/10000\n",
            "\n",
            "accuracy: tensor(4123)/10000\n",
            "\n",
            "accuracy: tensor(4135)/10000\n",
            "\n",
            "accuracy: tensor(4147)/10000\n",
            "\n",
            "accuracy: tensor(4161)/10000\n",
            "\n",
            "accuracy: tensor(4174)/10000\n",
            "\n",
            "accuracy: tensor(4188)/10000\n",
            "\n",
            "accuracy: tensor(4205)/10000\n",
            "\n",
            "accuracy: tensor(4216)/10000\n",
            "\n",
            "accuracy: tensor(4231)/10000\n",
            "\n",
            "accuracy: tensor(4245)/10000\n",
            "\n",
            "accuracy: tensor(4261)/10000\n",
            "\n",
            "accuracy: tensor(4273)/10000\n",
            "\n",
            "accuracy: tensor(4284)/10000\n",
            "\n",
            "accuracy: tensor(4302)/10000\n",
            "\n",
            "accuracy: tensor(4318)/10000\n",
            "\n",
            "accuracy: tensor(4330)/10000\n",
            "\n",
            "accuracy: tensor(4349)/10000\n",
            "\n",
            "accuracy: tensor(4364)/10000\n",
            "\n",
            "accuracy: tensor(4376)/10000\n",
            "\n",
            "accuracy: tensor(4389)/10000\n",
            "\n",
            "accuracy: tensor(4398)/10000\n",
            "\n",
            "accuracy: tensor(4413)/10000\n",
            "\n",
            "accuracy: tensor(4428)/10000\n",
            "\n",
            "accuracy: tensor(4445)/10000\n",
            "\n",
            "accuracy: tensor(4460)/10000\n",
            "\n",
            "accuracy: tensor(4474)/10000\n",
            "\n",
            "accuracy: tensor(4490)/10000\n",
            "\n",
            "accuracy: tensor(4509)/10000\n",
            "\n",
            "accuracy: tensor(4526)/10000\n",
            "\n",
            "accuracy: tensor(4541)/10000\n",
            "\n",
            "accuracy: tensor(4554)/10000\n",
            "\n",
            "accuracy: tensor(4568)/10000\n",
            "\n",
            "accuracy: tensor(4583)/10000\n",
            "\n",
            "accuracy: tensor(4595)/10000\n",
            "\n",
            "accuracy: tensor(4609)/10000\n",
            "\n",
            "accuracy: tensor(4624)/10000\n",
            "\n",
            "accuracy: tensor(4636)/10000\n",
            "\n",
            "accuracy: tensor(4647)/10000\n",
            "\n",
            "accuracy: tensor(4664)/10000\n",
            "\n",
            "accuracy: tensor(4680)/10000\n",
            "\n",
            "accuracy: tensor(4693)/10000\n",
            "\n",
            "accuracy: tensor(4705)/10000\n",
            "\n",
            "accuracy: tensor(4720)/10000\n",
            "\n",
            "accuracy: tensor(4734)/10000\n",
            "\n",
            "accuracy: tensor(4746)/10000\n",
            "\n",
            "accuracy: tensor(4760)/10000\n",
            "\n",
            "accuracy: tensor(4774)/10000\n",
            "\n",
            "accuracy: tensor(4788)/10000\n",
            "\n",
            "accuracy: tensor(4802)/10000\n",
            "\n",
            "accuracy: tensor(4816)/10000\n",
            "\n",
            "accuracy: tensor(4830)/10000\n",
            "\n",
            "accuracy: tensor(4842)/10000\n",
            "\n",
            "accuracy: tensor(4858)/10000\n",
            "\n",
            "accuracy: tensor(4872)/10000\n",
            "\n",
            "accuracy: tensor(4884)/10000\n",
            "\n",
            "accuracy: tensor(4899)/10000\n",
            "\n",
            "accuracy: tensor(4914)/10000\n",
            "\n",
            "accuracy: tensor(4929)/10000\n",
            "\n",
            "accuracy: tensor(4941)/10000\n",
            "\n",
            "accuracy: tensor(4957)/10000\n",
            "\n",
            "accuracy: tensor(4966)/10000\n",
            "\n",
            "accuracy: tensor(4977)/10000\n",
            "\n",
            "accuracy: tensor(4991)/10000\n",
            "\n",
            "accuracy: tensor(5004)/10000\n",
            "\n",
            "accuracy: tensor(5017)/10000\n",
            "\n",
            "accuracy: tensor(5036)/10000\n",
            "\n",
            "accuracy: tensor(5049)/10000\n",
            "\n",
            "accuracy: tensor(5064)/10000\n",
            "\n",
            "accuracy: tensor(5079)/10000\n",
            "\n",
            "accuracy: tensor(5096)/10000\n",
            "\n",
            "accuracy: tensor(5105)/10000\n",
            "\n",
            "accuracy: tensor(5120)/10000\n",
            "\n",
            "accuracy: tensor(5133)/10000\n",
            "\n",
            "accuracy: tensor(5148)/10000\n",
            "\n",
            "accuracy: tensor(5160)/10000\n",
            "\n",
            "accuracy: tensor(5172)/10000\n",
            "\n",
            "accuracy: tensor(5188)/10000\n",
            "\n",
            "accuracy: tensor(5204)/10000\n",
            "\n",
            "accuracy: tensor(5221)/10000\n",
            "\n",
            "accuracy: tensor(5240)/10000\n",
            "\n",
            "accuracy: tensor(5256)/10000\n",
            "\n",
            "accuracy: tensor(5272)/10000\n",
            "\n",
            "accuracy: tensor(5284)/10000\n",
            "\n",
            "accuracy: tensor(5300)/10000\n",
            "\n",
            "accuracy: tensor(5311)/10000\n",
            "\n",
            "accuracy: tensor(5328)/10000\n",
            "\n",
            "accuracy: tensor(5342)/10000\n",
            "\n",
            "accuracy: tensor(5355)/10000\n",
            "\n",
            "accuracy: tensor(5368)/10000\n",
            "\n",
            "accuracy: tensor(5384)/10000\n",
            "\n",
            "accuracy: tensor(5398)/10000\n",
            "\n",
            "accuracy: tensor(5411)/10000\n",
            "\n",
            "accuracy: tensor(5427)/10000\n",
            "\n",
            "accuracy: tensor(5442)/10000\n",
            "\n",
            "accuracy: tensor(5458)/10000\n",
            "\n",
            "accuracy: tensor(5469)/10000\n",
            "\n",
            "accuracy: tensor(5485)/10000\n",
            "\n",
            "accuracy: tensor(5499)/10000\n",
            "\n",
            "accuracy: tensor(5514)/10000\n",
            "\n",
            "accuracy: tensor(5530)/10000\n",
            "\n",
            "accuracy: tensor(5545)/10000\n",
            "\n",
            "accuracy: tensor(5555)/10000\n",
            "\n",
            "accuracy: tensor(5571)/10000\n",
            "\n",
            "accuracy: tensor(5584)/10000\n",
            "\n",
            "accuracy: tensor(5597)/10000\n",
            "\n",
            "accuracy: tensor(5615)/10000\n",
            "\n",
            "accuracy: tensor(5629)/10000\n",
            "\n",
            "accuracy: tensor(5640)/10000\n",
            "\n",
            "accuracy: tensor(5654)/10000\n",
            "\n",
            "accuracy: tensor(5668)/10000\n",
            "\n",
            "accuracy: tensor(5681)/10000\n",
            "\n",
            "accuracy: tensor(5697)/10000\n",
            "\n",
            "accuracy: tensor(5713)/10000\n",
            "\n",
            "accuracy: tensor(5729)/10000\n",
            "\n",
            "accuracy: tensor(5741)/10000\n",
            "\n",
            "accuracy: tensor(5751)/10000\n",
            "\n",
            "accuracy: tensor(5768)/10000\n",
            "\n",
            "accuracy: tensor(5782)/10000\n",
            "\n",
            "accuracy: tensor(5796)/10000\n",
            "\n",
            "accuracy: tensor(5814)/10000\n",
            "\n",
            "accuracy: tensor(5829)/10000\n",
            "\n",
            "accuracy: tensor(5844)/10000\n",
            "\n",
            "accuracy: tensor(5859)/10000\n",
            "\n",
            "accuracy: tensor(5872)/10000\n",
            "\n",
            "accuracy: tensor(5890)/10000\n",
            "\n",
            "accuracy: tensor(5908)/10000\n",
            "\n",
            "accuracy: tensor(5920)/10000\n",
            "\n",
            "accuracy: tensor(5935)/10000\n",
            "\n",
            "accuracy: tensor(5951)/10000\n",
            "\n",
            "accuracy: tensor(5965)/10000\n",
            "\n",
            "accuracy: tensor(5981)/10000\n",
            "\n",
            "accuracy: tensor(5994)/10000\n",
            "\n",
            "accuracy: tensor(6010)/10000\n",
            "\n",
            "accuracy: tensor(6021)/10000\n",
            "\n",
            "accuracy: tensor(6031)/10000\n",
            "\n",
            "accuracy: tensor(6047)/10000\n",
            "\n",
            "accuracy: tensor(6060)/10000\n",
            "\n",
            "accuracy: tensor(6076)/10000\n",
            "\n",
            "accuracy: tensor(6090)/10000\n",
            "\n",
            "accuracy: tensor(6104)/10000\n",
            "\n",
            "accuracy: tensor(6116)/10000\n",
            "\n",
            "accuracy: tensor(6132)/10000\n",
            "\n",
            "accuracy: tensor(6145)/10000\n",
            "\n",
            "accuracy: tensor(6159)/10000\n",
            "\n",
            "accuracy: tensor(6174)/10000\n",
            "\n",
            "accuracy: tensor(6187)/10000\n",
            "\n",
            "accuracy: tensor(6200)/10000\n",
            "\n",
            "accuracy: tensor(6214)/10000\n",
            "\n",
            "accuracy: tensor(6227)/10000\n",
            "\n",
            "accuracy: tensor(6237)/10000\n",
            "\n",
            "accuracy: tensor(6252)/10000\n",
            "\n",
            "accuracy: tensor(6268)/10000\n",
            "\n",
            "accuracy: tensor(6281)/10000\n",
            "\n",
            "accuracy: tensor(6297)/10000\n",
            "\n",
            "accuracy: tensor(6307)/10000\n",
            "\n",
            "accuracy: tensor(6323)/10000\n",
            "\n",
            "accuracy: tensor(6340)/10000\n",
            "\n",
            "accuracy: tensor(6355)/10000\n",
            "\n",
            "accuracy: tensor(6372)/10000\n",
            "\n",
            "accuracy: tensor(6386)/10000\n",
            "\n",
            "accuracy: tensor(6401)/10000\n",
            "\n",
            "accuracy: tensor(6415)/10000\n",
            "\n",
            "accuracy: tensor(6429)/10000\n",
            "\n",
            "accuracy: tensor(6442)/10000\n",
            "\n",
            "accuracy: tensor(6456)/10000\n",
            "\n",
            "accuracy: tensor(6473)/10000\n",
            "\n",
            "accuracy: tensor(6488)/10000\n",
            "\n",
            "accuracy: tensor(6505)/10000\n",
            "\n",
            "accuracy: tensor(6519)/10000\n",
            "\n",
            "accuracy: tensor(6534)/10000\n",
            "\n",
            "accuracy: tensor(6550)/10000\n",
            "\n",
            "accuracy: tensor(6568)/10000\n",
            "\n",
            "accuracy: tensor(6580)/10000\n",
            "\n",
            "accuracy: tensor(6595)/10000\n",
            "\n",
            "accuracy: tensor(6611)/10000\n",
            "\n",
            "accuracy: tensor(6626)/10000\n",
            "\n",
            "accuracy: tensor(6642)/10000\n",
            "\n",
            "accuracy: tensor(6655)/10000\n",
            "\n",
            "accuracy: tensor(6673)/10000\n",
            "\n",
            "accuracy: tensor(6687)/10000\n",
            "\n",
            "accuracy: tensor(6701)/10000\n",
            "\n",
            "accuracy: tensor(6713)/10000\n",
            "\n",
            "accuracy: tensor(6728)/10000\n",
            "\n",
            "accuracy: tensor(6742)/10000\n",
            "\n",
            "accuracy: tensor(6754)/10000\n",
            "\n",
            "accuracy: tensor(6771)/10000\n",
            "\n",
            "accuracy: tensor(6787)/10000\n",
            "\n",
            "accuracy: tensor(6797)/10000\n",
            "\n",
            "accuracy: tensor(6813)/10000\n",
            "\n",
            "accuracy: tensor(6830)/10000\n",
            "\n",
            "accuracy: tensor(6845)/10000\n",
            "\n",
            "accuracy: tensor(6860)/10000\n",
            "\n",
            "accuracy: tensor(6872)/10000\n",
            "\n",
            "accuracy: tensor(6888)/10000\n",
            "\n",
            "accuracy: tensor(6903)/10000\n",
            "\n",
            "accuracy: tensor(6918)/10000\n",
            "\n",
            "accuracy: tensor(6934)/10000\n",
            "\n",
            "accuracy: tensor(6951)/10000\n",
            "\n",
            "accuracy: tensor(6968)/10000\n",
            "\n",
            "accuracy: tensor(6979)/10000\n",
            "\n",
            "accuracy: tensor(6991)/10000\n",
            "\n",
            "accuracy: tensor(7003)/10000\n",
            "\n",
            "accuracy: tensor(7015)/10000\n",
            "\n",
            "accuracy: tensor(7027)/10000\n",
            "\n",
            "accuracy: tensor(7041)/10000\n",
            "\n",
            "accuracy: tensor(7056)/10000\n",
            "\n",
            "accuracy: tensor(7073)/10000\n",
            "\n",
            "accuracy: tensor(7088)/10000\n",
            "\n",
            "accuracy: tensor(7100)/10000\n",
            "\n",
            "accuracy: tensor(7114)/10000\n",
            "\n",
            "accuracy: tensor(7124)/10000\n",
            "\n",
            "accuracy: tensor(7135)/10000\n",
            "\n",
            "Loss de Teste: 0.045024435967206955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "OtOVNyAUZk1O"
      },
      "source": [
        "# Atividade 2\n",
        "\n",
        "\n",
        "\n",
        "*  Agora, utilizando como base a atividade anterior, treine um defina e treine um AutoEncoder Convolucional. \n",
        "*  Repita os passos de definição do modelo, função de custo e otimizador, além do treinamento\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "nSO1IbttaDmn"
      },
      "source": [
        "Definição do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk1f6p9to_zG"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68Qj3W4AZlNX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30cf04b2-58cc-4a5b-ef69-0eab3f66571f"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# define the NN architecture\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        # encoder\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)  \n",
        "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        ## decoder\n",
        "        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n",
        "        self.t_conv2 = nn.ConvTranspose2d(16, 3, 2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encode\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        \n",
        "        # decode\n",
        "        x = F.relu(self.t_conv1(x))\n",
        "        x = F.sigmoid(self.t_conv2(x))\n",
        "                \n",
        "        return x\n",
        "\n",
        "# initialize\n",
        "model = ConvAutoencoder()\n",
        "print(model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvAutoencoder(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (t_conv1): ConvTranspose2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (t_conv2): ConvTranspose2d(16, 3, kernel_size=(2, 2), stride=(2, 2))\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "AO4yPV1xbwE6"
      },
      "source": [
        "**Alocação do modelo e device adequado (dependente da disponibilidade GPU)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6d6VaPcbwb3"
      },
      "source": [
        "# joga o modelo para GPU, caso aplicável\n",
        "if usar_gpu:\n",
        "  torch.cuda.init()\n",
        "  conv_ae.cuda()\n",
        "  cudnn.benchmark = True"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "c3Uqxwx8bZOt"
      },
      "source": [
        "**Função de Custo e Otimizador**. \n",
        "\n",
        "Vide [`torch.optim`](https://pytorch.org/docs/stable/optim.html) e [`torch.nn` Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions). Atentem para todos os parametros necessários."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpmNi4WNbZlw"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# especifique a funcao de custo (loss) adequada [Lembrem-se: reconstrução]\n",
        "#DONE\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# especifique o otimizador\n",
        "#DONE\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "kWTbCMBTbZ8k"
      },
      "source": [
        "**Treinamento**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlcnQJdHbaSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f617cf-30e6-4981-aeb2-243ca712cd88"
      },
      "source": [
        "# definição do numero de épocas\n",
        "n_epocas = 10\n",
        "\n",
        "for epoca in range(n_epocas):\n",
        "  loss_treino = 0\n",
        "\n",
        "  for data in train_loader:\n",
        "    # mover tensores para GPU, se aplicável\n",
        "    if usar_gpu:\n",
        "      x, y = x.cuda(), y.cuda()\n",
        "\n",
        "    images, _ = data\n",
        "    # TODO: 1. Zere o gradiente de todas as variávels do otimizador \n",
        "    #          (vide zero_grad do nn.optim)\n",
        "    #DONE\n",
        "    optim.zero_grad()\n",
        "\n",
        "    # TODO: 2. Faça um forward no modelo passando x como entrada \n",
        "    #          (não deixe de capturar a saída)\n",
        "    #DONE\n",
        "    outputs = model(images)\n",
        "\n",
        "    # TODO: 3. Calcule a loss do batch (utilize o criterion para comparar a \n",
        "    #          saída do modelo com os targets y)\n",
        "    #DONE\n",
        "    loss = criterion(outputs, images)\n",
        "\n",
        "    # TODO: 4. Compute o gradiente (backward na loss)\n",
        "    #DONE\n",
        "    loss.backward()\n",
        "\n",
        "\n",
        "    # TODO: 5. Realize um passo de otimização (step no optim)\n",
        "    #DONE\n",
        "    optim.step()\n",
        "\n",
        "    # atualização da loss de treinamento\n",
        "    loss_treino += loss.item()*images.size(0)\n",
        "\n",
        "  loss_treino = loss_treino/len(train_loader.dataset)\n",
        "  print(f\"Epoca {epoca+1}{n_epocas} \\t Loss de Treinamento: {loss_treino}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 110 \t Loss de Treinamento: 0.5870850721836091\n",
            "Epoca 210 \t Loss de Treinamento: 0.5744796787261963\n",
            "Epoca 310 \t Loss de Treinamento: 0.5730132230758667\n",
            "Epoca 410 \t Loss de Treinamento: 0.5723545840740204\n",
            "Epoca 510 \t Loss de Treinamento: 0.5718990381717682\n",
            "Epoca 610 \t Loss de Treinamento: 0.5715627536773682\n",
            "Epoca 710 \t Loss de Treinamento: 0.5713542158126831\n",
            "Epoca 810 \t Loss de Treinamento: 0.5711923103809357\n",
            "Epoca 910 \t Loss de Treinamento: 0.5710692519426346\n",
            "Epoca 1010 \t Loss de Treinamento: 0.5708919091701508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMQeDDu1rfMm"
      },
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  \n",
        "    plt.imshow(np.transpose(img, (1, 2, 0))) "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "syWqQoFUmluQ",
        "outputId": "3228e1d2-21bc-4016-fd5d-cac9aaa1a460"
      },
      "source": [
        "  # TODO: 6. plote as 10 primeiras imagens (reconstruídas e decodificadas)\n",
        "  # do último batch  (usem o matplotlib)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# test batch\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "\n",
        "output = model(images)\n",
        "images = images.numpy()\n",
        "output = output.view(batch_size, 3, 32, 32)\n",
        "output = output.detach().numpy()\n",
        "\n",
        "# reconstruídas\n",
        "fig, axes = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True, figsize=(28,4))\n",
        "for i in np.arange(10):\n",
        "    ax = fig.add_subplot(1, 10, i+1, xticks=[], yticks=[])\n",
        "    imshow(output[i])\n",
        "    ax.set_title(classes[labels[i]])\n",
        "    \n",
        "# originais\n",
        "fig, axes = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True, figsize=(28,4))\n",
        "for i in np.arange(10):\n",
        "    ax = fig.add_subplot(1, 10, i+1, xticks=[], yticks=[])\n",
        "    imshow(images[i])\n",
        "    ax.set_title(classes[labels[i]])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2016x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2016x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}